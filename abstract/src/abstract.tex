\section*{Abstract}

Nowadays, audio constitutes a central part of most generated content. Usually, manual labor is behind the terabytes of audio published daily. If some creator desires a specific sound, they must research it under online databases, synthesize it, or even record it themselves. This amount of work is a barrier to content creation, mainly if such sounds are elaborate or too specific.

Audio files present solely one dimension, \textit{i.e.}, the amplitude of its sound wave collected at a designated rate interval. In comparison, image files --- whose generator models are becoming mainstream --- present three data dimensions. Regardless, long-term dependencies convey a challenge in sound as they are more complex and intricate than those of images. For instance, there is the expectation that the timbre of a given instrument or the background noise of a given sonic environment remains over time.

This dissertation studies avant-garde generative AI models and proposes and follows the implementation of a system capable of synthesizing audio snippets with high fidelity through textual input. A generated audio snippet has high fidelity if it is challenging for humans to distinguish it from a real-world sound. The proposed system does not confine these audio snippets to a domain like music or speech but accepts any textual prompt. The dissertation focuses on the development and experimentation of various model architectures based on state-of-the-art systems, ideally resulting in an instance capable of yielding good results.

Given that meaningful work on generative models for images or text-to-speech already exists, the expectation is that a general audio synthesis model generates satisfactory outcomes. This work's results will impact the speed of the production process for content creators, sound engineers, and everyone interested in creating audio outputs.

@misc{elsea_basics_1996,
  title      = {Basics of {Digital} {Recording}},
  shorttitle = {Digital {Recording}},
  url        = {http://artsites.ucsc.edu/EMS/music/tech_background/TE-16/teces_16.html},
  urldate    = {2022-11-09},
  journal    = {Univeristy of California, Santa Cruz},
  author     = {Elsea, Peter},
  year       = {1996},
  file       = {Digital Recording:/Users/marcio/Zotero/storage/7ACP8LT9/teces_16.html:text/html}
}

@incollection{huzaifah_deep_2021,
  address   = {Cham},
  title     = {Deep {Generative} {Models} for {Musical} {Audio} {Synthesis}},
  isbn      = {978-3-030-72116-9},
  url       = {https://doi.org/10.1007/978-3-030-72116-9_22},
  abstract  = {Sound modelling is the process of developing algorithms that generate sound under parametric control.},
  booktitle = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
  publisher = {Springer International Publishing},
  author    = {Huzaifah, Muhammad and Wyse, Lonce},
  editor    = {Miranda, Eduardo Reck},
  year      = {2021},
  doi       = {10.1007/978-3-030-72116-9_22},
  pages     = {639--678},
  file      = {Full Text:/Users/marcio/Zotero/storage/I5J5VUEX/Huzaifah and Wyse - 2021 - Deep Generative Models for Musical Audio Synthesis.pdf:application/pdf}
}


@inproceedings{kumar_melgan_2019,
  title     = {{MelGAN}: {Generative} {Adversarial} {Networks} for {Conditional} {Waveform} {Synthesis}},
  volume    = {32},
  url       = {https://proceedings.neurips.cc/paper/2019/file/6804c9bca0a615bdb9374d00a9fcba59-Paper.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brébisson, Alexandre and Bengio, Yoshua and Courville, Aaron C},
  editor    = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
  year      = {2019},
  file      = {Full Text:/Users/marcio/Zotero/storage/CTF59XSN/Kumar et al. - 2019 - MelGAN Generative Adversarial Networks for Condit.pdf:application/pdf}
}

@inproceedings{tahiroglu_-terity_2020,
  series    = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
  title     = {Al-terity: {Non}-{Rigid} {Musical} {Instrument} with {Artificial} {Intelligence} {Applied} to {Real}-{Time} {Audio} {Synthesis}},
  abstract  = {A deformable musical instrument can take numerous dis- tinct shapes with its non-rigid features. Building audio syn- thesis module for such an interface behaviour can be chal- lenging. In this paper, we present the Al-terity, a non-rigid musical instrument that comprises a deep learning model with generative adversarial network architecture and use it for generating audio samples for real-time audio synthesis. The particular deep learning model we use for this instru- ment was trained with existing data set as input for pur- poses of further experimentation. The main benefits of the model used are the ability to produce the realistic range of timbre of the trained data set and the ability to generate new audio samples in real-time, in the moment of playing, with the characteristics of sounds that the performer ever heard before. We argue that these advanced intelligence features on the audio synthesis level could allow us to ex- plore performing music with particular response features that define the instrument’s digital idiomaticity and allow us reinvent the instrument in the act of music performance.},
  language  = {English},
  booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
  publisher = {International Conference on New Interfaces for Musical Expression},
  author    = {Tahiroğlu, Koray and Kastemaa, Miranda and Koli, Oskar},
  month     = jul,
  year      = {2020},
  keywords  = {Artificial Intelligence (AI), deep learning, GAN, NIME, SOPI},
  pages     = {337--342},
  file      = {Proceedings of the International Conference on New.pdf:/Users/marcio/Zotero/storage/H3LYBRT6/Proceedings of the International Conference on New.pdf:application/pdf}
}

@inproceedings{kong_hifi-gan_2020,
  title     = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for {Efficient} and {High} {Fidelity} {Speech} {Synthesis}},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  editor    = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year      = {2020},
  pages     = {17022--17033},
  file      = {Full Text:/Users/marcio/Zotero/storage/HT7ZGKEE/Kong et al. - 2020 - HiFi-GAN Generative Adversarial Networks for Effi.pdf:application/pdf}
}

@misc{oord_wavenet_2016,
  title      = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
  shorttitle = {{WaveNet}},
  url        = {http://arxiv.org/abs/1609.03499},
  doi        = {10.48550/arXiv.1609.03499},
  abstract   = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  urldate    = {2022-10-29},
  publisher  = {arXiv},
  author     = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  month      = sep,
  year       = {2016},
  note       = {arXiv:1609.03499 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/P6KZR7GL/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/VLWC73AW/1609.html:text/html}
}

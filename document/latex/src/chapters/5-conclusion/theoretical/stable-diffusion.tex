\subsubsection{Fine-Tune Stable Diffusion for Spectrograms}

A promising avenue of research in the \ac{DL} community is the fine-tuning of existing generative models to improve their capabilities and produce high-quality audio output. This section introduces the concept of fine-tuning generative models for audio synthesis, and proposes fine-tuning the stable diffusion model to generate spectrograms.

The stable diffusion model uses the principles of diffusion processes to generate high-fidelity and diverse image samples (see Section\ \ref{sec:stable-diffusion}). One of the key advantages of the stable diffusion model is its ability to capture complex dependencies and generate realistic output.

It is worth noting that the stable diffusion model is an open source model, meaning that its code and implementation details are publicly available. This accessibility allows researchers and practitioners to study, modify, and build upon the foundations of the model. The availability of open source code for the stable diffusion model facilitates its fine-tuning for specific tasks, such as audio synthesis.

The fine-tuning of the stable diffusion model to spectrograms provides an exciting opportunity to explore the generation of high-quality audio output based on this visual representation.

One of the key advantages of the stable diffusion model is its ability to capture complex dependencies and generate realistic images. By fine-tuning the stable diffusion model on spectrograms, one can leverage its prior knowledge and adapt it to the specific characteristics of audio signals represented in the frequency and time domains.

The potential benefits of using the prior knowledge of the stable diffusion model for audio synthesis are manifold. First, the stable diffusion model has already shown impressive results in other domains, in this case image synthesis. By building on this foundation, one can exploit the model's ability to generate high-fidelity and diverse outputs, which can greatly improve the quality of the synthesized audio.

Furthermore, fine-tuning the stable diffusion model to spectrograms can provide a unique perspective on audio synthesis. By treating spectrograms as images and applying the stable diffusion model, one can explore the potential of generating audio based on this visual representation. This approach opens up new possibilities for manipulating and synthesizing audio in innovative ways.

To prove the usefulness of this method, Riffusion (see Section~\ref{sec:riffusion}), which already performs a similar task of generating audio from visual representations, has shown considerable results. By considering the insights and techniques used in Riffusion, one can build on its foundations and adapt the stable diffusion model accordingly.

It is important to note that the success of the proposed approach depends on the availability of a sufficiently large dataset for fine-tuning.

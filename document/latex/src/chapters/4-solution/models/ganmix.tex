\subsection{GANmix} \label{sec:ganmix}

This section introduces GANmix as a novel technique for audio generation that fuses components of both a \ac{GAN} and a \ac{VAE}. GANmix addresses the problem of generating high-quality audio under resource constraints.

Audio generation is computationally expensive, especially when constrained by hardware capabilities. GANmix offers a potential solution by combining the strengths of \acp{GAN} and \acp{VAE} to improve audio synthesis in limited computational environments through latent space manipulation.

\paragraph{Model Architecture Plan}

The GANmix architecture consists of a generator and a discriminator that operate in latent space rather than in the space of the sound or soundscape. This approach was inspired by stable diffusion (see Section~\ref{sec:stable-diffusion}), which generates samples through its latent space instead of working on the sample space itself.

Traditionally, in \acp{GAN}, the generator produces samples to be evaluated by the discriminator. However, GANmix takes a different approach: its generator produces values in an embedding space defined by the pre-trained AudioLDM \ac{VAE} encoder~\cite{liu_audioldm_2023}. The discriminator, on the other hand, objectively verifies latent features by comparing the generated features with those obtained by the \ac{VAE} encoder. Samples are generated posteriorly by passing the generated features through the decoder of the \ac{VAE}.

The decision to incorporate the AudioLDM \ac{VAE} model within the GANmix architecture was based on thoughtful reasons.

\begin{enumerate}
    \item \textbf{\ac{VAE} Training:} Training \acp{VAE} poses a challenge since it typically requires significant computational resources and extensive datasets to achieve effective convergence. Previous experiences with model development (in Section~\ref{sec:training-vae}) for this thesis highlighted the intricacies involved in \ac{VAE} training.
    \item \textbf{AudioLDM's High-Performance:} At the time of GANmix's development, the AudioLDM model was one of the top models for audio generation.
    \item \textbf{Accessibility and Open Source Nature of AudioLDM:} One key benefit of integrating the AudioLDM \ac{VAE} model into GANmix architecture was its openness and accessibility through platforms like Hugging Face's model hub through \url{https://huggingface.co/cvssp/audioldm}. This accessibility streamlined the incorporation of the high-performing AudioLDM.
\end{enumerate}

The generator is designed to convert random Gaussian noise vectors into latent features that resemble the encodings of the AudioLDM \ac{VAE} model. Initially, a convolutional model was designed. It used four convolutional transpose 2D layer blocks, with Leaky \ac{ReLU} activation functions following three of them. Each block comprised several convolutional layers. Furthermore, after the deconvolutional layer, two convolutional layers are employed to preserve the data's shape. The final block, responsible for producing the transformed audio samples, did not use an activation function. This design decision enabled the generator to generate unrestricted values, ensuring the accuracy of the produced audio samples.

The decision to utilize 2D convolutions arised from the innate characteristics of the latent features generated by the AudioLDM \ac{VAE} model. These latent features have various dimensions, and integrating 2D convolutional layers enabled GANmix to efficiently grasp complicated patterns across these dimensions. By utilizing 2D convolutions, the architecture could more effectively utilize the multidimensional information in the latent representations, resulting in an improved quality of the generated audio output.

The discriminator architecture mimicked that of the generator with three blocks of convolutional 2D layers using leaky \ac{ReLU} activation. Each block contained multiple convolutional layers (three in the proposed architecture). The final layer of the discriminator applied a sigmoid activation function to generate a probability score indicating the authenticity of the input audio sample.

\paragraph{Experimental Results}

Preliminary experiments with GANmix and the Audio MNIST dataset produced promising but flawed results. Objective analysis reveals that the generator's performance lagged behind the discriminator's, resulting in suboptimal sample quality. Therefore, the training process requires further refinement.

In order to bridge the performance gap between generator and discriminator, a range of refinements were explored. These efforts included exploring different optimizers, such as Adam, RMSProp, and SGD (see Section~\ref{sec:dl-foundations}), each with varying hyperparameters. In addition, the author adjusted model sizes, experimented with diverse loss functions, and introduced noise to training samples. However, the speed of training and comprehensive experimentation were still hindered by hardware resource limitations.

More information regarding these explorations can be found in Chapter~\ref{chap:results}.

With access to a more powerful computing environment, GANmix was further developed using the Clotho dataset (see Section~\ref{sec:clotho}), leading to a significant improvement in the quality of generated audio samples. Nonetheless, there were still issues with achieving equilibrium between the generator and discriminator, even with the upgraded dataset.

\paragraph{Final Model}

After conducting a series of experiments, the GANmix model reached its culmination. This architecture incorporates the Clotho dataset and includes significant improvements to both the generator and discriminator.

The GANmix model is the final model used in this thesis, corresponding to the model used in Experiment 10. Unlike most state-of-the-art generative models that use \acp{CNN}, the GANmix model uses fully connected neural networks for both the generator and the discriminator. The rationale for this was empirical and is given in section~\ref{sec:exp10}.

The generator takes as input a random Gaussian noise vector of size $NZ$ and passes it through a fully connected hidden layer of $NH$ neurons. The output of the hidden layer is then fully connected to another layer of size $EW \times EH \times ED$, where $EW$, $EH$, and $ED$ are the width, height, and number of dimensions of the embeddings generated by the \ac{VAE}, respectively. The output layer is reshaped to match the shape of the \ac{VAE} embeddings.

The discriminator takes as input an embedding of shape $EW \times EH \times ED$ and flattens it to a vector. The vector is then passed through a fully connected hidden layer of $NH$ neurons, and then fully connected to a single output neuron that applies a tanh activation function.

The \ac{VAE} encoder and decoder are pre-trained by a state-of-the-art network, AudioLDM. However, the GANmix model can work with any \ac{VAE} network, as long as the size of the last layer of the generator (and the first layer of the discriminator) is adjusted accordingly.

The loss function used to train the GANmix model is \ac{BCE}. The generator is optimized using Adam with a learning rate of $1 \times 10^{-3}$, while the discriminator is optimized using Adam with a learning rate of $1 \times 10^{-4}$. A scheduler updates the learning rate every 10 epochs.

The implementation of GANmix can be found in Appendix~\ref{ann:ganmix-implementation}. More about the configuration and parameters of the GANmix model can be found in Appendix~\ref{ann:ganmix-conf}. The model's architecture is shown in Figure~\ref{fig:ganmix-architecture}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/4-solution/ganmix.pdf}
    \caption[GANmix architecture]{\textbf{GANmix architecture} --- On the left is the GANmix generator with three layers: the initial noise layer with $NZ$ neurons, the hidden layer with $NH$ neurons, and the layer corresponding to the embedding space with $EW \times EH \times ED$ neurons. The outputs of this layer are transformed into the form of the outputs of the \ac{VAE}. On the right is the discriminator that mirrors the generator. The last layer has a single neuron with a sigmoid activation function. At the top is a spectrogram that passes through the pre-trained \ac{VAE} generator to generate the latent space. At the bottom, the opposite happens: embeddings in the latent space go through the \ac{VAE} decoder to generate the audio sample in the form of a spectrogram.}
    \label{fig:ganmix-architecture}
\end{figure}

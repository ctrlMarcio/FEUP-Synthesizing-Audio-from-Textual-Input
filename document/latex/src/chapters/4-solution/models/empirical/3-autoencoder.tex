\subsubsection{Simple Autoencoder for Audio Data Compression}

This section outlines the procedure for building a basic \ac{AE} (refer to Section~\ref{sec:autoencoders}), which is an essential step in making more complex versions like the \ac{VAE} (refer to Section~\ref{sec:vae}) in future research. The PyTorch-based implementation of the \ac{AE} is available in the given code repository (refer to Annex~\ref{ann:AE}).

The same preprocessing applied to previous models (padding and random cropping) is applied in this one.

The architecture of the \ac{AE} is reminiscent of a U-Net (see Section~\ref{sec:u-net}), incorporating four types of layers: convolutional, max pooling, upsampling, and transposed convolutional layers (see Section~\ref{sec:conv-layers}). The activation function used in convolutional and transposed convolutional layers is the \ac{tanh} function (see Section~\ref{sec:activation}). The \ac{tanh} activation function ensures that the output of the \ac{AE} remains within the range of -1 to 1, which is crucial for the subsequent denormalization process. Code for defining and training the model, as well as a result example can be seen in Annex~\ref{ann:AE}, an abstract representation of this architecture is in Figure~\ref{fig:autoencoder}.

The encoder is constructed as a sequence of convolutional layers, followed by batch normalization, activation functions, and max-pooling operations. The input to the encoder is a 1D audio waveform with a single channel. The first convolutional layer has 32 filters, a kernel size of 9, a stride of 1, and a padding of 4. It is followed by batch normalization and the \ac{tanh} activation function. A max-pooling layer with kernel size 2 and stride 2 is then applied. 

There were four convolutional layers. For each one, the number of filters is doubled from the previous layer, and the exact configuration of convolution, batch normalization, activation, and max-pooling operations is repeated. The kernel size, stride, and padding remain consistent for all convolutional layers.

The decoder is constructed as a sequence of upsampling (by a factor of 2), transposed convolutional layers, batch normalization, and activation functions. The decoder's architecture is symmetric to the encoder, with the number of filters halving at each layer until reaching the original number of channels (1). The transposed convolutional layers have the same kernel size, stride, and padding as the corresponding encoder convolutional layers. The last layer of the decoder applies batch normalization, a \ac{tanh} activation function, and produces the reconstructed audio waveform.

The \ac{ReLU} activation function was tested during experimentation, but the results proved unsatisfactory, as every sound frame was above 0. Consequently, the decision was made to continue using the \ac{tanh} activation function for the simple \ac{AE}.

A series of steps is carried out during the training loop to train the model. First, each batch of audio samples undergoes a normalization process to ensure consistent data representation. The normalized data is then fed through the model, resulting in two crucial outputs: an encoded representation of the audio and a reconstructed audio waveform.

To assess the quality of the reconstructed audio, a loss function is employed, which quantifies the dissimilarity between the reconstructed waveform and the original input using \ac{MSE} (see Section~\ref{sec:mse}). This loss value serves as a measure of how well the \ac{AE} can capture and reproduce the essential characteristics of the audio data.

The calculated backpropagation gradients are subsequently used to update the model's parameters using the Adam optimizer (see Section~\ref{sec:adam}), iteratively refining the \ac{AE}'s ability to encode and decode the audio samples.

The training process follows an iterative approach, where the model is trained for multiple epochs. The training data is iterated over in each epoch, and the model's parameters are updated based on the computed gradients. The best model (with the lowest loss) obtained during training is saved for future use.

By constructing this simple \ac{AE}, valuable insights into the underlying mechanisms of \ac{AE}s are gained, which is instrumental in developing more sophisticated techniques. Moreover, the provided code implementation in PyTorch (refer to Annex~\ref{ann:AE}) facilitates a deeper understanding and exploration of the \ac{AE} architecture, enabling improvements and extensions to audio data compression.


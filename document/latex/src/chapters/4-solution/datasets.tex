\section{Dataset Selection and Analysis} \label{sec:sol-datasets}

This section is crucial to synthesizing audio from text inputs, as it reveals the foundation on which the generative models are developed and improved. Datasets are the foundation of this research. These datasets serve as the learning material for \ac{DL} algorithms and a means to evaluate the perceptual quality, coherence, and creativity of the resulting audio outputs.

It presents the datasets researched and used to implement the models. It is worth noting that datasets that are well-known within the field but are neither used nor relevant to the specific problem at hand, are excluded from citation. For example, this discussion does not include datasets specialized in speech or music. In addition, although the data size is significant for \ac{DL} algorithms, all the datasets discussed in this section include more than 1000 sound samples. Nonetheless, it is important to recognize that this number is relatively small compared to image recognition datasets with millions of entries, like CLIP~\cite{radford_learning_2021}, commonly used in the field.

Two types of datasets are cataloged, differentiated mostly by the nature of the adopted label â€“ \emph{categorical} and \emph{descriptive}. A categorical-label dataset comprises sounds that are associated with a single, distinct label, such as ``music'', ``piano'', or ``singing''. On the other hand, descriptive-label datasets comprise natural language sentences or textual descriptions of soundscapes, such as  ``boy singing while playing the piano''. In the realm of \ac{DL}, descriptive-labeled datasets are deemed more conducive. Nevertheless, datasets containing categorical labels can still be valuable when undergoing augmentation techniques. To the author's knowledge, Table~\ref{tab:datasets} provides a comprehensive list of audio datasets that meet the aforementioned criteria to date.


\begin{table}[ht]
\centering
\caption{Comparison of datasets for soundscapes}
\label{tab:datasets}
\begin{tabularx}{\textwidth}{|X|X|X|X|X|}
\hline
\textbf{Name} &
  \textbf{Type} &
  \textbf{\# Samples} &
  \textbf{Duration} &
  \textbf{Labels} \\ \hline

Acoustic Event Dataset \cite{takahashi_deep_2016} &
  Categorical labeled &
  5223 &
  Average 8.8s &
  One of 28 labels \\ \hline

AudioCaps \cite{kim_audiocaps_2019} &
  Descriptive labeled &
  39597 &
  10s each &
  9 words per caption \\ \hline

AudioSet \cite{gemmeke_audio_2017} &
  Categorical labeled &
  2084320 &
  Average 10s &
  One or more of 527 labels \\ \hline

Audio MNIST~\cite{becker_interpreting_2018}&
  Categorical labeled &
  30000 &
  Average 0.6s &
  One of 10 labels \\ \hline
  
Clotho \cite{drossos_clotho_2019} &
    Descriptive labeled &
    4981 &
    15 to 30s &
    24 905 captions (5 per audio). 8 to 20 words long each \\ \hline
    
FSDKaggle2018 \cite{fonseca_general-purpose_2018} &
  Categorical labeled &
  11073 &
  From 300ms to 30s &
  One or more of 41 labels \\ \hline
  
UrbanSound8K \cite{salamon_dataset_2014} &
  Categorical labeled &
  8732 &
  Less or equal to 4s &
  One of 10 labels \\ \hline
  
YouTube-8M Segments \cite{abu-el-haija_youtube-8m_2016} &
  Categorical labeled &
  237000 &
  5s &
  One or more of 1000 labels \\ \hline
\end{tabularx}
\end{table}

\subsection{Categorical Labeled Datasets}

The datasets represented here are the ones where the audio samples are tagged with simple labels. An example of one of these labels might be ``Dog''. This labeling contrasts complete with descriptive labeling as seen in Section~\ref{sec:sound-nl-labelled}. Even though these datasets are not perfect for this use case, they might be augmented or enhanced (see Section~\ref{sec:text-augmentation}). One such example would be taping different sounds. For instance, taping a ``dog''  ``bark'' with a ``truck'' ``honk'' could generate the label ``a dog barking followed by a truck honking''.

The datasets presented here contain audio samples tagged with simple labels such as ``Dog'', in contrast to the descriptive labeling discussed in Section~\ref{sec:sound-nl-labelled}. While these datasets may not be ideal for the purposes of this thesis, they can be augmented or enhanced (as explained in Section~\ref{sec:text-augmentation}) through means such as taping difference sounds. For example, joining a ``do barking'' and a ``truck honking'' could produce the label ``a dog barking followed by a truck honking.''


\subsubsection{Acoustic Event Dataset}

Information regarding this dataset is not much, and the classes have rather specific names such as ``hammer'' or ``mouse\_click''. Nevertheless, it has 5223 samples, completing 768.4 minutes, representing 28 different classes \cite{takahashi_deep_2016}.

\subsubsection{Audio MNIST} \label{sec:dataset-amnist}

Audio MNIST \cite{becker_interpreting_2018} is a sonic take on the famous MNIST dataset \cite{deng_mnist_2012}. It has a series of 60 speakers saying different numbers from 0 to 9. These samples add up to 30000 samples.

This dataset is not used in the model but rather as a first take for every baseline model for its simplicity and small size.

\subsubsection{AudioSet}

AudioSet is the most comprehensive dataset existent for audio \cite{gemmeke_audio_2017}.

It is a large-scale, high-quality dataset of annotated audio clips. It was created by Google Research and released in 2017 as part of the ongoing effort to advance state-of-the-art audio understanding.
The dataset consists of over 2 million 10-second audio clips, each annotated with one or more labels from a set of 527 classes, covering a wide range of sounds such as human speech, animal vocalizations, musical instruments, environmental sounds, and more. The audio clips are sourced from YouTube videos and cover diverse content, including news broadcasts, movie trailers, music videos, and everyday videos.

The annotations in AudioSet are created using a hierarchical ontology, where the classes are organized into a tree-like structure based on their semantic relationship. This allows for a more fine-grained representation of audio events and makes it easier for machine learning models to learn the relationship between different sound categories.

\subsubsection{FSDKaggle2018}

Freesound Dataset Kaggle 2018 \cite{fonseca_general-purpose_2018} is an audio dataset containing 11,073 audio files annotated with 41 labels following the same ontology as AudioSet. The sounds were taken from Freesound which is an online database maintained by the community. So, the quality may vary.

This dataset was used for competitions on Kaggle regarding sound classification.

\subsubsection{UrbanSound8K}

The UrbanSound8K \cite{salamon_dataset_2014} is an available dataset focusing on urban sounds. It is one of the largest and most diverse datasets of its kind.

The UrbanSound8K dataset consists of 8732 audio clips, each lasting approximately 4 seconds, collected from various urban environments. The audio clips are annotated with one of 10 classes. The classes in the dataset represent a wide range of typical urban sounds.
The audio clips in the UrbanSound8K dataset are collected from various sources, including YouTube and Freesound, and are carefully selected to ensure a diverse and representative sample of urban sounds. The dataset includes audio recordings from different cities, captured in different seasons and weather conditions, to provide a wide range of sound characteristics and environments.

\subsubsection{YouTube-8M Segments}

The YouTube-8M Dataset is a significant video dataset \cite{abu-el-haija_youtube-8m_2016} that contains millions of YouTube video IDs with high-quality machine-generated annotations from a diverse vocabulary of over 3,800 visual entities. This dataset aims to accelerate research on large-scale video understanding, representation learning, noisy data modeling, transfer learning, and domain adaptation approaches for video.

Upon reviewing the YouTube-8M dataset, it is evident that the minimum video length constraint of 120 seconds makes it unsuitable for this research as it should focus on snippets shorter than this length.

An alternative dataset that may suit this research's needs is the YouTube-8M Segments Dataset. This dataset is an extension of the YouTube-8M dataset and comes with human-verified segment annotations that temporally localize entities in videos. With about 237K human-verified segment labels on 1000 classes, the dataset enables the training of models for temporal localization using a large amount of noisy video-level labels in the training set.

The vocabulary of the segment-level dataset is a subset of the YouTube-8M dataset vocabulary, excluding entities that are not temporally localizable, such as movies or TV series that usually occur in the whole video. The dataset also provides time-localized frame-level features, enabling classifier predictions at the segment-level granularity.

While the YouTube-8M Segments Dataset provides temporal annotations for video entities, this dataset could still be valuable if one extracts the audio from each YouTube video, as it includes time-localized frame-level features that could be used for audio analysis. Thus, while the video content may not be essential for this research project, the YouTube-8M Segments Dataset could still be a valuable audio-analysis resource.

\subsection{Descriptive Labeled Datasets} \label{sec:sound-nl-labelled}

The datasets presented here present way richer textual information regarding the sound. For instance, instead of a sound being connected with the label ``engine'', datasets in this section connect it with a sentence such as ``a vehicle engine revving''. This allows for training the natural language input, one of the main assumed problems.

\subsubsection{AudioCaps}

AudioCaps \cite{kim_audiocaps_2019} is a large-scale dataset of 46 thousand audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset.

\subsubsection{Clotho Dataset} \label{sec:clotho}

Clotho \cite{drossos_clotho_2019} is an audio dataset that consists of 4981 audio samples. Each audio sample has five captions (a total of 24 905 captions). Audio samples are 15 to 30 seconds long, and captions are 8 to 20 words long. 

A practical example is a sound that is described both by ``a car honks from the midst of a rainstorm'', ``rain from a rainstorm is hitting surfaces and a car honks'', and others.
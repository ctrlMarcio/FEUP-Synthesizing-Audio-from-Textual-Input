\subsubsection{GLIDE} \label{sec:glide}

The \acf{GLIDE} model is a state-of-the-art approach for generating high-fidelity synthetic images from free-form natural-language text prompts. The model is based on a guided diffusion-based approach, which is the first attempt by OpenAI at text-conditioned image generation using guided diffusion. The approach involves two types of guidance strategies during model training: classifier-free guidance~\cite{ho_classifier-free_2022}, which relies solely on the model's knowledge, and \ac{CLIP} guidance which uses a pre-trained \ac{CLIP} model~\cite{radford_learning_2021} to provide guidance based on caption matching.

The experimental results of the \ac{GLIDE} paper demonstrate several benefits of the proposed approach. Human evaluators preferred images generated with classifier-free guidance over \ac{CLIP} guidance regarding photorealism and caption similarity. Samples from the 3.5 billion parameter \ac{GLIDE} model were also found to outperform DALL-E (see Section~\ref{fig:dall-e}) samples according to human evaluations. Additionally, the model can perform text-driven image editing tasks beyond zero-shot image generation from text prompts. Text-driven image editing refers to editing existing images according to text prompts, such as changing attributes or objects within an image as directed by the text.

Despite its success, the \ac{GLIDE} model has some limitations. It fails to generate images for some complex or unusual text prompts. Moreover, the model's generation speed is slow, taking several seconds to generate one image on a flagship \ac{GPU}. Possible solutions to address these limitations include improving the model architecture, optimization techniques, and combining \ac{GLIDE} with faster \ac{GAN}-based methods.

\Acf{CLIP} refers to a model trained to determine if an image and text caption match. It consists of a transformer-based text encoder (see Section \ref{sec:transformers}) and a convolutional neural network-based image encoder (see Section \ref{sec:CNN}). The text encoder produces an embedding of the text, and the image encoder produces an embedding of the image. These embeddings are then compared, and during training, the model learns to produce similar embeddings for matching image-text pairs and dissimilar embeddings for mismatching pairs. This contrastive learning approach allowed CLIP to learn cross-modal understanding between text and images in an unsupervised manner. The pre-trained CLIP model can provide additional guidance to other text-to-image models, such as \ac{GLIDE}, by scoring how well-generated images match given text prompts.
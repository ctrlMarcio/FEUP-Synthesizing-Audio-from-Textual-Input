\paragraph{Linguistic Data Augmentation} \label{sec:text-augmentation}

Linguistic data transfiguration involves metamorphosing textual data to augment its diversity and quantity. This can ameliorate the performance and robustness of natural language processing models that rely on text data.   

For sonic milieu generation, linguistic data transfiguration provides an efficacious approach to creating more varied and verisimilitudinous soundscape descriptions from text. Nonetheless, various existing soundscape datasets discussed in Section \ref{sec:sol-datasets} contain categorical labels or tags instead of descriptive annotations.   

It is imperative to note that some linguistic data augmentation techniques only function when the original text is in natural language, while others can still be applied to categorical labels.

Variations in input text can facilitate the generation of soundscapes with more variability and legitimacy. The following sections cover specific linguistic data augmentation techniques and their applications for soundscape generation.

While linguistic data augmentation has several advantages, it poses some issues and challenges~\cite{shorten_text_2021}.

% Advantages
The main benefits of textual augmentation are as follows:
\begin{enumerate}
    \item It can reduce the costs of collecting and annotating textual data.
    \item It can improve the accuracy of models by increasing the training data size, alleviating data scarcity, mitigating overfitting, and creating variability in the data. 
    \item It can boost the generalizability of models by exposing them to different linguistic patterns and styles. 
    \item It can increase the robustness of our models by making them resilient against adversarial attacks that attempt to deceive them through expert alterations of the input sequences.  
\end{enumerate}

% Disadvantages
However, there are also some potential downsides:     
\begin{enumerate}   
    \item It can introduce noise or errors into the data that may impact the quality and readability of the augmented text.
    \item It can change or lose the original text's meaning, style, or complexity, mainly if the transformation is inappropriate or irrelevant to the task or domain.   
    \item It can be computationally expensive or time-consuming to generate high-quality and diverse augmented text, especially if it involves using external resources or models such as dictionaries, corpora, word embeddings, generative models, or translation models.
\end{enumerate}

Various frameworks have been developed for augmenting text data linguistically. These can be broadly grouped into symbolic and neural augmentation models.

\subparagraph{Symbolic Augmentation Models}

These methods employ rule-based transformations operating directly on the surface form of text via predetermined heuristics. They include:

\begin{itemize}     
  \item Rule-based augmentation replaces, inserts, or deletes tokens according to specified rules. An example is replacing named entities with alternatives \cite{wei_eda_2019}.   
   \item Graph-based augmentation uses graph structures to perturb the text, \textit{e.g.} swapping adjacent adjectives and nouns \cite{ahmed_text_2023}.  
  \item MixUp combines existing examples via interpolation to synthesize augmented instances, \textit{e.g.} combining ``The cat sat on the mat'' and  ``The dog lay on the rug'' to generate ``The cat lay on the mat'' \cite{guo_augmenting_2019}.
  \item Feature-based augmentation applies transformations to word embeddings, \textit{e.g.} adding noise to the embedding space \cite{cheung_modals_2021}.        
\end{itemize}   

Despite their interpretability, symbolic methods struggle with complex transformations.

\subparagraph{Neural Augmentation Models}

These techniques leverage deep neural networks and large language models. They encompass:   

\begin{itemize}    
   \item Back-translation, which translates text into another language and back, producing paraphrases, \textit{e.g.} translating ``The book was interesting.'' to French and back to English, yielding  ``The book was fascinating'' \cite{pham_meta_2021}.
  \item Generative augmentation employs generative language models to synthesize novel text, \textit{e.g.} using an \ac{LLM} such as GPT to rephrase sentences or simply fine-tune them.
\end{itemize} 

While more complex, neural methods can generate diverse and realistic augmented instances.

Both symbolic and neural augmentation aim to expose models to more variability during training, helping combat overfitting and improve performance. However, symbolic methods offer interpretability, while neural methods provide more flexibility and variation.

\begin{table}[ht]
\centering
\caption{A taxonomy of text augmentation methods for transformer language models according to their algorithmic properties and underlying approaches.}  
\begin{tabularx}{\textwidth}{|X|X|X|X|X|}   
\hline
  &\textbf{Interpretability} &\textbf{Algorithmic }   & \textbf{Capacity to} &\textbf{Paradigmatic} \\
 &\textbf{Transparency} & \textbf{Complexity}& \textbf{Leverage Labels} &\\[0.5ex] 
\hline
\textbf{Rule-based} & High &Relatively low &Incapable& Lexical substitution \\   
\hline
\textbf{Graph-based} &High & Moderate & Incapable&Knowledge graph-based\\       
\hline      
\textbf{Sample Combination} &Medium&Moderate & Capable& Embeddings summation\\
 \hline    
\textbf{Feature-based}&Medium &Moderate& Incapable &Noise injection\\     
\hline
\textbf{Generative}&Low &High &Capable &Text auto-generation\\    
\hline
\textbf{Back-translation}&Medium &High&Capable &Inverse translation\\             
\hline           
\end{tabularx}
\label{tab:textaugmethods}  
\end{table}

Table \ref{tab:textaugmethods} categorizes several common text augmentation strategies according to their transparency, complexity, dependence on labels, and paradigm.  

Regarding interpretability, rule-based and graph-based methods exhibit high transparency since they employ explicit symbolic transformations. In contrast, the stochastic nature of generative models and back-translation compromises their interpretability.

Computational complexity also differs. Rule-based and graph-based augmentation are relatively efficient since they apply explicit symbolic transformations. In comparison, training neural networks for generative modeling and back-translation requires more computational resources.

For leveraging labels, given that some datasets mentioned in Section \ref{sec:sol-datasets} contain categorical labels, and we desire descriptive annotations, it is pertinent to assess whether these techniques can transform categorical labels into text descriptions. Employing generative models or back-translation potentially enables this since the models attempt to make sense of the input and transform it into a sentence.
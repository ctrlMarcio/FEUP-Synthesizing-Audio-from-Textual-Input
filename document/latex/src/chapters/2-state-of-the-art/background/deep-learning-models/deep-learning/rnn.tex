\paragraph{Recurrent Neural Network (RNN) (1986)} \label{sec:rnn}

In 1986, Rumelhart and McClelland, psychologists, in the aftermath of the discovery of the backpropagation algorithm, wrote a book \cite{rumelhart_parallel_1986} where, between others, they took the declarations made in Perceptrons \cite{marvin_minsky_perceptrons_1969}, presented in Section \ref{sec:context}, and showed that, although the perceptron alone might not be enough, networks of perceptrons, with the suitable algorithms, might suffice.

One example they gave was a theoretical network that would be recursive \cite{rumelhart_learning_1987}. This would be the first time the ``recurrent'' term would appear in the literature. They explained that a recursive network is a feedforward network where some weights are kept the same between layers. They showed that it is easy to backpropagate the error and train such a network with this simplification. At the time, ``the major problem with this procedure is the memory required''. This new network was able to solve problems related to sequences. In this chapter, they showed the development of a system that would learn to complete straightforward sentences using \aclp{RNN}.

Furthermore, a \ac{RNN} used nowadays is not very different from the one theorized in 1986. A \ac{RNN} is a neural network derived from a feedforward neural network (see Section~\ref{sec:feedforward}), where some connections between nodes can create cycles. This is because some nodes' output is the same nodes' input. This operation makes the network have memory. These networks can exhibit temporal behavior and process variable-length sequence of inputs. In 1996 it was shown that \acp{RNN} could perform any operation that a regular computer can, meaning that they are Turing complete \cite{hyotyniemi_turing_1996}.

\Ac{RNN} have the problem of forgetting terms seen long ago as new terms come along~\cite{hochreiter_long_1997}. This may be a problem in, for instance, long texts where the model has to remember the name of a given entity that was given at the beginning of the text.

Learning happens as in feedforward neural networks with one additional step: the recurrent nodes are unrolled, as seen in Figure \ref{fig:rnn}.

\begin{figure}[!ht]
    \centering
    \ctikzfig{figures/2-sota/rnn}
    \caption[Simple recurrent neural network]{\textbf{Simple \acl{RNN}} --- The left side of the image displays the most straightforward representation of a \ac{RNN} because it only has one recurrent neuron. Apart from the bias, the neuron has an input weight, a weight for the recursive connection, and a weight for the output. The images on the left and the right are equivalent. The image on the right represents the same simple network. However, it shows what exactly happens when it receives multiple inputs: an output is generated for each one, and a value is passed to the next iteration. One may notice that even though the inputs and outputs change, the weights on the edges do not.}
    \label{fig:rnn}
\end{figure}

New takes that solve the problems with \acp{RNN} have been developed over the years. The basis of these models is to include learnable gates that choose what and when to remember and to forget. The most prominent examples are the \Acfp{GRU} \cite{cho_properties_2014}, and the \Acfp{LSTM} \cite{hochreiter_long_1997}.
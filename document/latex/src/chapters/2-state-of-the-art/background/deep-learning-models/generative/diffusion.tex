\paragraph{Diffusion Models} \label{sec:diffusion}

Until the proliferation of diffusion models, the architecture most used for data generation was the \ac{GAN} (Section \ref{sec:gan}). The problem is that \acp{GAN} are hard to train. For instance, mode collapse can happen. In mode collapse, the generator always generates the same data that fools the discriminator.

\textit{Diffusion models} \cite{sohl-dickstein_deep_2015} simplify this generation process into more intuitive small steps where the work of the network is lighter and is run multiple times. This is done by taking inspiration from non-equilibrium thermodynamics. These models define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise.

Practically, diffusion models use a Markov chain to gradually convert one distribution into another. This chain starts from a simple known distribution (\textit{e.g.} a Gaussian) into a target distribution using a diffusion process. Learning in this framework involves estimating small perturbations to a diffusion process, using a network such as the U-Net (Section \ref{sec:u-net}). Estimating small perturbations is more tractable than explicitly describing the whole distribution with a single, non-analytically-normalizable potential function. This process can be seen in Figure \ref{fig:diffusion}

\begin{figure}[ht]
    \centering
    \ctikzfig{figures/2-sota/diffusion}
    \caption[Diffusion model]{\textbf{Diffusion model} --- This illustration was based on \cite{ho_denoising_2020} and shows the process of applying Gaussian noise to an image sample through multiple steps $q(X_t|X_{t-1})$. The model will then learn the operation $p$ that transforms $X_t$ into $X_{t-1}$ with $p(X_{t-1}|X_t)$ and so on until $X_0$. At this point, the model has generated a new data sample.}
    \label{fig:diffusion}
\end{figure}

The ultimate goal is to define a forward (or inference) diffusion process which converts any complex data distribution into a simple, tractable distribution and then learn a finite-time reversal of this diffusion process which defines the generative model distribution \cite{sohl-dickstein_deep_2015}.

One problem is that one needs to decide how much noise one wants to increment per iteration. For instance, if one decides to train a network that directly learns to denoise full Gaussian to a real image, then one is simply training a \ac{GAN} generator. It is easier to remove a small amount of noise per iteration. The amount of noise added per iteration is a hyperparameter called a scheduler. For instance, one can add the same amount of noise per iteration, called the \textit{linear schedule}. Multiple schedules may have different impacts.

For instance, given a linear scheduler, one can define that for $t = x$, the sample would be the original one with $k = x \times 10$ random data points with Gaussian noise. This allows data generation in different timestamps without running through all timestamps. For instance, generating a data sample with $t = 5$ would be as easy as noising $k = 50$ random data points.

To train these networks, one would give pairs of the original data sample $X$ plus a data sample at a random timestamp $X_t$ plus the random step $t$, $X_t = X + N(t)$ where $N$ is a noising function. The network would learn to get the noise from the data given a timestamp. This means that the network would learn to predict $N(t)$ using image segmentation. This will not always be perfect, so the network learns to predict $\tilde{N(t)}$. Then, theoretically, by applying $X_t - \tilde{N(t)}$, one gets $\tilde{X}$, which should be as close as possible to $X$. This process for $t = 50$ is challenging, as most of the data is Gaussian noise. However, applying the process for, for instance, $t = 1$, should be quite easy.

For inference, one gets noisy data $X_t$ and a given timestamp $t$. Applying the network returns $\tilde{N(t)}$ as explained previously. By doing $\tilde{X} = X_t - \tilde{N(t)}$, one generates a bad data sample. But then, the algorithm takes $\tilde{X}$ and applies $N(t - 1)$. This results in another noisy data sample with less noise. This process loops until $t = 0$. By then, a new data sample is generated.
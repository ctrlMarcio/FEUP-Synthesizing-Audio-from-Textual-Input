 \subsection{Deep Learning Frameworks} \label{sec:dl-frameworks}

\Ac{DL} frameworks have revolutionized the field of \ac{AI}, enabling researchers and practitioners to efficiently develop and deploy complex neural networks for the multiple \ac{DL} tasks. These frameworks provide a wide range of tools and techniques for building, training, and evaluating deep neural networks and have significantly accelerated the pace of progress in the field. This section explores some of the most popular \ac{DL} frameworks, their key features and capabilities, and how they have been used to develop state-of-the-art generative \ac{AI} models for audio synthesis from textual input.

Several \ac{DL} frameworks are available, and they differ in several ways, including their programming languages, ease of use, and performance. However, to the best of the author's knowledge, there is no recent study on the performance of today's \ac{DL} networks. The most recent is from 2017 \cite{parvat_survey_2017}.	

\subsubsection{TensorFlow} \label{sec:tensorflow}

Developed by Google, TensorFlow \cite{martin_abadi_tensorflow_2015} is one of the most widely used \ac{DL}. It supports both \ac{CPU} and \ac{GPU} computations and provides a variety of \acp{API} for building different types of neural networks. TensorFlow is written in Python, but its core functionality is implemented in C++ for optimal performance.

TensorFlow is an interface for expressing \ac{ML} algorithms and an implementation for executing them. It allows computations to be executed with little or no change on various systems, from mobile devices to large-scale distributed systems. The system is flexible and can express many different algorithms, including training and inference algorithms for deep neural network models.

TensorFlow can be used with various programming languages, including Python, JavaScript, C++, and Java. Python is the recommended language for TensorFlow, but other languages' \acp{API} may offer some performance advantages. Other languages like Julia, R, Haskell, and others have bindings.

\subsubsection{PyTorch} \label{sec:pytorch}

PyTorch is an open-source \ac{DL} framework developed by Facebook \cite{paszke_pytorch_2019}. It has gained popularity due to its ease of use and dynamic computation graph, which allows for more flexible and intuitive programming. PyTorch also supports \ac{CPU} and \ac{GPU} computations, and although it supports Python and C++, it has a Python-first approach, making it easy to integrate with other Python libraries.

PyTorch is a popular deep-learning framework that is easy to use and learn. It has a simple and intuitive \ac{API} that makes it easy to learn and use. PyTorch is also flexible and can be used for various applications.

\subsubsection{Keras} \label{sec:keras}

Keras is a Python library for building and training neural network models at a high-level. It offers a user-friendly interface, and it is commonly used for \ac{DL} purposes~\cite{chollet_keras_2015}. Keras is built on top of lower-level libraries such as TensorFlow (see Section~\ref{sec:tensorflow}). It simplifies the creation and training of neural networks by abstracting away many low-level details.
Keras enables fast neural network model creation by assembling pre-built building block layers. These building blocks consist of input layers, \ac{CNN} layers, \ac{RNN} layers, fully connected layers, activation functions, and other components.

Keras is often regarded as more user-friendly than TensorFlow, as it offers a high-level interface that hides many low-level details.

Keras provides a simplified \ac{API} for constructing and training deep learning models. Keras includes a variety of utilities for manipulating data, such as data preprocessing, data augmentation, and data visualization, facilitating data manipulation.

\subsubsection{Conclusions on Deep Learning Frameworks} \label{sec:dl-frameworks-conclusions}

Selecting a \ac{DL} framework is critical in developing a \ac{DL} project. It is vital for the development of this thesis that the best framework that can offer flexibility, ease of use, and optimal performance of the \ac{DL} models is selected.

PyTorch uses a dynamic computation graph, which is created for each iteration in an epoch. In each iteration, the code executes the forward pass, computes the derivatives of output \textit{w.r.t} to the network parameters, and updates the parameters to fit the given examples. After doing the backward pass, the graph is freed to save memory. A dynamic graph can be changed on the fly, allowing for more freedom, easier debugging, and easier experimentation with architectures and hyperparameters during the model development process. Tensorflow, on the other hand, uses a static graph. There, the library creates and connects all the variables at the beginning and initializes them into a static (unchanging) session. This session and graph persist and are reused: it is not rebuilt after each iteration of training, making it more efficient and restrictive.

Furthermore, PyTorch offers a more straightforward and intuitive \ac{API} compared to TensorFlow. This feature makes it easier for developers to write and debug code. Additionally, PyTorch offers more flexibility when creating custom layers and functions, which is impossible in Keras. This flexibility enables developers to create more complex and innovative models, which can lead to better performance. Keras offers a straightforward but too simple network, while low-level Tensorflow offers a complicated and convoluted \ac{API} making it challenging to focus on the real problem. PyTorch offers a good balance between simplicity and features.

One of the significant drawbacks of Keras is the need for more flexibility when customizing \ac{DL} models. Keras offers a limited set of pre-defined layers, making it difficult for developers to create complex custom layers and functions. This lack of flexibility limits the ability to make changes to the architecture of a network during the development process, which can hinder the performance of the final model. When one needs more custom layers, one has to resort to the Tensorflow jungle, making the development a hassle.

Both PyTorch and TensorFlow have good hardware support. However, PyTorch has a feature that distinguishes it from TensorFlow: data parallelism. PyTorch optimizes performance by using native support for asynchronous execution from Python. In TensorFlow, one has to manually code and fine-tune every operation to be run on a specific device to allow distributed training. Running on top of TensorFlow, Keras presents the same hardware problems as the latter.

These conclusions are in table \ref{tab:dl-frameworks}. Given all of this, and considering that ease of use, debugging, and high customization are essential for this work, the clear choice is PyTorch. One should consider its use when this work uses practical terms.

\begin{table}[h]
\centering
\caption{Comparison of PyTorch, Raw TensorFlow, and Keras}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Feature} & \textbf{PyTorch} & \textbf{Raw TensorFlow} & \textbf{Keras} \\
\hline
Graph computation & Dynamic & Static & Static \\
Ease of use & Moderate & Difficult & Easy \\
Debugging & Good & Difficult & Moderate \\
Customization & High & High & Moderate \\
Hardware support & Good & Moderate & Moderate \\
\hline
\end{tabular}
\label{tab:dl-frameworks}
\end{table}

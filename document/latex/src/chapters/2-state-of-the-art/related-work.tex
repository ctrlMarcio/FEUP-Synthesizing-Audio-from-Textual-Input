\section{Related Work} \label{sec:related-work}

Traditionally, sound designers relied on manual labor to create audio, which involved recording and editing real-world sounds, mixing, and adding sound effects~\cite{sonnenschein_sound_2001}. Creating high-quality sounds is challenging, costly, and time-consuming, requiring specialized skills and resources. Hence, it engenders a notable impediment to the creation of soundscapes or any type of sound at scale~\cite{bernardes_seed_2016, strobl_sound_2006}, namely in light of its growing popularity and consumption within podcasts, movies, and video games.

Consumer data reported in 2021 showed compelling evidence regarding the listening habits of individuals within the United States of America. The findings indicate a substantial growth in podcast listenership over the past decade, with 41\% of Americans aged 12 or older having engaged with podcasts in the preceding month and 28\% within the last week. Moreover, at the beginning of the same year, a notable 68\% of Americans aged 12 and above had indulged in online audio consumption within the previous month, while 62\% had done so within the preceding week~\cite{research_infinite_2021}.

To overcome the aforementioned limitations, algorithmic audio generation has emerged as a promising solution that streamlines its creation altogether. Focusing on soundscapes, preceding 2018, prevailing models for their generation primarily revolved around statistical methods, featuring prominent employment of \ac{ML} techniques with feature engineering. For a comprehensive overview of techniques employed before the era of \ac{DL}, reference can be made to the review papers by Alias et al.\cite{alias_review_2016} and Kalonaris et al.\cite{kalonaris_computational_2018}. Noteworthy efforts at the feature engineering level are exemplified by Fernandez et al.~\cite{fernandez_ai_2013}, who represent sounds as high-level features, such as musical sheets, as an approach to generating musical sounds.

\Ac{DL} models for audio generation aim to produce high-quality audio signals by learning from existing audio data. Typically, these models consist of three main components: translation of the sound signal into a compressed representation, generation of a new representation from previous data, and translation back into an audio signal.

The first component of the model involves transforming the original sound signal into a Mel-spectrogram (or other) representation (see Section~\ref{sec:sound}), which is more compact and more accessible to process than the raw audio signal.

The second component involves generating new low-resolution representations from previous representations, such as spectrograms or feature vectors \cite{kong_hifi-gan_2020}. This is typically done using deep generative architectures. These models are trained on existing sound data to learn the target representation distribution and generate new, high-quality representations.

The final component of the model involves translating these representations back into an audio signal. These algorithms are called \textit{vocoders} (for more, see Section~\ref{sec:vocoders}). This component aims to produce high-quality audio signals that closely resemble the original sound data used to train the model.

The purpose of this section is to review the related work in this area. First, it examines the methods for generating traditional soundscapes, as described in Section~\ref{sec:trad-soundscape}. It then examines sound generation using machine learning. State-of-the-art models focus primarily on either using unsupervised sound generation techniques, as discussed in Section~\ref{sec:unsupervised-generation}, or generating sounds from internal representations, known as vocoders and discussed in Section~\ref{sec:vocoders}, or creating an end-to-end system, as discussed in Section~\ref{sec:end-to-end}.

\input{src/chapters/2-state-of-the-art/related-work/traditional-soundscape}
\input{src/chapters/2-state-of-the-art/related-work/unsupervised-generation}
\input{src/chapters/2-state-of-the-art/related-work/vocoders}
\input{src/chapters/2-state-of-the-art/related-work/end-to-end}
\section{Model Requirements and Design} \label{sec:model}

A model that solves the end-to-end problem aims to transform a textual prompt into a sound.

A model to solve this problem must be able to extract a representation of latent features of the given prompt. Then, it must condition a given generator on this representation. This generator must create the sound itself or create a lower-level sound representation. If the latter occurs, another piece must transform this representation into a raw sound, a vocoder.

The generator model needs to be trained with the text embeddings first. So, let us follow the example of training for a single sample: a pair of a sound and the respective natural language label. First, one must translate the sound into a lower-level feature representation. Second, one must have a model that translates the label into a latent features representation, this is, into a text embedding vector. Then, a model that generates a sound representation must be conditioned on these embeddings. Finally, the results of this model must be compared with the lower-level representation of the training sample. This process would train the generator of sound representations. A second process of training the vocoder could be done separately. Other architectures are possible. This simple architectural example would scale well because the two most difficult parts, the generator, and the vocoder, can be trained in parallel and do not depend on one another. Further discussion on this topic is presented in section \ref{sec:sol-models}.

For this architecture to be successful, at least three models need to be trained: the text embedding, the lower-level sound generation, and the vocoder. Vocoders already exist and are a field of relatively intense development, as shown in \ref{sec:vocoders}. Also, text embedding can be already done very well, as \ref{sec:text-embedding} shows. This simplifies the problem as, theoretically, only the generator needs to be developed and trained. The possibility of focusing on this generator is an asset because it eases the workload. However, for best results, one should develop all the models or at least fine-tune them to the specific data that one is working with.
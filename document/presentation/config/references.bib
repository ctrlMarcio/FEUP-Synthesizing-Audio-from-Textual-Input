
@incollection{huzaifah_deep_2021,
  address   = {Cham},
  title     = {Deep {Generative} {Models} for {Musical} {Audio} {Synthesis}},
  isbn      = {978-3-030-72116-9},
  url       = {https://doi.org/10.1007/978-3-030-72116-9_22},
  abstract  = {Sound modelling is the process of developing algorithms that generate sound under parametric control.},
  booktitle = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
  publisher = {Springer International Publishing},
  author    = {Huzaifah, Muhammad and Wyse, Lonce},
  editor    = {Miranda, Eduardo Reck},
  year      = {2021},
  doi       = {10.1007/978-3-030-72116-9_22},
  pages     = {639--678},
  file      = {Full Text:/Users/marcio/Zotero/storage/I5J5VUEX/Huzaifah and Wyse - 2021 - Deep Generative Models for Musical Audio Synthesis.pdf:application/pdf}
}

@inproceedings{kumar_melgan_2019,
  title     = {{MelGAN}: {Generative} {Adversarial} {Networks} for {Conditional} {Waveform} {Synthesis}},
  volume    = {32},
  url       = {https://proceedings.neurips.cc/paper/2019/file/6804c9bca0a615bdb9374d00a9fcba59-Paper.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brébisson, Alexandre and Bengio, Yoshua and Courville, Aaron C},
  editor    = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
  year      = {2019},
  file      = {Full Text:/Users/marcio/Zotero/storage/CTF59XSN/Kumar et al. - 2019 - MelGAN Generative Adversarial Networks for Condit.pdf:application/pdf}
}

@inproceedings{tahiroglu_-terity_2020,
  series    = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
  title     = {Al-terity: {Non}-{Rigid} {Musical} {Instrument} with {Artificial} {Intelligence} {Applied} to {Real}-{Time} {Audio} {Synthesis}},
  abstract  = {A deformable musical instrument can take numerous dis- tinct shapes with its non-rigid features. Building audio syn- thesis module for such an interface behaviour can be chal- lenging. In this paper, we present the Al-terity, a non-rigid musical instrument that comprises a deep learning model with generative adversarial network architecture and use it for generating audio samples for real-time audio synthesis. The particular deep learning model we use for this instru- ment was trained with existing data set as input for pur- poses of further experimentation. The main benefits of the model used are the ability to produce the realistic range of timbre of the trained data set and the ability to generate new audio samples in real-time, in the moment of playing, with the characteristics of sounds that the performer ever heard before. We argue that these advanced intelligence features on the audio synthesis level could allow us to ex- plore performing music with particular response features that define the instrument’s digital idiomaticity and allow us reinvent the instrument in the act of music performance.},
  language  = {English},
  booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
  publisher = {International Conference on New Interfaces for Musical Expression},
  author    = {Tahiroğlu, Koray and Kastemaa, Miranda and Koli, Oskar},
  month     = jul,
  year      = {2020},
  keywords  = {Artificial Intelligence (AI), deep learning, GAN, NIME, SOPI},
  pages     = {337--342},
  file      = {Proceedings of the International Conference on New.pdf:/Users/marcio/Zotero/storage/H3LYBRT6/Proceedings of the International Conference on New.pdf:application/pdf}
}

@inproceedings{kong_hifi-gan_2020,
  title     = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for {Efficient} and {High} {Fidelity} {Speech} {Synthesis}},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  editor    = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year      = {2020},
  pages     = {17022--17033},
  file      = {Full Text:/Users/marcio/Zotero/storage/HT7ZGKEE/Kong et al. - 2020 - HiFi-GAN Generative Adversarial Networks for Effi.pdf:application/pdf}
}

@article{kim_flowavenet_2018,
  title   = {{FloWaveNet} : {A} {Generative} {Flow} for {Raw} {Audio}},
  volume  = {abs/1811.02155},
  url     = {http://arxiv.org/abs/1811.02155},
  journal = {CoRR},
  author  = {Kim, Sungwon and Lee, Sang-gil and Song, Jongyoon and Yoon, Sungroh},
  year    = {2018},
  note    = {arXiv: 1811.02155},
  file    = {Full Text:/Users/marcio/Zotero/storage/R2N5I8BU/Kim et al. - 2018 - FloWaveNet  A Generative Flow for Raw Audio.pdf:application/pdf}
}

@misc{douwes_energy_2021,
  title     = {Energy {Consumption} of {Deep} {Generative} {Audio} {Models}},
  url       = {http://arxiv.org/abs/2107.02621},
  doi       = {10.48550/arXiv.2107.02621},
  abstract  = {In most scientific domains, the deep learning community has largely focused on the quality of deep generative models, resulting in highly accurate and successful solutions. However, this race for quality comes at a tremendous computational cost, which incurs vast energy consumption and greenhouse gas emissions. At the heart of this problem are the measures that we use as a scientific community to evaluate our work. In this paper, we suggest relying on a multi-objective measure based on Pareto optimality, which takes into account both the quality of the model and its energy consumption. By applying our measure on the current state-of-the-art in generative audio models, we show that it can drastically change the significance of the results. We believe that this type of metric can be widely used by the community to evaluate their work, while putting computational cost -- and in fine energy consumption -- in the spotlight of deep learning research.},
  urldate   = {2022-10-26},
  publisher = {arXiv},
  author    = {Douwes, Constance and Esling, Philippe and Briot, Jean-Pierre},
  month     = oct,
  year      = {2021},
  note      = {arXiv:2107.02621 [cs, eess]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  annote    = {Comment: 5 pages, 2 figures, ICASSP 2022},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/Q78FAEKM/Douwes et al. - 2021 - Energy Consumption of Deep Generative Audio Models.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/F6TZNRBA/2107.html:text/html}
}

@misc{oord_wavenet_2016,
  title      = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
  shorttitle = {{WaveNet}},
  url        = {http://arxiv.org/abs/1609.03499},
  doi        = {10.48550/arXiv.1609.03499},
  abstract   = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  urldate    = {2022-10-29},
  publisher  = {arXiv},
  author     = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  month      = sep,
  year       = {2016},
  note       = {arXiv:1609.03499 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/P6KZR7GL/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/VLWC73AW/1609.html:text/html}
}

@misc{engel_gansynth_2019,
  title      = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
  shorttitle = {{GANSynth}},
  url        = {http://arxiv.org/abs/1902.08710},
  doi        = {10.48550/arXiv.1902.08710},
  abstract   = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
  urldate    = {2022-10-29},
  publisher  = {arXiv},
  author     = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
  month      = apr,
  year       = {2019},
  note       = {arXiv:1902.08710 [cs, eess, stat]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
  annote     = {Comment: Colab Notebook: http://goo.gl/magenta/gansynth-demo},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/R9AC2KN4/Engel et al. - 2019 - GANSynth Adversarial Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/TJKV9U8R/1902.html:text/html}
}

@article{ghahramani_probabilistic_2015,
  title     = {Probabilistic machine learning and artificial intelligence},
  volume    = {521},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature14541},
  doi       = {10.1038/nature14541},
  abstract  = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
  language  = {en},
  number    = {7553},
  urldate   = {2022-11-08},
  journal   = {Nature},
  author    = {Ghahramani, Zoubin},
  month     = may,
  year      = {2015},
  note      = {Number: 7553
               Publisher: Nature Publishing Group},
  keywords  = {Computer science, Mathematics and computing, Neuroscience},
  pages     = {452--459},
  file      = {Full Text PDF:/Users/marcio/Zotero/storage/5A3M7EVZ/Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/YEHX2HD9/nature14541.html:text/html}
}

@misc{elsea_basics_1996,
  title      = {Basics of {Digital} {Recording}},
  shorttitle = {Digital {Recording}},
  url        = {http://artsites.ucsc.edu/EMS/music/tech_background/TE-16/teces_16.html},
  urldate    = {2022-11-09},
  journal    = {Univeristy of California, Santa Cruz},
  author     = {Elsea, Peter},
  year       = {1996},
  file       = {Digital Recording:/Users/marcio/Zotero/storage/7ACP8LT9/teces_16.html:text/html;Elsea - 1996 - Basics of Digital Recording.pdf:/Users/marcio/Zotero/storage/AF8HAT9B/Elsea - 1996 - Basics of Digital Recording.pdf:application/pdf}
}

@misc{university_of_york_what_nodate,
  title   = {What is {Computer} {Science}?},
  url     = {https://www.cs.york.ac.uk/undergraduate/what-is-cs/},
  urldate = {2023-01-01},
  journal = {Computer Science - Computer Science, University of York},
  author  = {{University of York}},
  file    = {What is Computer Science? - Computer Science, University of York:/Users/marcio/Zotero/storage/S2CMJYK7/what-is-cs.html:text/html}
}

@book{mitchell_machine_1997,
  address   = {New York},
  series    = {{McGraw}-{Hill} series in computer science},
  title     = {Machine {Learning}},
  isbn      = {978-0-07-042807-2},
  language  = {en},
  publisher = {McGraw-Hill},
  author    = {Mitchell, Tom M.},
  year      = {1997},
  keywords  = {Computer algorithms, Machine learning},
  file      = {Mitchell - 1997 - Machine Learning.pdf:/Users/marcio/Zotero/storage/HAPYJGNN/Mitchell - 1997 - Machine Learning.pdf:application/pdf}
}

@article{schulz_deep_2012,
  title    = {Deep {Learning}},
  volume   = {26},
  issn     = {1610-1987},
  url      = {https://doi.org/10.1007/s13218-012-0198-z},
  doi      = {10.1007/s13218-012-0198-z},
  abstract = {Hierarchical neural networks for object recognition have a long history. In recent years, novel methods for incrementally learning a hierarchy of features from unlabeled inputs were proposed as good starting point for supervised training. These deep learning methods—together with the advances of parallel computers—made it possible to successfully attack problems that were not practical before, in terms of depth and input size. In this article, we introduce the reader to the basic concepts of deep learning, discuss selected methods in detail, and present application examples from computer vision and speech recognition.},
  language = {en},
  number   = {4},
  urldate  = {2023-01-01},
  journal  = {KI - Künstliche Intelligenz},
  author   = {Schulz, Hannes and Behnke, Sven},
  month    = nov,
  year     = {2012},
  keywords = {Hierarchical feature learning, Object categorization, Unsupervised learning},
  pages    = {357--363},
  file     = {Schulz and Behnke - 2012 - Deep Learning.pdf:/Users/marcio/Zotero/storage/TX442LC2/Schulz and Behnke - 2012 - Deep Learning.pdf:application/pdf}
}

@misc{bishop_mixture_1994,
  type     = {Monograph},
  title    = {Mixture density networks},
  url      = {https://publications.aston.ac.uk/id/eprint/373/},
  abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the effectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.},
  language = {en-GB},
  urldate  = {2023-02-02},
  author   = {Bishop, Christopher M.},
  year     = {1994},
  note     = {Num Pages: 26
              Place: Birmingham
              Publisher: Aston University},
  file     = {Full Text PDF:/Users/marcio/Zotero/storage/7WPXXDGE/Bishop - 1994 - Mixture density networks.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/QC94TP64/373.html:text/html}
}

@article{fradkov_early_2020,
  series   = {21st {IFAC} {World} {Congress}},
  title    = {Early {History} of {Machine} {Learning}},
  volume   = {53},
  issn     = {2405-8963},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896320325027},
  doi      = {10.1016/j.ifacol.2020.12.1888},
  abstract = {Machine learning belongs to the crossroad of cybernetics (control science) and computer science. It is attracting recently an overwhelming interest, both of professionals and of the general public. In the talk a brief overview of the historical development of the machine learning field with a focus on the development of mathematical apparatus in its first decades is provided. A number of little-known facts published in hard to reach sources are presented.},
  language = {en},
  number   = {2},
  urldate  = {2023-02-02},
  journal  = {IFAC-PapersOnLine},
  author   = {Fradkov, Alexander L.},
  month    = jan,
  year     = {2020},
  keywords = {Convex Optimization, Machine Learning, Neural Networks, Separation Theorems},
  pages    = {1385--1390},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/MWAGJ6WW/Fradkov - 2020 - Early History of Machine Learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/8QVUQ6EY/S2405896320325027.html:text/html}
}

@article{rosenblatt_perceptron_1958,
  title      = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
  volume     = {65},
  issn       = {1939-1471, 0033-295X},
  shorttitle = {The perceptron},
  url        = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
  doi        = {10.1037/h0042519},
  language   = {en},
  number     = {6},
  urldate    = {2023-02-02},
  journal    = {Psychological Review},
  author     = {Rosenblatt, F.},
  year       = {1958},
  pages      = {386--408},
  file       = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:/Users/marcio/Zotero/storage/L4GHFAJT/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf}
}

@book{marvin_minsky_perceptrons_1969,
  title      = {Perceptrons: an introduction to computational geometry},
  isbn       = {0 262 13043 2},
  shorttitle = {Perceptrons},
  language   = {English},
  author     = {{Marvin Minsky} and {Seymour Papert}},
  year       = {1969}
}

@inproceedings{dean_mapreduce_2004,
  address   = {San Francisco, CA},
  title     = {{MapReduce}: {Simplified} {Data} {Processing} on {Large} {Clusters}},
  booktitle = {{OSDI}'04: {Sixth} {Symposium} on {Operating} {System} {Design} and {Implementation}},
  author    = {Dean, Jeffrey and Ghemawat, Sanjay},
  year      = {2004},
  pages     = {137--150}
}

@article{rumelhart_learning_1986,
  title    = {Learning {Representations} by {Back}-propagating {Errors}},
  volume   = {323},
  url      = {http://www.nature.com/articles/323533a0},
  doi      = {10.1038/323533a0},
  number   = {6088},
  journal  = {Nature},
  author   = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year     = {1986},
  keywords = {imported},
  pages    = {533--536},
  file     = {Rumelhart.backprop.nature.pdf:/Users/marcio/Zotero/storage/6SDQ5MJK/Rumelhart.backprop.nature.pdf:application/pdf}
}

@misc{ramesh_zero-shot_2021,
  title     = {Zero-{Shot} {Text}-to-{Image} {Generation}},
  url       = {http://arxiv.org/abs/2102.12092},
  doi       = {10.48550/arXiv.2102.12092},
  abstract  = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  urldate   = {2023-02-02},
  publisher = {arXiv},
  author    = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  month     = feb,
  year      = {2021},
  note      = {arXiv:2102.12092 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/49IMMPP5/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/7AA54J68/2102.html:text/html}
}

@misc{ramesh_hierarchical_2022,
  title     = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
  url       = {http://arxiv.org/abs/2204.06125},
  doi       = {10.48550/arXiv.2204.06125},
  abstract  = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  urldate   = {2023-02-02},
  publisher = {arXiv},
  author    = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  month     = apr,
  year      = {2022},
  note      = {arXiv:2204.06125 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/QPKNXP69/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/UE5DRLFE/2204.html:text/html}
}

@misc{brown_language_2020,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2005.14165},
  doi       = {10.48550/arXiv.2005.14165},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate   = {2023-02-02},
  publisher = {arXiv},
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month     = jul,
  year      = {2020},
  note      = {arXiv:2005.14165 [cs]},
  keywords  = {Computer Science - Computation and Language},
  annote    = {Comment: 40+32 pages},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/U6VI47C8/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/HNK4MK4Y/2005.html:text/html}
}

@incollection{rumelhart_learning_1987,
  title     = {Learning {Internal} {Representations} by {Error} {Propagation}},
  isbn      = {978-0-262-29140-8},
  url       = {https://ieeexplore.ieee.org/document/6302929},
  abstract  = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
  urldate   = {2023-02-04},
  booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
  publisher = {MIT Press},
  author    = {Rumelhart, David E. and McClelland, James L.},
  year      = {1987},
  note      = {Conference Name: Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
  pages     = {318--362},
  file      = {Chap8_PDP86.pdf:/Users/marcio/Zotero/storage/S83BNYE8/Chap8_PDP86.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/KP5LD8CN/6302929.html:text/html}
}

@book{rumelhart_parallel_1986,
  title     = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Vol}. 1: {Foundations}},
  publisher = {MIT Press},
  author    = {Rumelhart, D. E. and McClelland, J. L.},
  year      = {1986},
  keywords  = {imported}
}

@inproceedings{hyotyniemi_turing_1996,
  title     = {Turing machines are recurrent neural networks},
  language  = {English},
  booktitle = {{STeP} '96 - {Genes}, {Nets} and {Symbols}; {Finnish} {Artificial} {Intelligence} {Conference}, {Vaasa}, {Finland}, 20-23 {August} 1996},
  publisher = {University of Vaasa, Finnish Artificial Intelligence Society (FAIS)},
  author    = {Hyötyniemi, H.},
  editor    = {Alander, J. and Honkela, T.},
  year      = {1996},
  keywords  = {recurrent networks, Turing machines},
  pages     = {13--24}
}

@book{hebb_organization_1949,
  address   = {New York},
  title     = {The organization of behavior: {A} neuropsychological theory},
  isbn      = {0-8058-4300-0},
  abstract  = {Donald Hebb pioneered many current themes in behavioural neuroscience. He saw psychology as a biological science, but one in which the organization of behaviour must remain the central concern. Through penetrating theoretical concepts, including the "cell assembly," "phase sequence," and "Hebb synapse," he offered a way to bridge the gap between cells, circuits and behaviour. He saw the brain as a dynamically organized system of multiple distributed parts, with roots that extend into foundations of development and evolutionary heritage. He understood that behaviour, as brain, can be sliced at various levels and that one of our challenges is to bring these levels into both conceptual and empirical register. He could move between theory and fact with an ease that continues to inspire both students and professional investigators. Although facts continue to accumulate at an accelerating rate in both psychology and neuroscience, and although these facts continue to force revision in the details of Hebb's earlier contributions, his overall insistence that we look at behaviour and brain together â within a dynamic, relational and multilayered framework â remains. His work touches upon current studies of population coding, contextual factors in brain representations, synaptic plasticity, developmental construction of brain/behaviour relations, clinical syndromes, deterioration of performance with age and disease, and the formal construction of connectionist models. The collection of papers in this volume represent these and related themes that Hebb inspired. We also acknowledge our appreciation for Don Hebb as teacher, colleague and friend.},
  publisher = {Wiley},
  author    = {Hebb, Donald O.},
  month     = jun,
  year      = {1949},
  note      = {Published: Hardcover},
  keywords  = {MSc checked network neural seminal}
}

@article{rosenblatt_perceptron_1958-1,
  title    = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
  volume   = {65},
  issn     = {0033-295X},
  url      = {http://dx.doi.org/10.1037/h0042519},
  doi      = {10.1037/h0042519},
  number   = {6},
  journal  = {Psychological Review},
  author   = {Rosenblatt, F.},
  year     = {1958},
  keywords = {imported},
  pages    = {386--408}
}

@article{cybenko_approximation_1989,
  title    = {Approximation by superpositions of a sigmoidal function},
  volume   = {2},
  issn     = {0932-4194},
  url      = {http://dx.doi.org/10.1007/BF02551274},
  doi      = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  number   = {4},
  journal  = {Mathematics of Control, Signals, and Systems (MCSS)},
  author   = {Cybenko, G.},
  month    = dec,
  year     = {1989},
  note     = {Publisher: Springer London},
  keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
  pages    = {303--314}
}

@article{hubel_receptive_1959,
  title   = {Receptive fields of single neurones in the cat's striate cortex},
  volume  = {148},
  issn    = {0022-3751},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
  number  = {3},
  urldate = {2023-02-05},
  journal = {The Journal of Physiology},
  author  = {Hubel, D. H. and Wiesel, T. N.},
  month   = oct,
  year    = {1959},
  pmid    = {14403679},
  pmcid   = {PMC1363130},
  pages   = {574--591},
  file    = {PubMed Central Full Text PDF:/Users/marcio/Zotero/storage/2CECLBPR/Hubel and Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf}
}

@misc{gregor_deep_2014,
  title     = {Deep {AutoRegressive} {Networks}},
  url       = {http://arxiv.org/abs/1310.8499},
  doi       = {10.48550/arXiv.1310.8499},
  abstract  = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
  month     = may,
  year      = {2014},
  note      = {arXiv:1310.8499 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  annote    = {Comment: Appears in Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China, 2014},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/5XTVJB7V/Gregor et al. - 2014 - Deep AutoRegressive Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/5YFXYBFR/1310.html:text/html}
}

@misc{kingma_auto-encoding_2022,
  title     = {Auto-{Encoding} {Variational} {Bayes}},
  url       = {http://arxiv.org/abs/1312.6114},
  doi       = {10.48550/arXiv.1312.6114},
  abstract  = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Kingma, Diederik P. and Welling, Max},
  month     = dec,
  year      = {2022},
  note      = {arXiv:1312.6114 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  annote    = {Comment: Fixes a typo in the abstract, no other changes},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/6S7FJVH6/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/7IZVF8EH/1312.html:text/html}
}

@misc{goodfellow_generative_2014,
  title     = {Generative {Adversarial} {Networks}},
  url       = {http://arxiv.org/abs/1406.2661},
  doi       = {10.48550/arXiv.1406.2661},
  abstract  = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month     = jun,
  year      = {2014},
  note      = {arXiv:1406.2661 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/LR79KKU4/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/AP83RL62/1406.html:text/html}
}

@misc{ho_denoising_2020,
  title     = {Denoising {Diffusion} {Probabilistic} {Models}},
  url       = {http://arxiv.org/abs/2006.11239},
  doi       = {10.48550/arXiv.2006.11239},
  abstract  = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  month     = dec,
  year      = {2020},
  note      = {arXiv:2006.11239 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/JVYFISN4/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/MYU4AUYU/2006.html:text/html}
}

@misc{sohl-dickstein_deep_2015,
  title     = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
  url       = {http://arxiv.org/abs/1503.03585},
  doi       = {10.48550/arXiv.1503.03585},
  abstract  = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  month     = nov,
  year      = {2015},
  note      = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/NXL8WFAS/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/2DW6BUXF/1503.html:text/html}
}

@article{weng_what_2021,
  title   = {What are diffusion models?},
  url     = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  journal = {lilianweng.github.io},
  author  = {Weng, Lilian},
  month   = jul,
  year    = {2021}
}

@article{abayomi-alli_data_2022,
  title      = {Data {Augmentation} and {Deep} {Learning} {Methods} in {Sound} {Classification}: {A} {Systematic} {Review}},
  volume     = {11},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2079-9292},
  shorttitle = {Data {Augmentation} and {Deep} {Learning} {Methods} in {Sound} {Classification}},
  url        = {https://www.mdpi.com/2079-9292/11/22/3795},
  doi        = {10.3390/electronics11223795},
  abstract   = {The aim of this systematic literature review (SLR) is to identify and critically evaluate current research advancements with respect to small data and the use of data augmentation methods to increase the amount of data available for deep learning classifiers for sound (including voice, speech, and related audio signals) classification. Methodology: This SLR was carried out based on the standard SLR guidelines based on PRISMA, and three bibliographic databases were examined, namely, Web of Science, SCOPUS, and IEEE Xplore. Findings. The initial search findings using the variety of keyword combinations in the last five years (2017–2021) resulted in a total of 131 papers. To select relevant articles that are within the scope of this study, we adopted some screening exclusion criteria and snowballing (forward and backward snowballing) which resulted in 56 selected articles. Originality: Shortcomings of previous research studies include the lack of sufficient data, weakly labelled data, unbalanced datasets, noisy datasets, poor representations of sound features, and the lack of effective augmentation approach affecting the overall performance of classifiers, which we discuss in this article. Following the analysis of identified articles, we overview the sound datasets, feature extraction methods, data augmentation techniques, and its applications in different areas in the sound classification research problem. Finally, we conclude with the summary of SLR, answers to research questions, and recommendations for the sound classification task.},
  language   = {en},
  number     = {22},
  urldate    = {2023-02-05},
  journal    = {Electronics},
  author     = {Abayomi-Alli, Olusola O. and Damaševičius, Robertas and Qazi, Atika and Adedoyin-Olowe, Mariam and Misra, Sanjay},
  month      = jan,
  year       = {2022},
  note       = {Number: 22
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {deep learning, audio data, data augmentation, feature extraction, sound data},
  pages      = {3795},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/VMPKB9MI/Abayomi-Alli et al. - 2022 - Data Augmentation and Deep Learning Methods in Sou.pdf:application/pdf}
}

@article{mushtaq_environmental_2020,
  title    = {Environmental sound classification using a regularized deep convolutional neural network with data augmentation},
  volume   = {167},
  issn     = {0003-682X},
  url      = {https://www.sciencedirect.com/science/article/pii/S0003682X2030493X},
  doi      = {10.1016/j.apacoust.2020.107389},
  abstract = {The adoption of the environmental sound classification (ESC) tasks increases very rapidly over recent years due to its broad range of applications in our daily routine life. ESC is also known as Sound Event Recognition (SER) which involves the context of recognizing the audio stream, related to various environmental sounds. Some frequent and common aspects like non-uniform distance between acoustic source and microphone, the difference in the framework, presence of numerous sounds sources in audio recordings and overlapping various sound events make this ESC problem much complex and complicated. This study is to employ deep convolutional neural networks (DCNN) with regularization and data enhancement with basic audio features that have verified to be efficient on ESC tasks. In this study, the performance of DCNN with max-pooling (Model-1) and without max-pooling (Model-2) function are examined. Three audio attribute extraction techniques, Mel spectrogram (Mel), Mel Frequency Cepstral Coefficient (MFCC) and Log-Mel, are considered for the ESC-10, ESC-50, and Urban sound (US8K) datasets. Furthermore, to avoid the risk of overfitting due to limited numbers of data, this study also introduces offline data augmentation techniques to enhance the used datasets with a combination of L2 regularization. The performance evaluation illustrates that the best accuracy attained by the proposed DCNN without max-pooling function (Model-2) and using Log-Mel audio feature extraction on those augmented datasets. For ESC-10, ESC-50 and US8K, the highest achieved accuracies are 94.94\%, 89.28\%, and 95.37\% respectively. The experimental results show that the proposed approach can accomplish the best performance on environment sound classification problems.},
  language = {en},
  urldate  = {2023-02-05},
  journal  = {Applied Acoustics},
  author   = {Mushtaq, Zohaib and Su, Shun-Feng},
  month    = oct,
  year     = {2020},
  keywords = {Data augmentation, Deep convolutional neural network, Environmental sound classification, ESC-10, ESC-50, Regularization, Urbansound8k},
  pages    = {107389},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/C9LFLW25/Mushtaq and Su - 2020 - Environmental sound classification using a regular.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/NMKGVSJD/S0003682X2030493X.html:text/html}
}

@article{qian_data_2019,
  title    = {Data augmentation using generative adversarial networks for robust speech recognition},
  volume   = {114},
  issn     = {0167-6393},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167639319300044},
  doi      = {10.1016/j.specom.2019.08.006},
  abstract = {For noise robust speech recognition, data mismatch between training and testing is a significant challenge. Data augmentation is an effective way to enlarge the size and diversity of training data and solve this problem. Different from the traditional approaches by directly adding noise to the original waveform, in this work we utilize generative adversarial networks (GAN) for data generation to improve speech recognition under noise conditions. In this paper we investigate different configurations of GANs. Firstly the basic GAN is applied: the generated speech samples are based on spectrum feature level and produced frame by frame without dependence among them, and there is no true labels. Thus, an unsupervised learning framework is proposed to utilize these untranscribed data for acoustic modeling. Then, in order to better guide the data generation, condition information is introduced into GAN structures, and the conditional GAN is utilized: two different conditions are explored, including the acoustic state of each speech frame and the original paired clean speech of each speech frame. With the incorporation of specific condition information into data generation, these conditional GANs can provide true labels directly, which can be used for later acoustic modeling. During the acoustic model training, these true labels are combined with the soft labels which make the model better. The proposed GAN-based data augmentation approaches are evaluated on two different noisy tasks: Aurora4 (simulated data with additive noise and channel distortion) and the AMI meeting transcription task (real data with significant reverberation). The experiments show that the new data augmentation approaches can obtain the performance improvement under all noisy conditions, which including additive noise, channel distortion and reverberation. With these augmented data by basic GAN / conditional GAN, a relative 6\% to 14\% WER reduction can be obtained upon an advanced acoustic model.},
  language = {en},
  urldate  = {2023-02-05},
  journal  = {Speech Communication},
  author   = {Qian, Yanmin and Hu, Hu and Tan, Tian},
  month    = nov,
  year     = {2019},
  keywords = {Generative adversarial networks, Data augmentation, Conditional generative adversarial networks, Robust speech recognition, Very deep convolutional neural network},
  pages    = {1--9},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/E2K2VVTS/Qian et al. - 2019 - Data augmentation using generative adversarial net.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/FN2CIXSC/S0167639319300044.html:text/html}
}

@misc{vaswani_attention_2017,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  doi       = {10.48550/arXiv.1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = dec,
  year      = {2017},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  annote    = {Comment: 15 pages, 5 figures},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/6LKCT296/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/RREQWYV4/1706.html:text/html}
}

@article{deng_deep_2014,
  title      = {Deep {Learning}: {Methods} and {Applications}},
  volume     = {7},
  issn       = {1932-8346, 1932-8354},
  shorttitle = {Deep {Learning}},
  url        = {https://nowpublishers.com/article/Details/SIG-039},
  doi        = {10.1561/2000000039},
  abstract   = {Deep Learning: Methods and Applications},
  language   = {English},
  number     = {3–4},
  urldate    = {2023-02-05},
  journal    = {Foundations and Trends® in Signal Processing},
  author     = {Deng, Li and Yu, Dong},
  month      = jun,
  year       = {2014},
  note       = {Publisher: Now Publishers, Inc.},
  pages      = {197--387},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/CWCL9L88/Deng and Yu - 2014 - Deep Learning Methods and Applications.pdf:application/pdf}
}

@article{fukushima_neocognitron_1980,
  title      = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  volume     = {36},
  issn       = {0340-1200, 1432-0770},
  shorttitle = {Neocognitron},
  url        = {http://link.springer.com/10.1007/BF00344251},
  doi        = {10.1007/BF00344251},
  abstract   = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  language   = {en},
  number     = {4},
  urldate    = {2023-02-05},
  journal    = {Biological Cybernetics},
  author     = {Fukushima, Kunihiko},
  month      = apr,
  year       = {1980},
  pages      = {193--202},
  file       = {Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:/Users/marcio/Zotero/storage/CULEJW9C/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:application/pdf}
}

@misc{rezende_variational_2016,
  title     = {Variational {Inference} with {Normalizing} {Flows}},
  url       = {http://arxiv.org/abs/1505.05770},
  doi       = {10.48550/arXiv.1505.05770},
  abstract  = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  urldate   = {2023-02-05},
  publisher = {arXiv},
  author    = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1505.05770 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Computation, Statistics - Methodology},
  annote    = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/SY5GTYHH/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/LSCE4NUP/1505.html:text/html}
}

@article{weng_flow-based_2018,
  title   = {Flow-based {Deep} {Generative} {Models}},
  url     = {https://lilianweng.github.io/posts/2018-10-13-flow-models/},
  journal = {lilianweng.github.io},
  author  = {Weng, Lilian},
  year    = {2018}
}

@article{novotny_analysis_2019,
  title    = {Analysis of {DNN} {Speech} {Signal} {Enhancement} for {Robust} {Speaker} {Recognition}},
  volume   = {58},
  issn     = {0885-2308},
  url      = {https://www.sciencedirect.com/science/article/pii/S0885230818303607},
  doi      = {10.1016/j.csl.2019.06.004},
  abstract = {In this work, we present an analysis of a DNN-based autoencoder for speech enhancement, dereverberation and denoising. The target application is a robust speaker verification (SV) system. We start our approach by carefully designing a data augmentation process to cover a wide range of acoustic conditions and to obtain rich training data for various components of our SV system. We augment several well-known databases used in SV with artificially noised and reverberated data and we use them to train a denoising autoencoder (mapping noisy and reverberated speech to its clean version) as well as an x-vector extractor which is currently considered as state-of-the-art in SV. Later, we use the autoencoder as a preprocessing step for a text-independent SV system. We compare results achieved with autoencoder enhancement, multi-condition PLDA training and their simultaneous use. We present a detailed analysis with various conditions of NIST SRE 2010, 2016, PRISM and with re-transmitted data. We conclude that the proposed preprocessing can significantly improve both i-vector and x-vector baselines and that this technique can be used to build a robust SV system for various target domains.},
  language = {en},
  urldate  = {2023-02-05},
  journal  = {Computer Speech \& Language},
  author   = {Novotný, Ondřej and Plchot, Oldřich and Glembek, Ondřej and Cernocký, Jan Honza and Burget, Lukáš},
  month    = nov,
  year     = {2019},
  keywords = {Autoencoder, Embedding, Neural network, Robustness, Signal enhancement, Speaker verification},
  pages    = {403--421},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/RDUX75EN/Novotný et al. - 2019 - Analysis of DNN Speech Signal Enhancement for Robu.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/ZK8GBXBV/S0885230818303607.html:text/html}
}

@misc{mehri_samplernn_2017,
  title      = {{SampleRNN}: {An} {Unconditional} {End}-to-{End} {Neural} {Audio} {Generation} {Model}},
  shorttitle = {{SampleRNN}},
  url        = {http://arxiv.org/abs/1612.07837},
  doi        = {10.48550/arXiv.1612.07837},
  abstract   = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
  urldate    = {2023-02-06},
  publisher  = {arXiv},
  author     = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  month      = feb,
  year       = {2017},
  note       = {arXiv:1612.07837 [cs]},
  keywords   = {Computer Science - Sound, Computer Science - Artificial Intelligence},
  annote     = {Comment: Published as a conference paper at ICLR 2017},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/KJX35XI6/Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JJMDZLFE/1612.html:text/html}
}

@misc{kalchbrenner_efficient_2018,
  title     = {Efficient {Neural} {Audio} {Synthesis}},
  url       = {http://arxiv.org/abs/1802.08435},
  doi       = {10.48550/arXiv.1802.08435},
  abstract  = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
  urldate   = {2023-02-06},
  publisher = {arXiv},
  author    = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
  month     = jun,
  year      = {2018},
  note      = {arXiv:1802.08435 [cs, eess]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  annote    = {Comment: 10 pages},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/B4UUA7E3/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/K5FC44E8/1802.html:text/html}
}

@misc{cho_properties_2014,
  title      = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
  shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
  url        = {http://arxiv.org/abs/1409.1259},
  doi        = {10.48550/arXiv.1409.1259},
  abstract   = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  urldate    = {2023-02-06},
  publisher  = {arXiv},
  author     = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  month      = oct,
  year       = {2014},
  note       = {arXiv:1409.1259 [cs, stat]},
  keywords   = {Statistics - Machine Learning, Computer Science - Computation and Language},
  annote     = {Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/NX9TIBY3/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/3PTEVRVA/1409.html:text/html}
}

@article{hochreiter_long_1997,
  title    = {Long {Short}-{Term} {Memory}},
  volume   = {9},
  issn     = {0899-7667},
  url      = {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi      = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number   = {8},
  urldate  = {2023-02-06},
  journal  = {Neural Computation},
  author   = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  month    = nov,
  year     = {1997},
  pages    = {1735--1780}
}

@misc{engel_neural_2017,
  title     = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
  url       = {http://arxiv.org/abs/1704.01279},
  doi       = {10.48550/arXiv.1704.01279},
  abstract  = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
  urldate   = {2023-02-06},
  publisher = {arXiv},
  author    = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
  month     = apr,
  year      = {2017},
  note      = {arXiv:1704.01279 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Computer Science - Artificial Intelligence},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/X3IHGVR2/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/SRGM3W7Z/1704.html:text/html}
}

@misc{oord_conditional_2016,
  title     = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
  url       = {http://arxiv.org/abs/1606.05328},
  doi       = {10.48550/arXiv.1606.05328},
  abstract  = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  urldate   = {2023-02-06},
  publisher = {arXiv},
  author    = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  month     = jun,
  year      = {2016},
  note      = {arXiv:1606.05328 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/ZYP4KT6D/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/KVQHL65H/1606.html:text/html}
}

@misc{paine_fast_2016,
  title     = {Fast {Wavenet} {Generation} {Algorithm}},
  url       = {http://arxiv.org/abs/1611.09482},
  doi       = {10.48550/arXiv.1611.09482},
  abstract  = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2{\textasciicircum}L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
  urldate   = {2023-02-06},
  publisher = {arXiv},
  author    = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
  month     = nov,
  year      = {2016},
  note      = {arXiv:1611.09482 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Computer Science - Data Structures and Algorithms},
  annote    = {Comment: Technical Report},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/RJHQ2HEX/Paine et al. - 2016 - Fast Wavenet Generation Algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JPNGP2MX/1611.html:text/html}
}

@misc{donahue_adversarial_2019,
  title     = {Adversarial {Audio} {Synthesis}},
  url       = {http://arxiv.org/abs/1802.04208},
  doi       = {10.48550/arXiv.1802.04208},
  abstract  = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
  urldate   = {2023-02-06},
  publisher = {arXiv},
  author    = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
  month     = feb,
  year      = {2019},
  note      = {arXiv:1802.04208 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound},
  annote    = {Comment: Published as a conference paper at ICLR 2019},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/JQY4JJP6/Donahue et al. - 2019 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/HZXQNN6T/1802.html:text/html}
}

@article{cao_deconvolutional_2020,
  title    = {Deconvolutional neural network for image super-resolution},
  volume   = {132},
  issn     = {0893-6080},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020303403},
  doi      = {10.1016/j.neunet.2020.09.017},
  abstract = {This study builds a fully deconvolutional neural network (FDNN) and addresses the problem of single image super-resolution (SISR) by using the FDNN. Although SISR using deep neural networks has been a major research focus, the problem of reconstructing a high resolution (HR) image with an FDNN has received little attention. A few recent approaches toward SISR are to embed deconvolution operations into multilayer feedforward neural networks. This paper constructs a deep FDNN for SISR that possesses two remarkable advantages compared to existing SISR approaches. The first improves the network performance without increasing the depth of the network or embedding complex structures. The second replaces all convolution operations with deconvolution operations to implement an effective reconstruction. That is, the proposed FDNN only contains deconvolution layers and learns an end-to-end mapping from low resolution (LR) to HR images. Furthermore, to avoid the oversmoothness of the mean squared error loss, the trained image is treated as a probability distribution, and the Kullback–Leibler divergence is introduced into the final loss function to achieve enhanced recovery. Although the proposed FDNN only has 10 layers, it is successfully evaluated through extensive experiments. Compared with other state-of-the-art methods and deep convolution neural networks with 20 or 30 layers, the proposed FDNN achieves better performance for SISR.},
  language = {en},
  urldate  = {2023-02-06},
  journal  = {Neural Networks},
  author   = {Cao, Feilong and Yao, Kaixuan and Liang, Jiye},
  month    = dec,
  year     = {2020},
  keywords = {Convolutional neural networks (CNNs), Deconvolutional neural networks, Deep learning, Single image super-resolution (SISR)},
  pages    = {394--404},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/A2M5I6YD/Cao et al. - 2020 - Deconvolutional neural network for image super-res.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/88IEST29/S0893608020303403.html:text/html}
}

@inproceedings{rao_grapheme--phoneme_2015,
  address   = {South Brisbane, Queensland, Australia},
  title     = {Grapheme-to-phoneme conversion using {Long} {Short}-{Term} {Memory} recurrent neural networks},
  isbn      = {978-1-4673-6997-8},
  url       = {http://ieeexplore.ieee.org/document/7178767/},
  doi       = {10.1109/ICASSP.2015.7178767},
  abstract  = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoderdecoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
  language  = {en},
  urldate   = {2023-02-06},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  publisher = {IEEE},
  author    = {Rao, Kanishka and Peng, Fuchun and Sak, Hasim and Beaufays, Francoise},
  month     = apr,
  year      = {2015},
  pages     = {4225--4229},
  file      = {Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:/Users/marcio/Zotero/storage/9W6TPQHQ/Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:application/pdf}
}

@incollection{benois-pineau_deep_2021,
  address   = {Cham},
  title     = {Deep {Learning} for {Audio} and {Music}},
  isbn      = {978-3-030-74477-9 978-3-030-74478-6},
  url       = {https://link.springer.com/10.1007/978-3-030-74478-6_10},
  abstract  = {This chapter provides an overview of how deep learning techniques can be used for audio signals. We ﬁrst review the main DNN architectures, meta-architectures and training paradigms used for audio processing. By highlighting the speciﬁes of the audio signal, we discuss the various possible audio representations to be used as input of a DNN — time and frequency representations, waveform representations and knowledge-driven representations — and discuss how the ﬁrst layers of a DNN can be set to take into account these speciﬁcity’s. We then review a set of applications for three main classes of problems: audio recognition, audio processing and audio generation. We do this considering two types of audio content which are less commonly addressed in the literature: music and environmental sounds.},
  language  = {en},
  urldate   = {2023-02-09},
  booktitle = {Multi-faceted {Deep} {Learning}},
  publisher = {Springer International Publishing},
  author    = {Peeters, Geoffroy and Richard, Gaël},
  editor    = {Benois-Pineau, Jenny and Zemmari, Akka},
  year      = {2021},
  doi       = {10.1007/978-3-030-74478-6_10},
  pages     = {231--266},
  file      = {Peeters and Richard - 2021 - Deep Learning for Audio and Music.pdf:/Users/marcio/Zotero/storage/WHTN5T2H/Peeters and Richard - 2021 - Deep Learning for Audio and Music.pdf:application/pdf}
}

@article{rombach_high-resolution_2021,
  title   = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
  volume  = {abs/2112.10752},
  url     = {https://arxiv.org/abs/2112.10752},
  journal = {CoRR},
  author  = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  year    = {2021},
  note    = {arXiv: 2112.10752},
  file    = {Full Text:/Users/marcio/Zotero/storage/JGGY43PZ/Rombach et al. - 2021 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;Rombach et al. - 2021 - High-Resolution Image Synthesis with Latent Diffus.pdf:/Users/marcio/Zotero/storage/5TVZZXPC/Rombach et al. - 2021 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf}
}

@article{devlin_bert_2018,
  title   = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  volume  = {abs/1810.04805},
  url     = {http://arxiv.org/abs/1810.04805},
  journal = {CoRR},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year    = {2018},
  note    = {arXiv: 1810.04805}
}

@misc{mikolov_efficient_2013,
  title     = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1301.3781},
  publisher = {arXiv},
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year      = {2013},
  doi       = {10.48550/ARXIV.1301.3781},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@inproceedings{salamon_scaper_2017,
  title      = {Scaper: {A} library for soundscape synthesis and augmentation},
  shorttitle = {Scaper},
  doi        = {10.1109/WASPAA.2017.8170052},
  abstract   = {Sound event detection (SED) in environmental recordings is a key topic of research in machine listening, with applications in noise monitoring for smart cities, self-driving cars, surveillance, bioa-coustic monitoring, and indexing of large multimedia collections. Developing new solutions for SED often relies on the availability of strongly labeled audio recordings, where the annotation includes the onset, offset and source of every event. Generating such precise annotations manually is very time consuming, and as a result existing datasets for SED with strong labels are scarce and limited in size. To address this issue, we present Scaper, an open-source library for soundscape synthesis and augmentation. Given a collection of iso-lated sound events, Scaper acts as a high-level sequencer that can generate multiple soundscapes from a single, probabilistically defined, “specification”. To increase the variability of the output, Scaper supports the application of audio transformations such as pitch shifting and time stretching individually to every event. To illustrate the potential of the library, we generate a dataset of 10,000 sound-scapes and use it to compare the performance of two state-of-the-art algorithms, including a breakdown by soundscape characteristics. We also describe how Scaper was used to generate audio stimuli for an audio labeling crowdsourcing experiment, and conclude with a discussion of Scaper's limitations and potential applications.},
  booktitle  = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
  author     = {Salamon, Justin and MacConnell, Duncan and Cartwright, Mark and Li, Peter and Bello, Juan Pablo},
  month      = oct,
  year       = {2017},
  note       = {ISSN: 1947-1629},
  keywords   = {Data models, Acoustics, Libraries, Monitoring, Probabilistic logic, Signal to noise ratio, sound event detection, Soundscape, synthesis, Training},
  pages      = {344--348},
  file       = {IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/C8Z9PI4Z/8170052.html:text/html}
}

@misc{agostinelli_musiclm_2023,
  title     = {{MusicLM}: {Generating} {Music} {From} {Text}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2301.11325},
  publisher = {arXiv},
  author    = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zalán and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
  year      = {2023},
  doi       = {10.48550/ARXIV.2301.11325},
  keywords  = {FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), electronic engineering, FOS: Electrical engineering, information engineering, Machine Learning (cs.LG), Sound (cs.SD)},
  file      = {Agostinelli et al. - 2023 - MusicLM Generating Music From Text.pdf:/Users/marcio/Zotero/storage/2WWHRWLZ/Agostinelli et al. - 2023 - MusicLM Generating Music From Text.pdf:application/pdf}
}

@inproceedings{takahashi_deep_2016,
  title     = {Deep {Convolutional} {Neural} {Networks} and {Data} {Augmentation} for {Acoustic} {Event} {Recognition}},
  doi       = {10.21437/Interspeech.2016-805},
  booktitle = {Proc. {Interspeech} 2016},
  author    = {Takahashi, Naoya and Gygli, Michael and Pfister, Beat and Gool, Luc Van},
  year      = {2016},
  pages     = {2982--2986}
}

@article{becker_interpreting_2018,
  title   = {Interpreting and {Explaining} {Deep} {Neural} {Networks} for {Classification} of {Audio} {Signals}},
  volume  = {abs/1807.03418},
  journal = {CoRR},
  author  = {Becker, Sören and Ackermann, Marcel and Lapuschkin, Sebastian and Müller, Klaus-Robert and Samek, Wojciech},
  year    = {2018},
  note    = {\_eprint: 1807.03418}
}

@article{deng_mnist_2012,
  title   = {The mnist database of handwritten digit images for machine learning research},
  volume  = {29},
  number  = {6},
  journal = {IEEE Signal Processing Magazine},
  author  = {Deng, Li},
  year    = {2012},
  note    = {Publisher: IEEE},
  pages   = {141--142}
}

@inproceedings{gemmeke_audio_2017,
  address   = {New Orleans, LA},
  title     = {Audio {Set}: {An} ontology and human-labeled dataset for audio events},
  booktitle = {Proc. {IEEE} {ICASSP} 2017},
  author    = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  year      = {2017}
}

@misc{fonseca_general-purpose_2018,
  title     = {General-purpose {Tagging} of {Freesound} {Audio} with {AudioSet} {Labels}: {Task} {Description}, {Dataset}, and {Baseline}},
  copyright = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/1807.09902},
  publisher = {arXiv},
  author    = {Fonseca, Eduardo and Plakal, Manoj and Font, Frederic and Ellis, Daniel P. W. and Favory, Xavier and Pons, Jordi and Serra, Xavier},
  year      = {2018},
  doi       = {10.48550/ARXIV.1807.09902},
  keywords  = {FOS: Computer and information sciences, Audio and Speech Processing (eess.AS), electronic engineering, FOS: Electrical engineering, information engineering, Machine Learning (cs.LG), Sound (cs.SD), Machine Learning (stat.ML)}
}

@inproceedings{salamon_dataset_2014,
  address   = {Orlando, FL, USA},
  title     = {A {Dataset} and {Taxonomy} for {Urban} {Sound} {Research}},
  booktitle = {22nd {ACM} {International} {Conference} on {Multimedia} ({ACM}-{MM}'14)},
  author    = {Salamon, J. and Jacoby, C. and Bello, J. P.},
  month     = nov,
  year      = {2014},
  pages     = {1041--1044}
}

@inproceedings{kim_audiocaps_2019,
  title     = {{AudioCaps}: {Generating} {Captions} for {Audios} in {The} {Wild}},
  booktitle = {{NAACL}-{HLT}},
  author    = {Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee},
  year      = {2019}
}

@article{drossos_clotho_2019,
  title   = {Clotho: {An} {Audio} {Captioning} {Dataset}},
  volume  = {abs/1910.09387},
  url     = {http://arxiv.org/abs/1910.09387},
  journal = {CoRR},
  author  = {Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas},
  year    = {2019},
  note    = {arXiv: 1910.09387}
}

@inproceedings{van_den_oord_pixel_2016,
  address   = {New York, New York, USA},
  series    = {Proceedings of {Machine} {Learning} {Research}},
  title     = {Pixel {Recurrent} {Neural} {Networks}},
  volume    = {48},
  url       = {https://proceedings.mlr.press/v48/oord16.html},
  abstract  = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {van den Oord, Aäron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  month     = jun,
  year      = {2016},
  pages     = {1747--1756},
  file      = {van den Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf:/Users/marcio/Zotero/storage/2RQGW54X/van den Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf:application/pdf}
}

@misc{radford_learning_2021,
  title     = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2103.00020},
  publisher = {arXiv},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year      = {2021},
  doi       = {10.48550/ARXIV.2103.00020},
  keywords  = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV)},
  file      = {Full Text:/Users/marcio/Zotero/storage/M87AKDCF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf}
}

@misc{nichol_glide_2021,
  title     = {{GLIDE}: {Towards} {Photorealistic} {Image} {Generation} and {Editing} with {Text}-{Guided} {Diffusion} {Models}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2112.10741},
  publisher = {arXiv},
  author    = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  year      = {2021},
  doi       = {10.48550/ARXIV.2112.10741},
  keywords  = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR)},
  file      = {Full Text:/Users/marcio/Zotero/storage/JNN4NQ6P/Nichol et al. - 2021 - GLIDE Towards Photorealistic Image Generation and.pdf:application/pdf}
}

@misc{christopher_olah_understanding_2015,
  title   = {Understanding {LSTM} {Networks}},
  url     = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2023-03-08},
  author  = {{Christopher Olah}},
  month   = aug,
  year    = {2015}
}

@misc{alammar_illustrated_nodate,
  title    = {The {Illustrated} {Stable} {Diffusion}},
  url      = {https://jalammar.github.io/illustrated-stable-diffusion/},
  abstract = {Translations: Vietnamese.
              
              
              (V2 Nov 2022: Updated images for more precise description of forward diffusion. A few more images in this version)
              
              AI image generation is the most recent AI capability blowing people’s minds (mine included). The ability to create striking visuals from text descriptions has a magical quality to it and points clearly to a shift in how humans create art. The release of Stable Diffusion is a clear milestone in this development because it made a high-performance model available to the masses (performance in terms of image quality, as well as speed and relatively low resource/memory requirements).
              
              After experimenting with AI image generation, you may start to wonder how it works.
              
              This is a gentle introduction to how Stable Diffusion works.
              
              
              
              
              
              
              
              Stable Diffusion is versatile in that it can be used in a number of different ways. Let’s focus at first on image generation from text only (text2img). The image above shows an example text input and the resulting generated image (The actual complete prompt is here). Aside from text to image, another main way of using it is by making it alter images (so inputs are text + image).},
  urldate  = {2023-03-11},
  author   = {Alammar, Jay},
  file     = {Snapshot:/Users/marcio/Zotero/storage/SJ4PRKSM/illustrated-stable-diffusion.html:text/html}
}

@misc{abu-el-haija_youtube-8m_2016,
  title      = {{YouTube}-{8M}: {A} {Large}-{Scale} {Video} {Classification} {Benchmark}},
  shorttitle = {{YouTube}-{8M}},
  url        = {http://arxiv.org/abs/1609.08675},
  doi        = {10.48550/arXiv.1609.08675},
  abstract   = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\textasciitilde}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
  urldate    = {2023-03-16},
  publisher  = {arXiv},
  author     = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  month      = sep,
  year       = {2016},
  note       = {arXiv:1609.08675 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: 10 pages},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/P4IS8FFE/Abu-El-Haija et al. - 2016 - YouTube-8M A Large-Scale Video Classification Ben.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/94HVYB8U/1609.html:text/html}
}

@inproceedings{parvat_survey_2017,
  title     = {A survey of deep-learning frameworks},
  doi       = {10.1109/ICISC.2017.8068684},
  abstract  = {Deep learning is a model of machine learning loosely based on our brain. Artificial neural network has been around since the 1950s, but recent advances in hardware like graphical processing units (GPU), software like cuDNN, TensorFlow, Torch, Caffe, Theano, Deeplearning4j, etc. and new training methods have made training artificial neural networks fast and easy. In this paper, we are comparing some of the deep learning frameworks on the basis of parameters like modeling capability, interfaces available, platforms supported, parallelizing techniques supported, availability of pre-trained models, community support and documentation quality.},
  booktitle = {2017 {International} {Conference} on {Inventive} {Systems} and {Control} ({ICISC})},
  author    = {Parvat, Aniruddha and Chavan, Jai and Kadam, Siddhesh and Dev, Souradeep and Pathak, Vidhi},
  month     = jan,
  year      = {2017},
  keywords  = {Deep learning, Computational modeling, Graphics processing units, Neural networks, Mathematical model, Machine learning, Libraries, Training, Artificial neural networks, Software libraries},
  pages     = {1--7},
  file      = {IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/NB5MGH2B/8068684.html:text/html}
}

@misc{martin_abadi_tensorflow_2015,
  title  = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
  url    = {https://www.tensorflow.org/},
  author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year   = {2015},
  annote = {Software available from tensorflow.org}
}

@misc{chollet_keras_2015,
  title  = {Keras},
  url    = {https://github.com/fchollet/keras},
  author = {Chollet, Francois and {others}},
  year   = {2015},
  note   = {Publisher: GitHub}
}

@incollection{paszke_pytorch_2019,
  title     = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
  publisher = {Curran Associates, Inc.},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year      = {2019},
  pages     = {8024--8035}
}

@misc{maguolo_audiogmenter_2022,
  title      = {Audiogmenter: a {MATLAB} {Toolbox} for {Audio} {Data} {Augmentation}},
  shorttitle = {Audiogmenter},
  url        = {http://arxiv.org/abs/1912.05472},
  doi        = {10.48550/arXiv.1912.05472},
  abstract   = {Audio data augmentation is a key step in training deep neural networks for solving audio classification tasks. In this paper, we introduce Audiogmenter, a novel audio data augmentation library in MATLAB. We provide 15 different augmentation algorithms for raw audio data and 8 for spectrograms. We efficiently implemented several augmentation techniques whose usefulness has been extensively proved in the literature. To the best of our knowledge, this is the largest MATLAB audio data augmentation library freely available. We validate the efficiency of our algorithms evaluating them on the ESC-50 dataset. The toolbox and its documentation can be downloaded at https://github.com/LorisNanni/Audiogmenter.},
  urldate    = {2023-04-01},
  publisher  = {arXiv},
  author     = {Maguolo, Gianluca and Paci, Michelangelo and Nanni, Loris and Bonan, Ludovico},
  month      = jan,
  year       = {2022},
  note       = {arXiv:1912.05472 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/7HRLMTMP/Maguolo et al. - 2022 - Audiogmenter a MATLAB Toolbox for Audio Data Augm.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/J87JF63Q/1912.html:text/html}
}

@misc{oord_neural_2018,
  title     = {Neural {Discrete} {Representation} {Learning}},
  url       = {http://arxiv.org/abs/1711.00937},
  doi       = {10.48550/arXiv.1711.00937},
  abstract  = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  urldate   = {2023-04-02},
  publisher = {arXiv},
  author    = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
  month     = may,
  year      = {2018},
  note      = {arXiv:1711.00937 [cs]},
  keywords  = {Computer Science - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/BQGAGV2Z/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/QULXTKCL/1711.html:text/html}
}

@misc{dhariwal_jukebox_2020,
  title      = {Jukebox: {A} {Generative} {Model} for {Music}},
  shorttitle = {Jukebox},
  url        = {http://arxiv.org/abs/2005.00341},
  doi        = {10.48550/arXiv.2005.00341},
  abstract   = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  urldate    = {2023-04-05},
  publisher  = {arXiv},
  author     = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  month      = apr,
  year       = {2020},
  note       = {arXiv:2005.00341 [cs, eess, stat]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/FECAU52N/Dhariwal et al. - 2020 - Jukebox A Generative Model for Music.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/ITQHYLNG/2005.html:text/html}
}

@misc{borsos_audiolm_2022,
  title      = {{AudioLM}: a {Language} {Modeling} {Approach} to {Audio} {Generation}},
  shorttitle = {{AudioLM}},
  url        = {http://arxiv.org/abs/2209.03143},
  doi        = {10.48550/arXiv.2209.03143},
  abstract   = {We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
  urldate    = {2023-04-05},
  publisher  = {arXiv},
  author     = {Borsos, Zalán and Marinier, Raphaël and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.03143 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/85FFDHC5/Borsos et al. - 2022 - AudioLM a Language Modeling Approach to Audio Gen.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/8EC6WITI/2209.html:text/html}
}

@misc{kreuk_audiogen_2023,
  title      = {{AudioGen}: {Textually} {Guided} {Audio} {Generation}},
  shorttitle = {{AudioGen}},
  url        = {http://arxiv.org/abs/2209.15352},
  doi        = {10.48550/arXiv.2209.15352},
  abstract   = {We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen},
  urldate    = {2023-04-07},
  publisher  = {arXiv},
  author     = {Kreuk, Felix and Synnaeve, Gabriel and Polyak, Adam and Singer, Uriel and Défossez, Alexandre and Copet, Jade and Parikh, Devi and Taigman, Yaniv and Adi, Yossi},
  month      = mar,
  year       = {2023},
  note       = {arXiv:2209.15352 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
  annote     = {Comment: Accepted to ICLR 2023},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/VWJ342M5/Kreuk et al. - 2023 - AudioGen Textually Guided Audio Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/MUDV92V6/2209.html:text/html}
}

@misc{forsgren_riffusion_2022,
  title  = {Riffusion - {Stable} diffusion for real-time music generation},
  url    = {https://riffusion.com/about},
  author = {Forsgren, Seth* and Martiros, Hayk*},
  year   = {2022}
}

@misc{zeghidour_soundstream_2021,
  title      = {{SoundStream}: {An} {End}-to-{End} {Neural} {Audio} {Codec}},
  shorttitle = {{SoundStream}},
  url        = {http://arxiv.org/abs/2107.03312},
  abstract   = {We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.},
  urldate    = {2023-04-10},
  publisher  = {arXiv},
  author     = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco},
  month      = jul,
  year       = {2021},
  note       = {arXiv:2107.03312 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/YVKCY9KS/Zeghidour et al. - 2021 - SoundStream An End-to-End Neural Audio Codec.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/HCVCYR92/2107.html:text/html}
}

@misc{chung_w2v-bert_2021,
  title      = {W2v-{BERT}: {Combining} {Contrastive} {Learning} and {Masked} {Language} {Modeling} for {Self}-{Supervised} {Speech} {Pre}-{Training}},
  shorttitle = {W2v-{BERT}},
  url        = {http://arxiv.org/abs/2108.06209},
  abstract   = {Motivated by the success of masked language modeling{\textasciitilde}(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks{\textasciitilde}(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light{\textasciitilde}60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec{\textasciitilde}2.0 and HuBERT, our model shows{\textasciitilde}5{\textbackslash}\% to{\textasciitilde}10{\textbackslash}\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec{\textasciitilde}2.0 by more than{\textasciitilde}30{\textbackslash}\% relatively.},
  urldate    = {2023-04-10},
  publisher  = {arXiv},
  author     = {Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
  month      = sep,
  year       = {2021},
  note       = {arXiv:2108.06209 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/PPGWMZHL/Chung et al. - 2021 - W2v-BERT Combining Contrastive Learning and Maske.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/KSBMX3WG/2108.html:text/html}
}

@misc{huang_mulan_2022,
  title      = {{MuLan}: {A} {Joint} {Embedding} of {Music} {Audio} and {Natural} {Language}},
  shorttitle = {{MuLan}},
  url        = {http://arxiv.org/abs/2208.12415},
  abstract   = {Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.},
  urldate    = {2023-04-10},
  publisher  = {arXiv},
  author     = {Huang, Qingqing and Jansen, Aren and Lee, Joonseok and Ganti, Ravi and Li, Judith Yue and Ellis, Daniel P. W.},
  month      = aug,
  year       = {2022},
  note       = {arXiv:2208.12415 [cs, eess, stat]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Computation and Language},
  annote     = {Comment: To appear in ISMIR 2022},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/CRV322JD/Huang et al. - 2022 - MuLan A Joint Embedding of Music Audio and Natura.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/5I2DCIAH/2208.html:text/html}
}

@misc{kingma_adam_2017,
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  doi        = {10.48550/arXiv.1412.6980},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate    = {2023-04-11},
  publisher  = {arXiv},
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  month      = jan,
  year       = {2017},
  note       = {arXiv:1412.6980 [cs]},
  keywords   = {Computer Science - Machine Learning},
  annote     = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/TBMQ5QAR/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JFBA5BEY/1412.html:text/html}
}

@misc{rolfe_discrete_2017,
  title     = {Discrete {Variational} {Autoencoders}},
  url       = {http://arxiv.org/abs/1609.02200},
  doi       = {10.48550/arXiv.1609.02200},
  abstract  = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
  urldate   = {2023-04-12},
  publisher = {arXiv},
  author    = {Rolfe, Jason Tyler},
  month     = apr,
  year      = {2017},
  note      = {arXiv:1609.02200 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  annote    = {Comment: Published as a conference paper at ICLR 2017},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/IKEZ8773/Rolfe - 2017 - Discrete Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/5GLWCBDB/1609.html:text/html}
}

@misc{ping_clarinet_2019,
  title      = {{ClariNet}: {Parallel} {Wave} {Generation} in {End}-to-{End} {Text}-to-{Speech}},
  shorttitle = {{ClariNet}},
  url        = {http://arxiv.org/abs/1807.07281},
  doi        = {10.48550/arXiv.1807.07281},
  abstract   = {In this work, we propose a new solution for parallel wave generation by WaveNet. In contrast to parallel WaveNet (van den Oord et al., 2018), we distill a Gaussian inverse autoregressive flow from the autoregressive WaveNet by minimizing a regularized KL divergence between their highly-peaked output distributions. Our method computes the KL divergence in closed-form, which simplifies the training algorithm and provides very efficient distillation. In addition, we introduce the first text-to-wave neural architecture for speech synthesis, which is fully convolutional and enables fast end-to-end training from scratch. It significantly outperforms the previous pipeline that connects a text-to-spectrogram model to a separately trained WaveNet (Ping et al., 2018). We also successfully distill a parallel waveform synthesizer conditioned on the hidden representation in this end-to-end model.},
  urldate    = {2023-04-12},
  publisher  = {arXiv},
  author     = {Ping, Wei and Peng, Kainan and Chen, Jitong},
  month      = feb,
  year       = {2019},
  note       = {arXiv:1807.07281 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  annote     = {Comment: Published at ICLR 2019. (v3: add important details \& discussion in Appendix A)},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/GABQV4EN/Ping et al. - 2019 - ClariNet Parallel Wave Generation in End-to-End T.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JC5ALMUV/1807.html:text/html}
}

@misc{wang_neural_2023,
  title     = {Neural {Codec} {Language} {Models} are {Zero}-{Shot} {Text} to {Speech} {Synthesizers}},
  url       = {http://arxiv.org/abs/2301.02111},
  doi       = {10.48550/arXiv.2301.02111},
  abstract  = {We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.},
  urldate   = {2023-04-12},
  publisher = {arXiv},
  author    = {Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and He, Lei and Zhao, Sheng and Wei, Furu},
  month     = jan,
  year      = {2023},
  note      = {arXiv:2301.02111 [cs, eess]},
  keywords  = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
  annote    = {Comment: Working in progress},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/RDC753ER/Wang et al. - 2023 - Neural Codec Language Models are Zero-Shot Text to.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/CE7MJ6HH/2301.html:text/html}
}

@inproceedings{kalonaris_computational_2018,
  address    = {Dublin, Ireland},
  title      = {Computational {Music} {Aesthetics}: a survey and some thoughts},
  shorttitle = {Computational {Music} {Aesthetics}},
  url        = {http://galapagos.ucd.ie/wiki/pub/OpenAccess/CSMC/Kalonaris.pdf},
  abstract   = {While computational aesthetic evaluation has been applied to images and visual output, it is not as widely employed for generative music systems. Computational aesthetic evaluation is not to be confounded with numerical evaluation of the system’s output; such a notion is in danger of offering a reduced and impoverished interpretation of the aesthetic experience, which is innately dialogical, between the creator or the user, the sociological context, and the creative process or product.
                This paper reviews common computational aesthetic measures that have been used for musical applications, whilst arguing for a pragmatist perspective and a framework foregrounding the primacy of intentionality and agency in inducing aesthetic responses.},
  language   = {en},
  urldate    = {2023-04-13},
  author     = {Kalonaris, Stefano and Jordanous, Anna},
  month      = aug,
  year       = {2018},
  note       = {Accepted: 2018-07-09},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/HBSIKX4Q/Kalonaris and Jordanous - 2018 - Computational Music Aesthetics a survey and some .pdf:application/pdf}
}

@article{triantafyllopoulos_overview_2023,
  title    = {An {Overview} of {Affective} {Speech} {Synthesis} and {Conversion} in the {Deep} {Learning} {Era}},
  issn     = {1558-2256},
  doi      = {10.1109/JPROC.2023.3250266},
  abstract = {Speech is the fundamental mode of human communication, and its synthesis has long been a core priority in human–computer interaction research. In recent years, machines have managed to master the art of generating speech that is understandable by humans. However, the linguistic content of an utterance encompasses only a part of its meaning. Affect, or expressivity, has the capacity to turn speech into a medium capable of conveying intimate thoughts, feelings, and emotions—aspects that are essential for engaging and naturalistic interpersonal communication. While the goal of imparting expressivity to synthesized utterances has so far remained elusive, following recent advances in text-to-speech synthesis, a paradigm shift is well under way in the fields of affective speech synthesis and conversion as well. Deep learning, as the technology that underlies most of the recent advances in artificial intelligence, is spearheading these efforts. In this overview, we outline ongoing trends and summarize state-of-the-art approaches in an attempt to provide a broad overview of this exciting field.},
  journal  = {Proceedings of the IEEE},
  author   = {Triantafyllopoulos, Andreas and Schuller, Björn W. and İymen, Gökçe and Sezgin, Metin and He, Xiangheng and Yang, Zijiang and Tzirakis, Panagiotis and Liu, Shuo and Mertes, Silvan and André, Elisabeth and Fu, Ruibo and Tao, Jianhua},
  year     = {2023},
  note     = {Conference Name: Proceedings of the IEEE},
  keywords = {deep learning, Deep learning, Computational modeling, Task analysis, Affective computing, Appraisal, emotional voice conversion (EVC), Mood, speech synthesis, Speech synthesis},
  pages    = {1--27},
  file     = {IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/9ZDLMNS7/10065433.html:text/html;Submitted Version:/Users/marcio/Zotero/storage/JKR77FNM/Triantafyllopoulos et al. - 2023 - An Overview of Affective Speech Synthesis and Conv.pdf:application/pdf}
}

@misc{shi_survey_2021,
  title     = {A {Survey} on {Audio} {Synthesis} and {Audio}-{Visual} {Multimodal} {Processing}},
  url       = {http://arxiv.org/abs/2108.00443},
  doi       = {10.48550/arXiv.2108.00443},
  abstract  = {With the development of deep learning and artificial intelligence, audio synthesis has a pivotal role in the area of machine learning and shows strong applicability in the industry. Meanwhile, significant efforts have been dedicated by researchers to handle multimodal tasks at present such as audio-visual multimodal processing. In this paper, we conduct a survey on audio synthesis and audio-visual multimodal processing, which helps understand current research and future trends. This review focuses on text to speech(TTS), music generation and some tasks that combine visual and acoustic information. The corresponding technical methods are comprehensively classified and introduced, and their future development trends are prospected. This survey can provide some guidance for researchers who are interested in the areas like audio synthesis and audio-visual multimodal processing.},
  urldate   = {2023-04-13},
  publisher = {arXiv},
  author    = {Shi, Zhaofeng},
  month     = aug,
  year      = {2021},
  note      = {arXiv:2108.00443 [cs, eess]},
  keywords  = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/C4KBV996/Shi - 2021 - A Survey on Audio Synthesis and Audio-Visual Multi.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/7XEDZFQZ/2108.html:text/html}
}

@article{hajarolasvadi_generative_2020,
  title      = {Generative {Adversarial} {Networks} in {Human} {Emotion} {Synthesis}: {A} {Review}},
  volume     = {8},
  issn       = {2169-3536},
  shorttitle = {Generative {Adversarial} {Networks} in {Human} {Emotion} {Synthesis}},
  doi        = {10.1109/ACCESS.2020.3042328},
  abstract   = {Deep generative models have become an emerging topic in various research areas like computer vision and signal processing. These models allow synthesizing realistic data samples that are of great value for both academic and industrial communities. Affective computing, a topic of a broad interest in computer vision society, has been no exception and has benefited from this powerful approach. In fact, affective computing observed a rapid derivation of generative models during the last two decades. Applications of such models include but are not limited to emotion recognition and classification, unimodal emotion synthesis, and cross-modal emotion synthesis. As a result, we conducted a comprehensive survey of recent advances in human emotion synthesis by studying available databases, advantages, and disadvantages of the generative models along with the related training strategies considering two principal human communication modalities, namely audio and video. In this context, facial expression synthesis, speech emotion synthesis, and the audio-visual (cross-modal) emotion synthesis are reviewed extensively under different application scenarios. Gradually, we discuss open research problems to push the boundaries of this research area for future works. As conclusions, we indicate common problems that can be explored from the Generative Adversarial Networks (GAN) topologies and applications in emotion synthesis.},
  journal    = {IEEE Access},
  author     = {Hajarolasvadi, Noushin and Ramírez, Miguel Arjona and Beccaro, Wesley and Demirel, Hasan},
  year       = {2020},
  note       = {Conference Name: IEEE Access},
  keywords   = {Computational modeling, generative adversarial networks, Generative adversarial networks, Data models, Gallium nitride, Generators, Machine learning, Training, speech synthesis, emotion recognition, Emotion recognition, image processing, learning systems},
  pages      = {218499--218529},
  file       = {IEEE Xplore Full Text PDF:/Users/marcio/Zotero/storage/J3KIIV9E/Hajarolasvadi et al. - 2020 - Generative Adversarial Networks in Human Emotion S.pdf:application/pdf}
}

@article{kumar_comprehensive_2023,
  title    = {A comprehensive survey on generative adversarial networks used for synthesizing multimedia content},
  issn     = {1573-7721},
  url      = {https://doi.org/10.1007/s11042-023-15138-x},
  doi      = {10.1007/s11042-023-15138-x},
  abstract = {GAN’s are playing an important role in creating and generating a new set of data from the previously available content. GAN models are impressive in the results for image and video generation tasks. These models uses convolutional neural networks for generator and discriminator. GAN models are progressively improving by adding more latent approaches of deep learning. GAN model has been implemented for both supervised as well as unsupervised learning for various applications like image inpainting, image blending, video generation, music generation etc. During the implementation of the GAN model for all these applications, some issues arise during the training of discriminators like model collapse, Penalty Gradient etc. This manuscript contains a detailed survey of GAN models presented with varied classifications along with the challenges involved in GAN models. GAN is classified for all the domains in which GAN is used, i.e., Image, Video, and Audio. Along with all these things, we have described some applications where the GAN model is used. This manuscript also presents the performance of various GAN models for understanding its working with evaluation metrics (Qualitative and Quantitative).},
  language = {en},
  urldate  = {2023-04-13},
  journal  = {Multimedia Tools and Applications},
  author   = {Kumar, Lalit and Singh, Dushyant Kumar},
  month    = mar,
  year     = {2023},
  keywords = {Generative adversarial networks, Applications of GAN, Architectural-variants, Artificial intelligence, Computer vision},
  file     = {Full Text PDF:/Users/marcio/Zotero/storage/DVJ4J6MA/Kumar and Singh - 2023 - A comprehensive survey on generative adversarial n.pdf:application/pdf}
}

@article{iglesias_data_2023,
  title      = {Data {Augmentation} techniques in time series domain: a survey and taxonomy},
  issn       = {1433-3058},
  shorttitle = {Data {Augmentation} techniques in time series domain},
  url        = {https://doi.org/10.1007/s00521-023-08459-3},
  doi        = {10.1007/s00521-023-08459-3},
  abstract   = {With the latest advances in deep learning-based generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using data augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state of the art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will be analysed. The ultimate aim of this study is to provide a summary of the evolution and performance of areas that produce better results to guide future researchers in this field.},
  language   = {en},
  urldate    = {2023-04-13},
  journal    = {Neural Computing and Applications},
  author     = {Iglesias, Guillermo and Talavera, Edgar and González-Prieto, Angel and Mozo, Alberto and Gómez-Canaval, Sandra},
  month      = mar,
  year       = {2023},
  keywords   = {Deep learning, Generative models, Data augmentation, Time series},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/JFVR22I5/Iglesias et al. - 2023 - Data Augmentation techniques in time series domain.pdf:application/pdf}
}

@misc{ji_comprehensive_2020,
  title      = {A {Comprehensive} {Survey} on {Deep} {Music} {Generation}: {Multi}-level {Representations}, {Algorithms}, {Evaluations}, and {Future} {Directions}},
  shorttitle = {A {Comprehensive} {Survey} on {Deep} {Music} {Generation}},
  url        = {http://arxiv.org/abs/2011.06801},
  doi        = {10.48550/arXiv.2011.06801},
  abstract   = {The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.},
  urldate    = {2023-04-13},
  publisher  = {arXiv},
  author     = {Ji, Shulei and Luo, Jing and Yang, Xinyu},
  month      = nov,
  year       = {2020},
  note       = {arXiv:2011.06801 [cs, eess]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  annote     = {Comment: 96 pages,this is a draft},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/DDIVLIBA/Ji et al. - 2020 - A Comprehensive Survey on Deep Music Generation M.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/XC384MAV/2011.html:text/html}
}

@misc{broad_active_2021,
  title     = {Active {Divergence} with {Generative} {Deep} {Learning} -- {A} {Survey} and {Taxonomy}},
  url       = {http://arxiv.org/abs/2107.05599},
  doi       = {10.48550/arXiv.2107.05599},
  abstract  = {Generative deep learning systems offer powerful tools for artefact generation, given their ability to model distributions of data and generate high-fidelity results. In the context of computational creativity, however, a major shortcoming is that they are unable to explicitly diverge from the training data in creative ways and are limited to fitting the target data distribution. To address these limitations, there have been a growing number of approaches for optimising, hacking and rewriting these models in order to actively diverge from the training data. We present a taxonomy and comprehensive survey of the state of the art of active divergence techniques, highlighting the potential for computational creativity researchers to advance these methods and use deep generative models in truly creative systems.},
  urldate   = {2023-04-13},
  publisher = {arXiv},
  author    = {Broad, Terence and Berns, Sebastian and Colton, Simon and Grierson, Mick},
  month     = jul,
  year      = {2021},
  note      = {arXiv:2107.05599 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/NZ7GLJT4/Broad et al. - 2021 - Active Divergence with Generative Deep Learning --.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/IZ38TCZR/2107.html:text/html}
}

@article{roger_deep_2022,
  title      = {Deep neural networks for automatic speech processing: a survey from large corpora to limited data},
  volume     = {2022},
  issn       = {1687-4722},
  shorttitle = {Deep neural networks for automatic speech processing},
  url        = {https://doi.org/10.1186/s13636-022-00251-w},
  doi        = {10.1186/s13636-022-00251-w},
  abstract   = {Most state-of-the-art speech systems use deep neural networks (DNNs). These systems require a large amount of data to be learned. Hence, training state-of-the-art frameworks on under-resourced speech challenges are difficult tasks. As an example, a challenge could be the limited amount of data to model impaired speech. Furthermore, acquiring more data and/or expertise is time-consuming and expensive. In this paper, we focus on the following speech processing tasks: automatic speech recognition, speaker identification, and emotion recognition. To assess the problem of limited data, we firstly investigate state-of-the-art automatic speech recognition systems, as this is the hardest task (due to the wide variability in each language). Next, we provide an overview of techniques and tasks requiring fewer data. In the last section, we investigate few-shot techniques by interpreting under-resourced speech as a few-shot problem. In that sense, we propose an overview of few-shot techniques and the possibility of using such techniques for the speech problems addressed in this survey. It is true that the reviewed techniques are not well adapted for large datasets. Nevertheless, some promising results from the literature encourage the usage of such techniques for speech processing.},
  language   = {en},
  number     = {1},
  urldate    = {2023-04-13},
  journal    = {EURASIP Journal on Audio, Speech, and Music Processing},
  author     = {Roger, Vincent and Farinas, Jérôme and Pinquier, Julien},
  month      = aug,
  year       = {2022},
  keywords   = {Audio processing, Deep learning techniques, Deep neural networks, Few-shot learning, Speech analysis, Under-resourced languages},
  pages      = {19},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/YS7PKPQJ/Roger et al. - 2022 - Deep neural networks for automatic speech processi.pdf:application/pdf}
}

@article{kothari_literature_2022,
  title    = {Literature {Survey} for {Music} {Genre} {Classification} {Using} {Neural} {Network}},
  volume   = {09},
  language = {en},
  number   = {02},
  author   = {Kothari, Naman and Kumar, Pawan},
  year     = {2022},
  file     = {Kothari and Kumar - 2022 - Literature Survey for Music Genre Classification U.pdf:/Users/marcio/Zotero/storage/6NBF8B8I/Kothari and Kumar - 2022 - Literature Survey for Music Genre Classification U.pdf:application/pdf}
}

@article{bansal_environmental_2022,
  title      = {Environmental {Sound} {Classification}: {A} descriptive review of the literature},
  volume     = {16},
  issn       = {2667-3053},
  shorttitle = {Environmental {Sound} {Classification}},
  url        = {https://www.sciencedirect.com/science/article/pii/S2667305322000539},
  doi        = {10.1016/j.iswa.2022.200115},
  abstract   = {Automatic environmental sound classification (ESC) is one of the upcoming areas of research as most of the traditional studies are focused on speech and music signals. Classifying environmental sounds such as glass breaking, helicopter, baby crying and many more can aid in surveillance systems as well as criminal investigations. In this paper, a vast range of literature in the field of ESC is elucidated from various facets like preprocessing, feature extraction, and classification techniques. Researchers have used various noise removal and signal enhancement techniques to preprocess the signals. This paper explicates multitude of datasets used in recent studies along with the year of publication and maximum accuracy achieved with the dataset. Deep Neural Networks surpass the traditional machine learning classifiers. The future challenges and prospective research in this field are proposed. Since no recent review on ESC has been published, this study will open up novel ways for certain business applications and security systems.},
  language   = {en},
  urldate    = {2023-04-13},
  journal    = {Intelligent Systems with Applications},
  author     = {Bansal, Anam and Garg, Naresh Kumar},
  month      = nov,
  year       = {2022},
  keywords   = {Deep neural networks, Environmental Sound Classification, Feature extraction, Feature selection, Machine learning classifiers},
  pages      = {200115},
  file       = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/57G4QHBK/Bansal and Garg - 2022 - Environmental Sound Classification A descriptive .pdf:application/pdf}
}

@misc{xu_comprehensive_2022,
  title     = {A {Comprehensive} {Survey} of {Automated} {Audio} {Captioning}},
  url       = {http://arxiv.org/abs/2205.05357},
  doi       = {10.48550/arXiv.2205.05357},
  abstract  = {Automated audio captioning, a task that mimics human perception as well as innovatively links audio processing and natural language processing, has overseen much progress over the last few years. Audio captioning requires recognizing the acoustic scene, primary audio events and sometimes the spatial and temporal relationship between events in an audio clip. It also requires describing these elements by a fluent and vivid sentence. Deep learning-based approaches are widely adopted to tackle this problem. This current paper situates itself as a comprehensive review covering the benchmark datasets, existing deep learning techniques and the evaluation metrics in automated audio captioning.},
  urldate   = {2023-04-13},
  publisher = {arXiv},
  author    = {Xu, Xuenan and Wu, Mengyue and Yu, Kai},
  month     = may,
  year      = {2022},
  note      = {arXiv:2205.05357 [cs, eess]},
  keywords  = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/HA3M66BJ/Xu et al. - 2022 - A Comprehensive Survey of Automated Audio Captioni.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/AW7J3NTL/2205.html:text/html}
}

@article{chandrakala_environmental_2019,
  title      = {Environmental {Audio} {Scene} and {Sound} {Event} {Recognition} for {Autonomous} {Surveillance}: {A} {Survey} and {Comparative} {Studies}},
  volume     = {52},
  issn       = {0360-0300},
  shorttitle = {Environmental {Audio} {Scene} and {Sound} {Event} {Recognition} for {Autonomous} {Surveillance}},
  url        = {https://dl.acm.org/doi/10.1145/3322240},
  doi        = {10.1145/3322240},
  abstract   = {Monitoring of human and social activities is becoming increasingly pervasive in our living environment for public security and safety applications. The recognition of suspicious events is important in both indoor and outdoor environments, such as child-care centers, smart-homes, old-age homes, residential areas, office environments, elevators, and smart cities. Environmental audio scene and sound event recognition are the fundamental tasks involved in many audio surveillance applications. Although numerous approaches have been proposed, robust environmental audio surveillance remains a huge challenge due to various reasons, such as various types of overlapping audio sounds, background noises, and lack of universal and multi-modal datasets. The goal of this article is to review various features of representing audio scenes and sound events and provide appropriate machine learning algorithms for audio surveillance tasks. Benchmark datasets are categorized based on the real-world scenarios of audio surveillance applications. To have a quantitative understanding, some of the state-of-the-art approaches are evaluated based on two benchmark datasets for audio scenes and sound event recognition tasks. Finally, we outline the possible future directions for improving the recognition of environmental audio scenes and sound events.},
  number     = {3},
  urldate    = {2023-04-13},
  journal    = {ACM Computing Surveys},
  author     = {Chandrakala, S. and Jayalakshmi, S. L.},
  month      = jun,
  year       = {2019},
  keywords   = {acoustic source localization, audio features, audio tagging, environmental audio scene recognition, Environmental audio surveillance, sound event recognition},
  pages      = {63:1--63:34},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/7KWYPCRM/Chandrakala and Jayalakshmi - 2019 - Environmental Audio Scene and Sound Event Recognit.pdf:application/pdf}
}

@article{kumar_deep_2023,
  title      = {A deep learning approaches in text-to-speech system: a systematic review and recent research perspective},
  volume     = {82},
  issn       = {1573-7721},
  shorttitle = {A deep learning approaches in text-to-speech system},
  url        = {https://doi.org/10.1007/s11042-022-13943-4},
  doi        = {10.1007/s11042-022-13943-4},
  abstract   = {Text-to-speech systems (TTS) have come a long way in the last decade and are now a popular research topic for creating various human-computer interaction systems. Although, a range of speech synthesis models for various languages with several motive applications is available based on domain requirements. However, recent developments in speech synthesis have primarily attributed to deep learning-based techniques that have improved a variety of application scenarios, including intelligent speech interaction, chatbots, and conversational artificial intelligence (AI). Text-to-speech systems are discussed in this survey article as an active topic of study that has achieved significant progress in the recent decade, particularly for Indian and non-Indian languages. Furthermore, the study also covers the lifecycle of text-to-speech systems as well as developed platforms in it. We performed an efficient search for published survey articles up to May 2021 in the web of science, PubMed, Scopus, EBSCO(Elton B. Stephens CO (company)) and Google Scholar for Text-to-speech Systems (TTS) in various languages based on different approaches. This survey article offers a study of the contributions made by various researchers in Indian and non-Indian language text-to-speech systems and the techniques used to implement it with associated challenges in designing TTS systems. The work also compared different language text-to-speech systems based on the quality metrics such as recognition rate, accuracy, TTS score, precision, recall, and F1-score. Further, the study summarizes existing ideas and their shortcomings, emphasizing the scope of future research in Indian and non-Indian languages TTS, which may assist beginners in designing robust TTS systems.},
  language   = {en},
  number     = {10},
  urldate    = {2023-04-13},
  journal    = {Multimedia Tools and Applications},
  author     = {Kumar, Yogesh and Koul, Apeksha and Singh, Chamkaur},
  month      = apr,
  year       = {2023},
  keywords   = {Deep learning, Text-to-speech, Speech synthesis, Artificial intelligence, Linguistic analysis, Pronunciation generation},
  pages      = {15171--15197},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/Y9G33M32/Kumar et al. - 2023 - A deep learning approaches in text-to-speech syste.pdf:application/pdf}
}

@article{kaur_conventional_2022,
  title      = {Conventional and contemporary approaches used in text to speech synthesis: a review},
  issn       = {1573-7462},
  shorttitle = {Conventional and contemporary approaches used in text to speech synthesis},
  url        = {https://doi.org/10.1007/s10462-022-10315-0},
  doi        = {10.1007/s10462-022-10315-0},
  abstract   = {Nowadays speech synthesis or text to speech (TTS), an ability of system to produce human like natural sounding voice from the written text, is gaining popularity in the field of speech processing. For any TTS, intelligibility and naturalness are the two important measures for defining the quality of a synthesized sound which is highly dependent on the prosody modeling using acoustic model of synthesizer. The purpose of this review survey is firstly to study and analyze the various approaches used traditionally (articulatory synthesis, formant synthesis, concatenative speech synthesis and statistical parametric techniques based on hidden Markov model) and recently (statistical parametric based on deep learning approaches) for acoustic modeling with their pros and cons. The approaches based on deep learning to build the acoustic model has significantly contributed to the advancement of TTS as models based on deep learning are capable of modelling the complex context dependencies in the input data. Apart from these, this article also reviews the TTS approaches for generating speech with different voices and emotions to makes the TTS more realistic to use. It also addresses the subjective and objective metrics used to measure the quality of the synthesized voice. Various well known speech synthesis systems based on autoregressive and non-autoregressive models such as Tacotron, Deep Voice, WaveNet, Parallel WaveNet, Parallel Tacotron, FastSpeech by global tech-giant Google, Facebook, Microsoft employed the architecture of deep learning for end-to-end speech waveform generation and attained a remarkable mean opinion score (MOS).},
  language   = {en},
  urldate    = {2023-04-13},
  journal    = {Artificial Intelligence Review},
  author     = {Kaur, Navdeep and Singh, Parminder},
  month      = nov,
  year       = {2022},
  keywords   = {Articulatory speech synthesis, Autoregressive and non-autoregressive models, Concatenative speech synthesis, Expressive TTS, Formant speech synthesis, Multi-lingual and multi-speaker TTS, Speech corpus, Speech quality metric, Statistical parametric speech synthesis using hidden Markov model and deep learning methods},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/GFEFZBC6/Kaur and Singh - 2022 - Conventional and contemporary approaches used in t.pdf:application/pdf}
}

@inproceedings{abufadda_survey_2021,
  title     = {A {Survey} of {Synthetic} {Data} {Generation} for {Machine} {Learning}},
  doi       = {10.1109/ACIT53391.2021.9677302},
  abstract  = {Data is the fuel of machine learning algorithms, therefore data generation in machine learning is becoming an important topic. The problem is that finding enough data for machine learning algorithms in some domains or situations is difficult. For example, some data may invade the privacy of people or some other datasets can be related to national security and difficult to be unveiled. This paper reviews the related work in synthetic data generation in terms of available methods for data generation (augmentation) and how the generated data helps in improving the performance of machine learning algorithms. The main focus of this paper is data synthetic methods in the healthcare domain.},
  booktitle = {2021 22nd {International} {Arab} {Conference} on {Information} {Technology} ({ACIT})},
  author    = {Abufadda, Mohammad and Mansour, Khalid},
  month     = dec,
  year      = {2021},
  keywords  = {Machine learning, data generation, Data privacy, Fuels, healthcare, Information technology, machine learning, Machine learning algorithms, Medical services, National security},
  pages     = {1--7},
  file      = {Abufadda and Mansour - 2021 - A Survey of Synthetic Data Generation for Machine .pdf:/Users/marcio/Zotero/storage/TKKZNGUT/Abufadda and Mansour - 2021 - A Survey of Synthetic Data Generation for Machine .pdf:application/pdf}
}

@article{liu_audio_2022,
  title      = {Audio self-supervised learning: {A} survey},
  volume     = {3},
  issn       = {2666-3899},
  shorttitle = {Audio self-supervised learning},
  url        = {https://www.sciencedirect.com/science/article/pii/S2666389922002410},
  doi        = {10.1016/j.patter.2022.100616},
  abstract   = {Similar to humans’ cognitive ability to generalize knowledge and skills, self-supervised learning (SSL) targets discovering general representations from large-scale data. This, through the use of pre-trained SSL models for downstream tasks, alleviates the need for human annotation, which is an expensive and time-consuming task. Its success in the fields of computer vision and natural language processing have prompted its recent adoption into the field of audio and speech processing. Comprehensive reviews summarizing the knowledge in audio SSL are currently missing. To fill this gap, we provide an overview of the SSL methods used for audio and speech processing applications. Herein, we also summarize the empirical works that exploit audio modality in multi-modal SSL frameworks and the existing suitable benchmarks to evaluate the power of SSL in the computer audition domain. Finally, we discuss some open problems and point out the future directions in the development of audio SSL.},
  language   = {en},
  number     = {12},
  urldate    = {2023-04-13},
  journal    = {Patterns},
  author     = {Liu, Shuo and Mallol-Ragolta, Adria and Parada-Cabaleiro, Emilia and Qian, Kun and Jing, Xin and Kathan, Alexander and Hu, Bin and Schuller, Björn W.},
  month      = dec,
  year       = {2022},
  keywords   = {audio and speech processing, multi-modal SSL, representation learning, self-supervised learning, unsupervised learning},
  pages      = {100616},
  file       = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/HM2JX2P3/Liu et al. - 2022 - Audio self-supervised learning A survey.pdf:application/pdf}
}

@article{fernandez_ai_2013,
  title      = {{AI} {Methods} in {Algorithmic} {Composition}: {A} {Comprehensive} {Survey}},
  volume     = {48},
  copyright  = {Copyright (c)},
  issn       = {1076-9757},
  shorttitle = {{AI} {Methods} in {Algorithmic} {Composition}},
  url        = {https://www.jair.org/index.php/jair/article/view/10845},
  doi        = {10.1613/jair.3908},
  abstract   = {Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.},
  language   = {en},
  urldate    = {2023-04-13},
  journal    = {Journal of Artificial Intelligence Research},
  author     = {Fernandez, J. D. and Vico, F.},
  month      = nov,
  year       = {2013},
  pages      = {513--582},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/N7CXYFX9/Fernandez and Vico - 2013 - AI Methods in Algorithmic Composition A Comprehen.pdf:application/pdf}
}

@article{lionello_systematic_2020,
  title    = {A systematic review of prediction models for the experience of urban soundscapes},
  volume   = {170},
  issn     = {0003-682X},
  url      = {https://www.sciencedirect.com/science/article/pii/S0003682X20305831},
  doi      = {10.1016/j.apacoust.2020.107479},
  abstract = {A systematic review for soundscape modelling methods is presented. The methods for developing soundscape models are hereby questioned by investigating the following aspects: data acquisition methods, indicators used as predictors of descriptors in the models, descriptors targeted as output of the models, linear rather than non-linear model fitting, and overall performances. The inclusion criteria for the reviewed studies were: models dealing with soundscape dimensions aligned with the definitions provided in the ISO 12913 series; models based on soundscape data sampled at least at two different locations and using at least two variables as indicators. The Scopus database was queried. Biases on papers selection were considered and those related to the methods are discussed in the current study. Out of 256 results from Scopus, 22 studies were selected. Two studies were included from the references among the results. The data extraction from the 24 studies includes: data collection methods, input and output for the models, and model performance. Three main data collection methods were found. Several studies focus on the different combination of indicators among physical measurements, perceptual evaluations, temporal dynamics, demographic and psychological information, context information and visual amenity. The descriptors considered across the studies include: acoustic comfort, valence, arousal, calmness, chaoticness, sound quality, tranquillity, and vibrancy. The interpretation of the results is limited by the large variety of methods, and the large number of parameters in spite of a limited amount of studies obtained from the query. However, perceptual indicators, visual and contextual indicators, as well as time dynamic embedding, overall provide a better prediction of soundscape. Finally, although the compared performance between linear and non-linear methods does not show remarkable differences, non-linear methods might still represent a more suitable choice in models where complex structures of indicators are used.},
  language = {en},
  urldate  = {2023-04-13},
  journal  = {Applied Acoustics},
  author   = {Lionello, Matteo and Aletta, Francesco and Kang, Jian},
  month    = dec,
  year     = {2020},
  keywords = {Literature review, Soundscape indices, Soundscape modelling, Urban soundscape},
  pages    = {107479},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/C88AV4HG/Lionello et al. - 2020 - A systematic review of prediction models for the e.pdf:application/pdf}
}

@article{alias_review_2016,
  title     = {A {Review} of {Physical} and {Perceptual} {Feature} {Extraction} {Techniques} for {Speech}, {Music} and {Environmental} {Sounds}},
  volume    = {6},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2076-3417},
  url       = {https://www.mdpi.com/2076-3417/6/5/143},
  doi       = {10.3390/app6050143},
  abstract  = {Endowing machines with sensing capabilities similar to those of humans is a prevalent quest in engineering and computer science. In the pursuit of making computers sense their surroundings, a huge effort has been conducted to allow machines and computers to acquire, process, analyze and understand their environment in a human-like way. Focusing on the sense of hearing, the ability of computers to sense their acoustic environment as humans do goes by the name of machine hearing. To achieve this ambitious aim, the representation of the audio signal is of paramount importance. In this paper, we present an up-to-date review of the most relevant audio feature extraction techniques developed to analyze the most usual audio signals: speech, music and environmental sounds. Besides revisiting classic approaches for completeness, we include the latest advances in the field based on new domains of analysis together with novel bio-inspired proposals. These approaches are described following a taxonomy that organizes them according to their physical or perceptual basis, being subsequently divided depending on the domain of computation (time, frequency, wavelet, image-based, cepstral, or other domains). The description of the approaches is accompanied with recent examples of their application to machine hearing related problems.},
  language  = {en},
  number    = {5},
  urldate   = {2023-04-13},
  journal   = {Applied Sciences},
  author    = {Alías, Francesc and Socoró, Joan Claudi and Sevillano, Xavier},
  month     = may,
  year      = {2016},
  note      = {Number: 5
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {audio analysis, audio feature extraction, environmental sound, machine hearing, music, speech},
  pages     = {143},
  file      = {Full Text PDF:/Users/marcio/Zotero/storage/KW7XJYLA/Alías et al. - 2016 - A Review of Physical and Perceptual Feature Extrac.pdf:application/pdf}
}

@article{nogueira_sound_2022,
  title      = {Sound {Classification} and {Processing} of {Urban} {Environments}: {A} {Systematic} {Literature} {Review}},
  volume     = {22},
  issn       = {1424-8220},
  shorttitle = {Sound {Classification} and {Processing} of {Urban} {Environments}},
  url        = {https://www.mdpi.com/1424-8220/22/22/8608},
  doi        = {10.3390/s22228608},
  abstract   = {Audio recognition can be used in smart cities for security, surveillance, manufacturing, autonomous vehicles, and noise mitigation, just to name a few. However, urban sounds are everyday audio events that occur daily, presenting unstructured characteristics containing different genres of noise and sounds unrelated to the sound event under study, making it a challenging problem. Therefore, the main objective of this literature review is to summarize the most recent works on this subject to understand the current approaches and identify their limitations. Based on the reviewed articles, it can be realized that Deep Learning (DL) architectures, attention mechanisms, data augmentation techniques, and pretraining are the most crucial factors to consider while creating an efﬁcient sound classiﬁcation model. The best-found results were obtained by Mushtaq and Su, in 2020, using a DenseNet-161 with pretrained weights from ImageNet, and NA-1 and NA-2 as augmentation techniques, which were of 97.98\%, 98.52\%, and 99.22\% for UrbanSound8K, ESC-50, and ESC-10 datasets, respectively. Nonetheless, the use of these models in real-world scenarios has not been properly addressed, so their effectiveness is still questionable in such situations.},
  language   = {en},
  number     = {22},
  urldate    = {2023-04-16},
  journal    = {Sensors},
  author     = {Nogueira, Ana Filipa Rodrigues and Oliveira, Hugo S. and Machado, José J. M. and Tavares, João Manuel R. S.},
  month      = nov,
  year       = {2022},
  pages      = {8608},
  file       = {Nogueira et al. - 2022 - Sound Classification and Processing of Urban Envir.pdf:/Users/marcio/Zotero/storage/DPDRNXD7/Nogueira et al. - 2022 - Sound Classification and Processing of Urban Envir.pdf:application/pdf}
}

@misc{ronneberger_u-net_2015,
  title      = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
  shorttitle = {U-{Net}},
  url        = {http://arxiv.org/abs/1505.04597},
  doi        = {10.48550/arXiv.1505.04597},
  abstract   = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  urldate    = {2023-04-16},
  publisher  = {arXiv},
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  month      = may,
  year       = {2015},
  note       = {arXiv:1505.04597 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: conditionally accepted at MICCAI 2015},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/TN5IJM6W/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/FT935A7U/1505.html:text/html}
}

@misc{yang_diffsound_2022,
  title      = {Diffsound: {Discrete} {Diffusion} {Model} for {Text}-to-sound {Generation}},
  shorttitle = {Diffsound},
  url        = {http://arxiv.org/abs/2207.09983},
  doi        = {10.48550/arXiv.2207.09983},
  abstract   = {Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 {\textbackslash}textit\{v.s\} 2.786, and the generation speed is five times faster than the AR decoder.},
  urldate    = {2023-04-17},
  publisher  = {arXiv},
  author     = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
  month      = jul,
  year       = {2022},
  note       = {arXiv:2207.09983 [cs, eess]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence},
  annote     = {Comment: Submitted to TASLP2022},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/3GVJV582/Yang et al. - 2022 - Diffsound Discrete Diffusion Model for Text-to-so.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/8M6P9KFJ/2207.html:text/html}
}

@misc{research_infinite_2021,
  title    = {The {Infinite} {Dial} 2021},
  url      = {https://www.edisonresearch.com/the-infinite-dial-2021-2/},
  abstract = {Eighty Million Americans Listen to Podcasts Weekly with Most Diverse Audience Ever; Online Audio Listening and Smart Speaker Ownership Show Continued Growth Click here to download The Infinite Dial 2021 Click here for The State of Podcast Listening for 2021: Podcasting Finds a Way, from The Infinite Dial 2021 Scroll down to view a recording of  The Infinite Dial 2021 Podcasting continues its significant and steady growth while its overall audience is more diverse than …},
  language = {en-US},
  urldate  = {2023-04-26},
  journal  = {Edison Research},
  author   = {Research, Edison},
  month    = mar,
  year     = {2021},
  note     = {Section: Featured},
  file     = {Snapshot:/Users/marcio/Zotero/storage/CCS26K6G/the-infinite-dial-2021-2.html:text/html}
}

@book{schafer_tuning_1977,
  title     = {The {Tuning} of the {World}},
  isbn      = {978-0-394-40966-5},
  abstract  = {The soundscape is our acoustic environment, the ever-present noises with which we all live. The author suggests that we now suffer from acoustical overload and are less able to hear the nuances and subtleties of sound. Our task, he maintains, is to listen, analyze and make distinctions in spite of sound pollution.},
  language  = {en},
  publisher = {Knopf},
  author    = {Schafer, R. Murray},
  year      = {1977},
  note      = {Google-Books-ID: SIufAAAAMAAJ}
}

@misc{international_organization_for_standardization_iso_2014,
  title   = {{ISO} 12913-1:2014(en), {Acoustics} — {Soundscape}},
  url     = {https://www.iso.org/obp/ui/#iso:std:iso:12913:-1:ed-1:v1:en},
  urldate = {2023-04-26},
  author  = {{International Organization for Standardization}},
  year    = {2014},
  file    = {ISO 12913-1\:2014(en), Acoustics — Soundscape — Part 1\: Definition and conceptual framework:/Users/marcio/Zotero/storage/L8E6R3GN/ui.html:text/html}
}

@book{sonnenschein_sound_2001,
  title      = {Sound {Design}: {The} {Expressive} {Power} of {Music}, {Voice}, and {Sound} {Effects} in {Cinema}},
  isbn       = {978-0-941188-26-5},
  shorttitle = {Sound {Design}},
  abstract   = {The clash of light sabers in the electrifying duels of Star Wars. The chilling bass line signifying the lurking menace of the shark in Jaws. The otherworldly yet familiar pleas to "go home" in the enchanting E.T. These are examples of the many different ways in which sound can contribute to the overall dramatic impact of a film.},
  language   = {en},
  publisher  = {Michael Wiese Productions},
  author     = {Sonnenschein, David},
  year       = {2001},
  keywords   = {Performing Arts / Film / Direction \& Production, Performing Arts / Film / General}
}

@inproceedings{bernardes_seed_2016,
  title      = {Seed: {Resynthesizing} environmental sounds from examples},
  shorttitle = {{SEED}},
  doi        = {10.5281/zenodo.851183},
  abstract   = {In this paper we present SEED, a generative system capable of arbitrarily extending recorded environmental sounds while preserving their inherent structure. The system architecture is grounded in concepts from concatena-tive sound synthesis and includes three top-level modules for segmentation, analysis, and generation. An input audio signal is first temporally segmented into a collection of audio segments, which are then reduced into a dictionary of audio classes by means of an agglomerative clustering algorithm. This representation, together with a concatenation cost between audio segment boundaries, is finally used to generate sequences of audio segments with arbitrarily long duration. The system output can be varied in the generation process by the simple and yet effective parametric control over the creation of the natural, temporally coherent, and varied audio renderings of environmental sounds.},
  booktitle  = {Proceedings of the {Sound} and {Music} {Computing} {Conference}},
  author     = {Bernardes, Gilberto and Aly, Luis and Davies, Matthew},
  month      = sep,
  year       = {2016},
  pages      = {55--62},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/YZ8F62EW/Bernardes et al. - 2016 - SEED Resynthesizing environmental sounds from exa.pdf:application/pdf}
}

@inproceedings{strobl_sound_2006,
  title     = {Sound {Texture} {Modeling}: {A} {Survey},},
  abstract  = {Sound texture modeling is a widely used concept in computer music, that has its well analyzed counterpart in image processing. We report on the current state of different sound texture generation methods and try to outline common problems of the sound texture examples. Published results pursue different kinds of analysis /re-synthesis approaches that can be divided into methods that try to transfer existing techniques from computer graphics and methods that take advantage of well-known techniques found in common computer music systems. Furthermore we present the idea of a new texture generator framework, where different analysis and synthesis tools can be combined and tested with the goal of producing high quality sound examples.},
  language  = {en},
  booktitle = {Proceedings of the {Sound} and {Music} {Computing} {Conference}},
  author    = {Strobl, Gerda and Eckel, Gerhard and Rocchesso, Davide},
  year      = {2006},
  pages     = {61--65},
  file      = {Strobl et al. - SOUND TEXTURE MODELING A SURVEY.pdf:/Users/marcio/Zotero/storage/D5ISQXI2/Strobl et al. - SOUND TEXTURE MODELING A SURVEY.pdf:application/pdf}
}

@inproceedings{salamon_scaper_2017-1,
  address    = {New Paltz, NY},
  title      = {Scaper: {A} library for soundscape synthesis and augmentation},
  isbn       = {978-1-5386-1632-1},
  shorttitle = {Scaper},
  url        = {http://ieeexplore.ieee.org/document/8170052/},
  doi        = {10.1109/WASPAA.2017.8170052},
  abstract   = {Sound event detection (SED) in environmental recordings is a key topic of research in machine listening, with applications in noise monitoring for smart cities, self-driving cars, surveillance, bioacoustic monitoring, and indexing of large multimedia collections. Developing new solutions for SED often relies on the availability of strongly labeled audio recordings, where the annotation includes the onset, offset and source of every event. Generating such precise annotations manually is very time consuming, and as a result existing datasets for SED with strong labels are scarce and limited in size. To address this issue, we present Scaper, an open-source library for soundscape synthesis and augmentation. Given a collection of isolated sound events, Scaper acts as a high-level sequencer that can generate multiple soundscapes from a single, probabilistically deﬁned, “speciﬁcation”. To increase the variability of the output, Scaper supports the application of audio transformations such as pitch shifting and time stretching individually to every event. To illustrate the potential of the library, we generate a dataset of 10,000 soundscapes and use it to compare the performance of two state-of-the-art algorithms, including a breakdown by soundscape characteristics. We also describe how Scaper was used to generate audio stimuli for an audio labeling crowdsourcing experiment, and conclude with a discussion of Scaper’s limitations and potential applications.},
  language   = {en},
  urldate    = {2023-04-27},
  booktitle  = {2017 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics} ({WASPAA})},
  publisher  = {IEEE},
  author     = {Salamon, Justin and MacConnell, Duncan and Cartwright, Mark and Li, Peter and Bello, Juan Pablo},
  month      = oct,
  year       = {2017},
  pages      = {344--348},
  file       = {Salamon et al. - 2017 - Scaper A library for soundscape synthesis and aug.pdf:/Users/marcio/Zotero/storage/KKAPZTZ7/Salamon et al. - 2017 - Scaper A library for soundscape synthesis and aug.pdf:application/pdf}
}

@inproceedings{finney_soundscape_2010,
  title     = {Soundscape generation for virtual environments using community-provided audio databases},
  booktitle = {{W3C} {Workshop}: {Augmented} {Reality} on the {Web}},
  publisher = {Barcelona Spain},
  author    = {Finney, Nathaniel and Janer, Jordi},
  year      = {2010}
}

@article{flanagan_phase_1966,
  title    = {Phase {Vocoder}},
  volume   = {45},
  issn     = {1538-7305},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1966.tb01706.x},
  doi      = {10.1002/j.1538-7305.1966.tb01706.x},
  abstract = {Avocoder technique is described in which speech signals are represented by their short-time phase and amplitude spectra. A complete transmission system utilizing this approach is simulated on a digital computer. The encoding method leads to an economy in transmission bandwidth and to a means for time compression and expansion of speech signals.},
  language = {en},
  number   = {9},
  urldate  = {2023-05-16},
  journal  = {Bell System Technical Journal},
  author   = {Flanagan, J. L. and Golden, R. M.},
  year     = {1966},
  note     = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1966.tb01706.x},
  pages    = {1493--1509},
  file     = {Snapshot:/Users/marcio/Zotero/storage/HKU5Y4MX/j.1538-7305.1966.tb01706.html:text/html}
}

@article{willmott_advantages_2005,
  title    = {Advantages of the mean absolute error ({MAE}) over the root mean square error ({RMSE}) in assessing average model performance},
  volume   = {30},
  issn     = {0936-577X, 1616-1572},
  url      = {https://www.int-res.com/abstracts/cr/v30/n1/p79-82/},
  doi      = {10.3354/cr030079},
  abstract = {The relative abilities of 2, dimensioned statistics—the root-mean-square error (RMSE) and the mean absolute error (MAE)—to describe average model-performance error are examined. The RMSE is of special interest because it is widely reported in the climatic and environmental literature; nevertheless, it is an inappropriate and misinterpreted measure of average error. RMSE is inappropriate because it is a function of 3 characteristics of a set of errors, rather than of one (the average error). RMSE varies with the variability within the distribution of error magnitudes and with the square root of the number of errors (n1/2), as well as with the average-error magnitude (MAE). Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous. Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE.},
  language = {en},
  number   = {1},
  urldate  = {2023-05-17},
  journal  = {Climate Research},
  author   = {Willmott, Cort J. and Matsuura, Kenji},
  month    = dec,
  year     = {2005},
  keywords = {Mean absolute error, Model-performance measures, Root-mean-square error},
  pages    = {79--82},
  file     = {Full Text PDF:/Users/marcio/Zotero/storage/5KG7SDAF/Willmott and Matsuura - 2005 - Advantages of the mean absolute error (MAE) over t.pdf:application/pdf}
}

@misc{vinay_evaluating_2022,
  title     = {Evaluating generative audio systems and their metrics},
  url       = {http://arxiv.org/abs/2209.00130},
  doi       = {10.48550/arXiv.2209.00130},
  abstract  = {Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.},
  urldate   = {2023-05-18},
  publisher = {arXiv},
  author    = {Vinay, Ashvala and Lerch, Alexander},
  month     = aug,
  year      = {2022},
  note      = {arXiv:2209.00130 [cs, eess]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  annote    = {Comment: Accepted at ISMIR 2022},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/EI3AGZDD/Vinay and Lerch - 2022 - Evaluating generative audio systems and their metr.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/FTUM43RY/2209.html:text/html}
}

@article{hodson_mean_2021,
  title    = {Mean {Squared} {Error}, {Deconstructed}},
  volume   = {13},
  issn     = {1942-2466},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002681},
  doi      = {10.1029/2021MS002681},
  abstract = {As science becomes increasingly cross-disciplinary and scientific models become increasingly cross-coupled, standardized practices of model evaluation are more important than ever. For normally distributed data, mean squared error (MSE) is ideal as an objective measure of model performance, but it gives little insight into what aspects of model performance are “good” or “bad.” This apparent weakness has led to a myriad of specialized error metrics, which are sometimes aggregated to form a composite score. Such scores are inherently subjective, however, and while their components may be interpretable, the composite itself is not. We contend that, a better approach to model benchmarking and interpretation is to decompose MSE into interpretable components. To demonstrate the versatility of this approach, we outline some fundamental types of decomposition and apply them to predictions at 1,021 streamgages across the conterminous United States from three streamflow models. Through this demonstration, we hope to show that each component in a decomposition represents a distinct concept, like “season” or “variability,” and that simple decompositions can be combined to represent more complex concepts, like “seasonal variability,” creating an expressive language through which to interrogate models and data.},
  language = {en},
  number   = {12},
  urldate  = {2023-05-18},
  journal  = {Journal of Advances in Modeling Earth Systems},
  author   = {Hodson, Timothy O. and Over, Thomas M. and Foks, Sydney S.},
  year     = {2021},
  note     = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021MS002681},
  keywords = {bias, decomposition, mean squared error, model benchmarking, variance},
  pages    = {e2021MS002681},
  annote   = {e2021MS002681 2021MS002681},
  file     = {Full Text PDF:/Users/marcio/Zotero/storage/TCD6NI4A/Hodson et al. - 2021 - Mean Squared Error, Deconstructed.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/8CXJM5W7/2021MS002681.html:text/html}
}

@article{blei_variational_2017,
  title      = {Variational {Inference}: {A} {Review} for {Statisticians}},
  volume     = {112},
  issn       = {0162-1459, 1537-274X},
  shorttitle = {Variational {Inference}},
  url        = {http://arxiv.org/abs/1601.00670},
  doi        = {10.1080/01621459.2017.1285773},
  abstract   = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  number     = {518},
  urldate    = {2023-05-18},
  journal    = {Journal of the American Statistical Association},
  author     = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  month      = apr,
  year       = {2017},
  note       = {arXiv:1601.00670 [cs, stat]},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
  pages      = {859--877},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/4N3Z6XFL/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/DBJXRUTC/1601.html:text/html}
}

@misc{tjandra_vqvae_2019,
  title     = {{VQVAE} {Unsupervised} {Unit} {Discovery} and {Multi}-scale {Code2Spec} {Inverter} for {Zerospeech} {Challenge} 2019},
  url       = {http://arxiv.org/abs/1905.11449},
  doi       = {10.48550/arXiv.1905.11449},
  abstract  = {We describe our submitted system for the ZeroSpeech Challenge 2019. The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice. Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice. To tackle these problems and achieve the best trade-off, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-to-spectrogram (Code2Spec) inverter trained by mean square error and adversarial loss. The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation. Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE. In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates. Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.},
  urldate   = {2023-05-18},
  publisher = {arXiv},
  author    = {Tjandra, Andros and Sisman, Berrak and Zhang, Mingyang and Sakti, Sakriani and Li, Haizhou and Nakamura, Satoshi},
  month     = may,
  year      = {2019},
  note      = {arXiv:1905.11449 [cs, eess]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
  annote    = {Comment: Submitted to Interspeech 2019},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/IA3E4NS8/Tjandra et al. - 2019 - VQVAE Unsupervised Unit Discovery and Multi-scale .pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/5VSLQYEF/1905.html:text/html}
}

@article{chandrasekera_virtual_2015,
  title      = {Virtual environments with soundscapes: a study on immersion and effects of spatial abilities},
  volume     = {42},
  issn       = {0265-8135},
  shorttitle = {Virtual environments with soundscapes},
  url        = {https://doi.org/10.1068/b130087p},
  doi        = {10.1068/b130087p},
  abstract   = {In this study we explore how soundscapes can be used as navigational aids in virtual environments and empirically investigate the correlation between immersiveness in virtual environments and spatial abilities when soundscapes are used as landmarks for wayfinding. We attempt to advance knowledge regarding auditory cues contributing to enhanced immersive and navigational experience in virtual environments. Findings are likely to be utilized in effective design for physical environments and wayfinding.},
  language   = {en},
  number     = {6},
  urldate    = {2023-05-23},
  journal    = {Environment and Planning B: Planning and Design},
  author     = {Chandrasekera, Tilanka and Yoon, So-Yeon and D'Souza, Newton},
  month      = nov,
  year       = {2015},
  note       = {Publisher: SAGE Publications Ltd STM},
  pages      = {1003--1019},
  file       = {SAGE PDF Full Text:/Users/marcio/Zotero/storage/3BTJK3Y5/Chandrasekera et al. - 2015 - Virtual environments with soundscapes a study on .pdf:application/pdf}
}

@inproceedings{hoskinson_manipulation_2001,
  title     = {Manipulation and {Resynthesis} with {Natural} {Grains}},
  url       = {https://hdl.handle.net/2027/spo.bbp2372.2001.057},
  booktitle = {Proceedings of the 2001 {International} {Computer} {Music} {Conference}, {ICMC} 2001, {Havana}, {Cuba}, {September} 17-22, 2001},
  publisher = {Michigan Publishing},
  author    = {Hoskinson, Reynald and Pai, Dinesh K.},
  year      = {2001},
  file      = {Hoskinson and Pai - Manipulation and Resynthesis with Natural Grains.pdf:/Users/marcio/Zotero/storage/MXMFWKZ3/manipulation-and-resynthesis-with-natural-grains.pdf:application/pdf}
}

@article{shorten_text_2021,
  title    = {Text {Data} {Augmentation} for {Deep} {Learning}},
  volume   = {8},
  issn     = {2196-1115},
  url      = {https://doi.org/10.1186/s40537-021-00492-0},
  doi      = {10.1186/s40537-021-00492-0},
  abstract = {Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with the major motifs of Data Augmentation summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form. We follow these motifs with a concrete list of augmentation frameworks that have been developed for text data. Deep Learning generally struggles with the measurement of generalization and characterization of overfitting. We highlight studies that cover how augmentations can construct test sets for generalization. NLP is at an early stage in applying Data Augmentation compared to Computer Vision. We highlight the key differences and promising ideas that have yet to be tested in NLP. For the sake of practical implementation, we describe tools that facilitate Data Augmentation such as the use of consistency regularization, controllers, and offline and online augmentation pipelines, to preview a few. Finally, we discuss interesting topics around Data Augmentation in NLP such as task-specific augmentations, the use of prior knowledge in self-supervised learning versus Data Augmentation, intersections with transfer and multi-task learning, and ideas for AI-GAs (AI-Generating Algorithms). We hope this paper inspires further research interest in Text Data Augmentation.},
  number   = {1},
  urldate  = {2023-05-30},
  journal  = {Journal of Big Data},
  author   = {Shorten, Connor and Khoshgoftaar, Taghi M. and Furht, Borko},
  month    = jul,
  year     = {2021},
  keywords = {Data Augmentation, Big Data, Natural Language Processing, NLP, Overfitting, Text Data},
  pages    = {101},
  file     = {Full Text PDF:/Users/marcio/Zotero/storage/MN3FNP3L/Shorten et al. - 2021 - Text Data Augmentation for Deep Learning.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/I3BY6LV5/s40537-021-00492-0.html:text/html}
}

@inproceedings{wei_eda_2019,
  address   = {Hong Kong, China},
  title     = {{EDA}: {Easy} {Data} {Augmentation} {Techniques} for {Boosting} {Performance} on {Text} {Classification} {Tasks}},
  url       = {https://aclanthology.org/D19-1670},
  doi       = {10.18653/v1/D19-1670},
  abstract  = {We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author    = {Wei, Jason and Zou, Kai},
  month     = nov,
  year      = {2019},
  pages     = {6382--6388},
  file      = {Full Text:/Users/marcio/Zotero/storage/6QWJYQYN/Wei and Zou - 2019 - EDA Easy Data Augmentation Techniques for Boostin.pdf:application/pdf}
}

@article{ahmed_text_2023,
  title    = {Text augmentation using a graph-based approach and clonal selection algorithm},
  volume   = {11},
  issn     = {2666-8270},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666827023000051},
  doi      = {10.1016/j.mlwa.2023.100452},
  abstract = {Annotated data is critical for machine learning models, but producing large amounts of data with high-quality labeling is a time-consuming and labor-intensive process. Natural language processing (NLP) and machine learning models have traditionally relied on the labels given by human annotators with varying degrees of competency, training, and experience. These kinds of labels are incredibly problematic because they are defined and enforced by arbitrary and ambiguous standards. In order to solve these issues of insufficient high-quality labels, researchers are now investigating automated methods for enhancing training and testing data sets. In this paper, we demonstrate how our proposed method improves the quality and quantity of data in two cybersecurity problems (fake news identification \& sensitive data leak) by employing the clonal selection algorithm (CLONALG) and abstract meaning representation (AMR) graphs, and how it improves the performance of a classifier by at least 5\% on two datasets.},
  language = {en},
  urldate  = {2023-05-31},
  journal  = {Machine Learning with Applications},
  author   = {Ahmed, Hadeer and Traore, Issa and Mamun, Mohammad and Saad, Sherif},
  month    = mar,
  year     = {2023},
  keywords = {Data augmentation, Clonal selection, Cybersecurity, Text generation, Unstructured data},
  pages    = {100452},
  file     = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/EL4CMKIQ/Ahmed et al. - 2023 - Text augmentation using a graph-based approach and.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/ZJSIK2LY/S2666827023000051.html:text/html}
}

@misc{guo_augmenting_2019,
  title      = {Augmenting {Data} with {Mixup} for {Sentence} {Classification}: {An} {Empirical} {Study}},
  shorttitle = {Augmenting {Data} with {Mixup} for {Sentence} {Classification}},
  url        = {http://arxiv.org/abs/1905.08941},
  doi        = {10.48550/arXiv.1905.08941},
  abstract   = {Mixup, a recent proposed data augmentation method through linearly interpolating inputs and modeling targets of random samples, has demonstrated its capability of significantly improving the predictive accuracy of the state-of-the-art networks for image classification. However, how this technique can be applied to and what is its effectiveness on natural language processing (NLP) tasks have not been investigated. In this paper, we propose two strategies for the adaption of Mixup on sentence classification: one performs interpolation on word embeddings and another on sentence embeddings. We conduct experiments to evaluate our methods using several benchmark datasets. Our studies show that such interpolation strategies serve as an effective, domain independent data augmentation approach for sentence classification, and can result in significant accuracy improvement for both CNN and LSTM models.},
  urldate    = {2023-05-31},
  publisher  = {arXiv},
  author     = {Guo, Hongyu and Mao, Yongyi and Zhang, Richong},
  month      = may,
  year       = {2019},
  note       = {arXiv:1905.08941 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  annote     = {Comment: 7 pages},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/4ENYL9NM/Guo et al. - 2019 - Augmenting Data with Mixup for Sentence Classifica.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/9L67BV87/1905.html:text/html}
}

@inproceedings{cheung_modals_2021,
  title     = {\{{MODALS}\}: {Modality}-agnostic {Automated} {Data} {Augmentation} in the {Latent} {Space}},
  url       = {https://openreview.net/forum?id=XjYgR6gbCEc},
  booktitle = {International {Conference} on {Learning} {Representations}},
  author    = {Cheung, Tsz-Him and Yeung, Dit-Yan},
  year      = {2021}
}

@inproceedings{pham_meta_2021,
  title     = {Meta {Back}-{Translation}},
  url       = {https://openreview.net/forum?id=3jjmdp7Hha},
  booktitle = {International {Conference} on {Learning} {Representations}},
  author    = {Pham, Hieu and Wang, Xinyi and Yang, Yiming and Neubig, Graham},
  year      = {2021}
}

@book{smith_mathematics_2007,
  address   = {{\textbackslash}htmladdnormallinkhttp://www.w3k.org/books/http://www.w3k.org/books/},
  title     = {Mathematics of the {Discrete} {Fourier} {Transform} ({DFT})},
  isbn      = {978-0-9745607-4-8},
  publisher = {W3K Publishing},
  author    = {Smith, Julius O.},
  year      = {2007}
}

@misc{pluscec_data_2023,
  title     = {Data {Augmentation} for {Neural} {NLP}},
  url       = {http://arxiv.org/abs/2302.11412},
  doi       = {10.48550/arXiv.2302.11412},
  abstract  = {Data scarcity is a problem that occurs in languages and tasks where we do not have large amounts of labeled data but want to use state-of-the-art models. Such models are often deep learning models that require a significant amount of data to train. Acquiring data for various machine learning problems is accompanied by high labeling costs. Data augmentation is a low-cost approach for tackling data scarcity. This paper gives an overview of current state-of-the-art data augmentation methods used for natural language processing, with an emphasis on methods for neural and transformer-based models. Furthermore, it discusses the practical challenges of data augmentation, possible mitigations, and directions for future research.},
  urldate   = {2023-06-01},
  publisher = {arXiv},
  author    = {Pluščec, Domagoj and Šnajder, Jan},
  month     = feb,
  year      = {2023},
  note      = {arXiv:2302.11412 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/5TIEFUNZ/Pluščec and Šnajder - 2023 - Data Augmentation for Neural NLP.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/83UFQTCJ/2302.html:text/html}
}

@book{goodfellow_deep_2016,
  address   = {Cambridge, MA, USA},
  title     = {Deep {Learning}},
  publisher = {MIT Press},
  author    = {Goodfellow, Ian J. and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  annote    = {http://www.deeplearningbook.org}
}

@book{kemeny_finite_1960,
  title     = {Finite markov chains},
  volume    = {356},
  publisher = {van Nostrand Princeton, NJ},
  author    = {Kemeny, John G and Snell, James Laurie and {others}},
  year      = {1960},
  keywords  = {diss imported inthesis mixedtrails}
}

@article{peeters_timbre_2011,
  title      = {The {Timbre} {Toolbox}: {Extracting} audio descriptors from musical signals},
  volume     = {130},
  issn       = {0001-4966},
  shorttitle = {The {Timbre} {Toolbox}},
  url        = {https://pubs.aip.org/asa/jasa/article/130/5/2902-2916/842365},
  doi        = {10.1121/1.3642604},
  language   = {en},
  number     = {5},
  urldate    = {2023-06-05},
  journal    = {The Journal of the Acoustical Society of America},
  author     = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
  month      = nov,
  year       = {2011},
  pages      = {2902--2916},
  file       = {Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf:/Users/marcio/Zotero/storage/SKMXLCA5/Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf:application/pdf}
}

@article{pijanowski_soundscape_2011,
  title    = {Soundscape {Ecology}: {The} {Science} of {Sound} in the {Landscape}},
  volume   = {61},
  issn     = {0006-3568},
  url      = {https://doi.org/10.1525/bio.2011.61.3.6},
  doi      = {10.1525/bio.2011.61.3.6},
  abstract = {This article presents a unifying theory of soundscape ecology, which brings the idea of the soundscape—the collection of sounds that emanate from landscapes—into a research and application focus. Our conceptual framework of soundscape ecology is based on the causes and consequences of biological (biophony), geophysical (geophony), and human-produced (anthrophony) sounds. We argue that soundscape ecology shares many parallels with landscape ecology, and it should therefore be considered a branch of this maturing field. We propose a research agenda for soundscape ecology that includes six areas: (1) measurement and analytical challenges, (2) spatial-temporal dynamics, (3) soundscape linkage to environmental covariates, (4) human impacts on the soundscape, (5) soundscape impacts on humans, and (6) soundscape impacts on ecosystems. We present case studies that illustrate different approaches to understanding soundscape dynamics. Because soundscapes are our auditory link to nature, we also argue for their protection, using the knowledge of how sounds are produced by the environment and humans.},
  number   = {3},
  journal  = {BioScience},
  author   = {Pijanowski, Bryan C. and Villanueva-Rivera, Luis J. and Dumyahn, Sarah L. and Farina, Almo and Krause, Bernie L. and Napoletano, Brian M. and Gage, Stuart H. and Pieretti, Nadia},
  month    = mar,
  year     = {2011},
  note     = {\_eprint: https://academic.oup.com/bioscience/article-pdf/61/3/203/19404645/61-3-203.pdf},
  pages    = {203--216}
}

@misc{ho_classifier-free_2022,
  title     = {Classifier-{Free} {Diffusion} {Guidance}},
  url       = {http://arxiv.org/abs/2207.12598},
  doi       = {10.48550/arXiv.2207.12598},
  abstract  = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  urldate   = {2023-06-10},
  publisher = {arXiv},
  author    = {Ho, Jonathan and Salimans, Tim},
  month     = jul,
  year      = {2022},
  note      = {arXiv:2207.12598 [cs]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  annote    = {Comment: A short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications: https://openreview.net/pdf?id=qw8AKxfYbI},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/QPKFNUKP/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/3CXUCUDY/2207.html:text/html}
}

@inproceedings{kahn_libri-light_2020,
  title     = {Libri-light: {A} benchmark for asr with limited or no supervision},
  booktitle = {{ICASSP} 2020-2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  publisher = {IEEE},
  author    = {Kahn, Jacob and Riviere, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazaré, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and {others}},
  year      = {2020},
  pages     = {7669--7673}
}

@inproceedings{panayotov_librispeech_2015,
  title     = {Librispeech: {An} {ASR} corpus based on public domain audio books},
  doi       = {10.1109/ICASSP.2015.7178964},
  abstract  = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  month     = apr,
  year      = {2015},
  note      = {ISSN: 2379-190X},
  pages     = {5206--5210}
}

@misc{adigwe_emotional_2018,
  title      = {The {Emotional} {Voices} {Database}: {Towards} {Controlling} the {Emotion} {Dimension} in {Voice} {Generation} {Systems}},
  shorttitle = {The {Emotional} {Voices} {Database}},
  url        = {http://arxiv.org/abs/1806.09514},
  doi        = {10.48550/arXiv.1806.09514},
  abstract   = {In this paper, we present a database of emotional speech intended to be open-sourced and used for synthesis and generation purpose. It contains data for male and female actors in English and a male actor in French. The database covers 5 emotion classes so it could be suitable to build synthesis and voice transformation systems with the potential to control the emotional dimension in a continuous way. We show the data's efficiency by building a simple MLP system converting neutral to angry speech style and evaluate it via a CMOS perception test. Even though the system is a very simple one, the test show the efficiency of the data which is promising for future work.},
  urldate    = {2023-06-10},
  publisher  = {arXiv},
  author     = {Adigwe, Adaeze and Tits, Noé and Haddad, Kevin El and Ostadabbas, Sarah and Dutoit, Thierry},
  month      = jun,
  year       = {2018},
  note       = {arXiv:1806.09514 [cs, eess]},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
  annote     = {Comment: Submitted to SLSP 2018},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/MZLU3WT9/Adigwe et al. - 2018 - The Emotional Voices Database Towards Controlling.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/T62ZBG2M/1806.html:text/html}
}

@misc{verma_generative_2021,
  title     = {A {Generative} {Model} for {Raw} {Audio} {Using} {Transformer} {Architectures}},
  url       = {http://arxiv.org/abs/2106.16036},
  doi       = {10.48550/arXiv.2106.16036},
  abstract  = {This paper proposes a novel way of doing audio synthesis at the waveform level using Transformer architectures. We propose a deep neural network for generating waveforms, similar to wavenet. This is fully probabilistic, auto-regressive, and causal, i.e. each sample generated depends only on the previously observed samples. Our approach outperforms a widely used wavenet architecture by up to 9\% on a similar dataset for predicting the next step. Using the attention mechanism, we enable the architecture to learn which audio samples are important for the prediction of the future sample. We show how causal transformer generative models can be used for raw waveform synthesis. We also show that this performance can be improved by another 2\% by conditioning samples over a wider context. The flexibility of the current model to synthesize audio from latent representations suggests a large number of potential applications. The novel approach of using generative transformer architectures for raw audio synthesis is, however, still far away from generating any meaningful music, without using latent codes/meta-data to aid the generation process.},
  urldate   = {2023-08-02},
  publisher = {arXiv},
  author    = {Verma, Prateek and Chafe, Chris},
  month     = jul,
  year      = {2021},
  note      = {arXiv:2106.16036 [cs, eess]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Multimedia},
  annote    = {Comment: DAFX 2021},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/7WWEDSQ5/Verma and Chafe - 2021 - A Generative Model for Raw Audio Using Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/UWU6GFV3/2106.html:text/html}
}

@misc{baevski_wav2vec_2020,
  title  = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  year   = {2020},
  note   = {\_eprint: 2006.11477}
}

@book{alpaydin_introduction_2020,
  address   = {Cambridge, Massachusetts},
  edition   = {Fourth edition},
  series    = {Adaptive computation and machine learning series},
  title     = {Introduction to machine learning},
  isbn      = {978-0-262-04379-3},
  abstract  = {"Since the third edition of this text appeared in 2014, most recent advances in machine learning, both in theory and application, are related to neural networks and deep learning. In this new edition, the author has extended the discussion of multilayer perceptrons. He has also added a new chapter on deep learning including training deep neural networks, regularizing them so they learn better, structuring them to improve learning, e.g., through convolutional layers, and their recurrent extensions with short-term memory necessary for learning sequences. There is a new section on generative adversarial networks that have found an impressive array of applications in recent years. Alpaydin has also extended the chapter on reinforcement learning to discuss the use of deep networks in reinforcement learning. There is a new section on the policy gradient method that has been used frequently in recent years with neural networks, and two additional sections on two examples of deep reinforcement learning, which both made headlines when they were announced in 2015 and 2016 respectively. One is a network that learns to play arcade video games, and the other one learns to play Go. There are also revisions in other chapters reflecting new approaches, such as embedding methods for dimensionality reduction, and multi-label classification. In response to requests from instructors, this new edition contains two new appendices on linear algebra and optimization, to remind the reader of the basics of those topics that find use in machine learning"--},
  publisher = {The MIT Press},
  author    = {Alpaydin, Ethem},
  year      = {2020},
  keywords  = {Machine learning}
}

@misc{martinez_artificial_2019,
  title     = {Artificial intelligence: {Short} history, present developments, and future outlook},
  publisher = {MIT Lincoln Laboratory},
  author    = {Martinez, Dave and Malyska, Nick and Streilein, Bill and Caceres, Rajmonda and Campbell, William and Dagli, Charlie and Gadepally, Vijay and Greenfield, Kara and Hall, Robert and King, Andre and {others}},
  year      = {2019}
}

@article{dean_golden_2022,
  title      = {A {Golden} {Decade} of {Deep} {Learning}: {Computing} {Systems} \& {Applications}},
  volume     = {151},
  issn       = {0011-5266, 1548-6192},
  shorttitle = {A {Golden} {Decade} of {Deep} {Learning}},
  url        = {https://direct.mit.edu/daed/article/151/2/58/110623/A-Golden-Decade-of-Deep-Learning-Computing-Systems},
  doi        = {10.1162/daed_a_01900},
  abstract   = {Abstract
                The past decade has seen tremendous progress in the field of artificial intelligence thanks to the resurgence of neural networks through deep learning. This has helped improve the ability for computers to see, hear, and understand the world around them, leading to dramatic advances in the application of AI to many fields of science and other areas of human endeavor. In this essay, I examine the reasons for this progress, including the confluence of progress in computing hardware designed to accelerate machine learning and the emergence of open-source software frameworks to dramatically expand the set of people who can use machine learning effectively. I also present a broad overview of some of the areas in which machine learning has been applied over the past decade. Finally, I sketch out some likely directions from which further progress in artificial intelligence will come.},
  language   = {en},
  number     = {2},
  urldate    = {2023-08-10},
  journal    = {Daedalus},
  author     = {Dean, Jeffrey},
  month      = may,
  year       = {2022},
  pages      = {58--74},
  file       = {Dean - 2022 - A Golden Decade of Deep Learning Computing System.pdf:/Users/marcio/Zotero/storage/DP9P2J5T/Dean - 2022 - A Golden Decade of Deep Learning Computing System.pdf:application/pdf}
}

@article{holden_origin_2004,
  title   = {The {Origin} of {Speech}},
  volume  = {303},
  url     = {https://www.science.org/doi/abs/10.1126/science.303.5662.1316},
  doi     = {10.1126/science.303.5662.1316},
  number  = {5662},
  journal = {Science},
  author  = {Holden, Constance},
  year    = {2004},
  note    = {\_eprint: https://www.science.org/doi/pdf/10.1126/science.303.5662.1316},
  pages   = {1316--1319}
}

@article{strong_chatbot_2023,
  title    = {Chatbot vs {Medical} {Student} {Performance} on {Free}-{Response} {Clinical} {Reasoning} {Examinations}},
  issn     = {2168-6106},
  url      = {https://doi.org/10.1001/jamainternmed.2023.2909},
  doi      = {10.1001/jamainternmed.2023.2909},
  abstract = {A popular chatbot is an interface for the generative pretrained transformer (GPT) large language model artificial intelligence (AI) system that generates humanlike text in response to user input. An upgrade from the initial chatbot to a newer model was evaluated, and it exceeded the passing threshold for multiple-choice questions that simulate the US Medical License Examinations. This cross-sectional study examined how well the chatbot responded to free-response, multiphase, case-based questions that better reflect real-life synthesis and application of data with broader implications for the instruction and assessment of clinical reasoning. Using clinical reasoning final examinations given to first- and second-year students at Stanford School of Medicine, we compared performance of students vs 2 chatbot models.},
  urldate  = {2023-08-10},
  journal  = {JAMA Internal Medicine},
  author   = {Strong, Eric and DiGiammarino, Alicia and Weng, Yingjie and Kumar, Andre and Hosamani, Poonam and Hom, Jason and Chen, Jonathan H.},
  month    = jul,
  year     = {2023}
}

@misc{cho_learning_2014,
  title     = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
  url       = {http://arxiv.org/abs/1406.1078},
  doi       = {10.48550/arXiv.1406.1078},
  abstract  = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  urldate   = {2023-08-10},
  publisher = {arXiv},
  author    = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  month     = sep,
  year      = {2014},
  note      = {arXiv:1406.1078 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
  annote    = {Comment: EMNLP 2014},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/QW5372MC/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/GEWCPLHK/1406.html:text/html}
}

@article{schuster_bidirectional_1997,
  title   = {Bidirectional recurrent neural networks},
  volume  = {45},
  number  = {11},
  journal = {IEEE transactions on Signal Processing},
  author  = {Schuster, Mike and Paliwal, Kuldip K},
  year    = {1997},
  note    = {Publisher: Ieee},
  pages   = {2673--2681}
}

@misc{mehrish_review_2023,
  title     = {A {Review} of {Deep} {Learning} {Techniques} for {Speech} {Processing}},
  url       = {http://arxiv.org/abs/2305.00359},
  abstract  = {The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field's evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field.},
  urldate   = {2023-08-11},
  publisher = {arXiv},
  author    = {Mehrish, Ambuj and Majumder, Navonil and Bhardwaj, Rishabh and Mihalcea, Rada and Poria, Soujanya},
  month     = may,
  year      = {2023},
  note      = {arXiv:2305.00359 [eess]},
  keywords  = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  file      = {arXiv.org Snapshot:/Users/marcio/Zotero/storage/ZSBA2XY2/2305.html:text/html;Full Text PDF:/Users/marcio/Zotero/storage/XSFQ2P82/Mehrish et al. - 2023 - A Review of Deep Learning Techniques for Speech Pr.pdf:application/pdf}
}

@article{griffin_signal_1984,
  title    = {Signal estimation from modified short-time {Fourier} transform},
  volume   = {32},
  issn     = {0096-3518},
  doi      = {10.1109/TASSP.1984.1164317},
  abstract = {In this paper, we present an algorithm to estimate a signal from its modified short-time Fourier transform (STFT). This algorithm is computationally simple and is obtained by minimizing the mean squared error between the STFT of the estimated signal and the modified STFT. Using this algorithm, we also develop an iterative algorithm to estimate a signal from its modified STFT magnitude. The iterative algorithm is shown to decrease, in each iteration, the mean squared error between the STFT magnitude of the estimated signal and the modified STFT magnitude. The major computation involved in the iterative algorithm is the discrete Fourier transform (DFT) computation, and the algorithm appears to be real-time implementable with current hardware technology. The algorithm developed in this paper has been applied to the time-scale modification of speech. The resulting system generates very high-quality speech, and appears to be better in performance than any existing method.},
  number   = {2},
  journal  = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  author   = {Griffin, D. and Lim, Jae},
  month    = apr,
  year     = {1984},
  note     = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
  keywords = {Fourier transforms, Monitoring, Degradation, Discrete Fourier transforms, Estimation theory, Hardware, Iterative algorithms, Sampling methods, Signal processing, Speech enhancement},
  pages    = {236--243},
  file     = {IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/6D9ZJE8P/1164317.html:text/html}
}

@misc{he_deep_2015,
  title     = {Deep {Residual} {Learning} for {Image} {Recognition}},
  url       = {http://arxiv.org/abs/1512.03385},
  doi       = {10.48550/arXiv.1512.03385},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate   = {2023-08-23},
  publisher = {arXiv},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month     = dec,
  year      = {2015},
  note      = {arXiv:1512.03385 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  annote    = {Comment: Tech report},
  file      = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/KYWCEQTQ/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/C7A5QEBX/1512.html:text/html}
}

@incollection{trautmann_classical_2003,
  address   = {Boston, MA},
  title     = {Classical {Synthesis} {Methods} {Based} on {Physical} {Models}},
  isbn      = {978-1-4615-0049-0},
  url       = {https://doi.org/10.1007/978-1-4615-0049-0_4},
  abstract  = {In contrast to the sound-based synthesis methods presented in chapter 2, this chapter reviews classical synthesis methods based on the simulation of the sound production mechanisms. These methods are also well known as physical modeling. From the signal processing point of view physical modeling can be interpreted as an extension of the time dependent sound-based methods with the inclusion of the spatial dimensions. They are explained in more detail than the sound-based synthesis methods since they compete with the functional transformation method (FTM), which is introduced as a new physical modeling method in chapter 5.},
  language  = {en},
  urldate   = {2023-08-24},
  booktitle = {Digital {Sound} {Synthesis} by {Physical} {Modeling} {Using} the {Functional} {Transformation} {Method}},
  publisher = {Springer US},
  author    = {Trautmann, Lutz and Rabenstein, Rudolf},
  editor    = {Trautmann, Lutz and Rabenstein, Rudolf},
  year      = {2003},
  doi       = {10.1007/978-1-4615-0049-0_4},
  keywords  = {Dispersion Error, Excitation Function, Finite Difference Method, Modal Synthesis, String Vibration},
  pages     = {63--93}
}

@article{riesenhuber_hierarchical_1999,
  title     = {Hierarchical models of object recognition in cortex},
  volume    = {2},
  copyright = {1999 Nature America Inc.},
  issn      = {1546-1726},
  url       = {https://www.nature.com/articles/nn1199_1019},
  doi       = {10.1038/14819},
  abstract  = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
  language  = {en},
  number    = {11},
  urldate   = {2023-08-24},
  journal   = {Nature Neuroscience},
  author    = {Riesenhuber, Maximilian and Poggio, Tomaso},
  month     = nov,
  year      = {1999},
  note      = {Number: 11
               Publisher: Nature Publishing Group},
  keywords  = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
  pages     = {1019--1025}
}

@inproceedings{akaishi_improving_2023,
  title     = {Improving {Phase}-{Vocoder}-{Based} {Time} {Stretching} by {Time}-{Directional} {Spectrogram} {Squeezing}},
  doi       = {10.1109/ICASSP49357.2023.10095348},
  abstract  = {Time stretching of music signals has a crucial problem, i.e., smearing of percussive sounds. Some time stretching algorithms have addressed this problem by detecting percussive components and manipulating them differently from the other components. However, conventional methods cause artifacts. In this paper, to prevent percussion smearing, we propose a preprocessing for time stretching. The proposed algorithm aims to preserve time scale of percussive components while stretching the rest of components in the ordinary way. To do so, time-frequency bins dominated by percussive components are squeezed in time direction so as to preserve the shape of spectrogram of percussive components. Our experiment showed that our method could improve sound quality for long stretching.},
  booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  author    = {Akaishi, Natsuki and Yatabe, Kohei and Oikawa, Yasuhiro},
  month     = jun,
  year      = {2023},
  keywords  = {Spectrogram, Speech processing, Time-frequency analysis, Acoustics, Multiple signal classification, percussion smearing, phase derivative, phase vocoder, positive quartic C2-spline function, Shape, Signal processing algorithms, Time-scale modification},
  pages     = {1--5},
  file      = {IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/XPFMRUUW/10095348.html:text/html}
}

@article{gruetzemacher_deep_2022,
  title      = {Deep {Transfer} {Learning} \& {Beyond}: {Transformer} {Language} {Models} in {Information} {Systems} {Research}},
  volume     = {54},
  issn       = {0360-0300},
  shorttitle = {Deep {Transfer} {Learning} \& {Beyond}},
  url        = {https://dl.acm.org/doi/10.1145/3505245},
  doi        = {10.1145/3505245},
  abstract   = {AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.},
  number     = {10s},
  urldate    = {2023-08-24},
  journal    = {ACM Computing Surveys},
  author     = {Gruetzemacher, Ross and Paradice, David},
  month      = sep,
  year       = {2022},
  keywords   = {deep learning, artificial intelligence, language models, Natural language processing, text mining, transfer learning},
  pages      = {204:1--204:35},
  file       = {Full Text PDF:/Users/marcio/Zotero/storage/GMDE78BZ/Gruetzemacher and Paradice - 2022 - Deep Transfer Learning & Beyond Transformer Langu.pdf:application/pdf}
}

@misc{liu_audioldm_2023,
  title      = {{AudioLDM}: {Text}-to-{Audio} {Generation} with {Latent} {Diffusion} {Models}},
  shorttitle = {{AudioLDM}},
  url        = {http://arxiv.org/abs/2301.12503},
  doi        = {10.48550/arXiv.2301.12503},
  abstract   = {Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
  urldate    = {2023-08-25},
  publisher  = {arXiv},
  author     = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
  month      = feb,
  year       = {2023},
  note       = {arXiv:2301.12503 [cs, eess]},
  keywords   = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Multimedia, Electrical Engineering and Systems Science - Signal Processing},
  annote     = {Comment: Demo and implementation at https://audioldm.github.io. Evaluation toolbox at https://github.com/haoheliu/audioldm\_eval},
  file       = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/MKSBC7N2/Liu et al. - 2023 - AudioLDM Text-to-Audio Generation with Latent Dif.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/H7ELJFK5/2301.html:text/html}
}

@misc{amazon_web_services_inc_amazon_nodate,
  title    = {Amazon {EC2} {P3} – {Ideal} for {Machine} {Learning} and {HPC} - {AWS}},
  url      = {https://aws.amazon.com/ec2/instance-types/p3/},
  abstract = {Amazon EC2 P3 instances are the next generation of Amazon EC2 GPU compute instances that are powerful and scalable to provide GPU-based parallel compute capabilities. P3 instances are ideal for computationally challenging applications, including machine learning, high-performance computing, computational fluid dynamics, computational finance, seismic analysis, molecular modeling, genomics, and development of autonomous vehicle systems.},
  language = {en-US},
  urldate  = {2023-08-26},
  journal  = {Amazon Web Services, Inc.},
  author   = {{Amazon Web Services, Inc.}},
  file     = {Snapshot:/Users/marcio/Zotero/storage/7TPG54H6/p3.html:text/html}
}

@article{dingsoyr_decade_2012,
  series     = {Special {Issue}: {Agile} {Development}},
  title      = {A decade of agile methodologies: {Towards} explaining agile software development},
  volume     = {85},
  issn       = {0164-1212},
  shorttitle = {A decade of agile methodologies},
  url        = {https://www.sciencedirect.com/science/article/pii/S0164121212000532},
  doi        = {10.1016/j.jss.2012.02.033},
  abstract   = {Ever since the agile manifesto was created in 2001, the research community has devoted a great deal of attention to agile software development. This article examines publications and citations to illustrate how the research on agile has progressed in the 10 years following the articulation of the manifesto. Specifically, we delineate the conceptual structure underlying agile scholarship by performing an analysis of authors who have made notable contributions to the field. Further, we summarize prior research and introduce contributions in this special issue on agile software development. We conclude by discussing directions for future research and urging agile researchers to embrace a theory-based approach in their scholarship.},
  number     = {6},
  urldate    = {2023-08-26},
  journal    = {Journal of Systems and Software},
  author     = {Dingsøyr, Torgeir and Nerur, Sridhar and Balijepally, VenuGopal and Moe, Nils Brede},
  month      = jun,
  year       = {2012},
  keywords   = {Agile software development, Crystal method, eXtreme programming, XP, Feature-driven development, Information systems, Lean software development, Scrum, Software engineering, Theory},
  pages      = {1213--1221},
  file       = {ScienceDirect Snapshot:/Users/marcio/Zotero/storage/R6YCA3X3/S0164121212000532.html:text/html}
}

@article{forero_j_are_2023,
  title   = {Are words enough?},
  journal = {AIMC 2023},
  author  = {{Forero, J.} and {Bernardes, G.} and {Mendes, M.}},
  month   = aug,
  year    = {2023},
  annote  = {https://aimc2023.pubpub.org/pub/9z68g7d2},
  file    = {Forero, J. et al. - 2023 - Are words enough.pdf:/Users/marcio/Zotero/storage/K34NQ7CH/Forero, J. et al. - 2023 - Are words enough.pdf:application/pdf}
}

@article{zou_regularization_2005,
  title    = {Regularization and {Variable} {Selection} via the {Elastic} {Net}},
  volume   = {67},
  issn     = {1369-7412},
  url      = {https://www.jstor.org/stable/3647580},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  number   = {2},
  urldate  = {2023-08-31},
  journal  = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  author   = {Zou, Hui and Hastie, Trevor},
  year     = {2005},
  note     = {Publisher: [Royal Statistical Society, Wiley]},
  pages    = {301--320}
}

@inproceedings{magalhaes_physics-based_2020,
  title     = {Physics-based {Concatenative} {Sound} {Synthesis} of {Photogrammetric} models for {Aural} and {Haptic} {Feedback} in {Virtual} {Environments}},
  doi       = {10.1109/VRW50115.2020.00081},
  abstract  = {We present a novel physics-based concatenative sound synthesis (CSS) methodology for congruent interactions across physical, graphical, aural and haptic modalities in Virtual Environments. Navigation in aural and haptic corpora of annotated audio units is driven by user interactions with highly realistic photogrammetric based models in a game engine, where automated and interactive positional, physics and graphics data are supported. From a technical perspective, the current contribution expands existing CSS frameworks in avoiding mapping or mining the annotation data to real-time performance attributes, while guaranteeing degrees of novelty and variation for the same gesture.},
  booktitle = {2020 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
  author    = {Magalhäes, E and Jacob, J and Nilsson, N and Nordahl, R and Bernardes, G},
  month     = mar,
  year      = {2020},
  pages     = {376--379},
  file      = {Full Text:/Users/marcio/Zotero/storage/TLQU4AV8/Magalhäes et al. - 2020 - Physics-based Concatenative Sound Synthesis of Pho.pdf:application/pdf}
}

@misc{oxford_english_dictionary_music_2023,
  title     = {music, n. \& adj.},
  url       = {https://doi.org/10.1093/OED/7569208909},
  publisher = {Oxford University Press},
  author    = {{Oxford English Dictionary}},
  month     = jul,
  year      = {2023}
}

@inproceedings{rao_grapheme--phoneme_2015-1,
  address   = {South Brisbane, Queensland, Australia},
  title     = {Grapheme-to-phoneme conversion using {Long} {Short}-{Term} {Memory} recurrent neural networks},
  isbn      = {978-1-4673-6997-8},
  url       = {http://ieeexplore.ieee.org/document/7178767/},
  doi       = {10.1109/ICASSP.2015.7178767},
  abstract  = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoderdecoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
  language  = {en},
  urldate   = {2023-09-01},
  booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
  publisher = {IEEE},
  author    = {Rao, Kanishka and Peng, Fuchun and Sak, Hasim and Beaufays, Francoise},
  month     = apr,
  year      = {2015},
  pages     = {4225--4229},
  file      = {Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:/Users/marcio/Zotero/storage/BEW6M5UF/Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:application/pdf}
}

@article{sotelo_char2wav_2017,
  title  = {Char2wav: {End}-to-end speech synthesis},
  author = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao Felipe and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
  year   = {2017}
}

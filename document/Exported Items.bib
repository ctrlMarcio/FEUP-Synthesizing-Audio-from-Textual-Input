
@incollection{huzaifah_deep_2021,
	address = {Cham},
	title = {Deep {Generative} {Models} for {Musical} {Audio} {Synthesis}},
	isbn = {978-3-030-72116-9},
	url = {https://doi.org/10.1007/978-3-030-72116-9_22},
	abstract = {Sound modelling is the process of developing algorithms that generate sound under parametric control.},
	booktitle = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	publisher = {Springer International Publishing},
	author = {Huzaifah, Muhammad and Wyse, Lonce},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9_22},
	pages = {639--678},
	file = {Full Text:/Users/marcio/Zotero/storage/I5J5VUEX/Huzaifah and Wyse - 2021 - Deep Generative Models for Musical Audio Synthesis.pdf:application/pdf},
}

@inproceedings{kumar_melgan_2019,
	title = {{MelGAN}: {Generative} {Adversarial} {Networks} for {Conditional} {Waveform} {Synthesis}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/file/6804c9bca0a615bdb9374d00a9fcba59-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kumar, Kundan and Kumar, Rithesh and de Boissiere, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and de Brébisson, Alexandre and Bengio, Yoshua and Courville, Aaron C},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	file = {Full Text:/Users/marcio/Zotero/storage/CTF59XSN/Kumar et al. - 2019 - MelGAN Generative Adversarial Networks for Condit.pdf:application/pdf},
}

@inproceedings{tahiroglu_-terity_2020,
	series = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	title = {Al-terity: {Non}-{Rigid} {Musical} {Instrument} with {Artificial} {Intelligence} {Applied} to {Real}-{Time} {Audio} {Synthesis}},
	abstract = {A deformable musical instrument can take numerous dis- tinct shapes with its non-rigid features. Building audio syn- thesis module for such an interface behaviour can be chal- lenging. In this paper, we present the Al-terity, a non-rigid musical instrument that comprises a deep learning model with generative adversarial network architecture and use it for generating audio samples for real-time audio synthesis. The particular deep learning model we use for this instru- ment was trained with existing data set as input for pur- poses of further experimentation. The main benefits of the model used are the ability to produce the realistic range of timbre of the trained data set and the ability to generate new audio samples in real-time, in the moment of playing, with the characteristics of sounds that the performer ever heard before. We argue that these advanced intelligence features on the audio synthesis level could allow us to ex- plore performing music with particular response features that define the instrument’s digital idiomaticity and allow us reinvent the instrument in the act of music performance.},
	language = {English},
	booktitle = {Proceedings of the {International} {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	publisher = {International Conference on New Interfaces for Musical Expression},
	author = {Tahiroğlu, Koray and Kastemaa, Miranda and Koli, Oskar},
	month = jul,
	year = {2020},
	keywords = {Artificial Intelligence (AI), deep learning, GAN, NIME, SOPI},
	pages = {337--342},
	file = {Proceedings of the International Conference on New.pdf:/Users/marcio/Zotero/storage/H3LYBRT6/Proceedings of the International Conference on New.pdf:application/pdf},
}

@inproceedings{kong_hifi-gan_2020,
	title = {{HiFi}-{GAN}: {Generative} {Adversarial} {Networks} for {Efficient} and {High} {Fidelity} {Speech} {Synthesis}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {17022--17033},
	file = {Full Text:/Users/marcio/Zotero/storage/HT7ZGKEE/Kong et al. - 2020 - HiFi-GAN Generative Adversarial Networks for Effi.pdf:application/pdf},
}

@article{kim_flowavenet_2018,
	title = {{FloWaveNet} : {A} {Generative} {Flow} for {Raw} {Audio}},
	volume = {abs/1811.02155},
	url = {http://arxiv.org/abs/1811.02155},
	journal = {CoRR},
	author = {Kim, Sungwon and Lee, Sang-gil and Song, Jongyoon and Yoon, Sungroh},
	year = {2018},
	note = {arXiv: 1811.02155},
	file = {Full Text:/Users/marcio/Zotero/storage/R2N5I8BU/Kim et al. - 2018 - FloWaveNet  A Generative Flow for Raw Audio.pdf:application/pdf},
}

@misc{douwes_energy_2021,
	title = {Energy {Consumption} of {Deep} {Generative} {Audio} {Models}},
	url = {http://arxiv.org/abs/2107.02621},
	doi = {10.48550/arXiv.2107.02621},
	abstract = {In most scientific domains, the deep learning community has largely focused on the quality of deep generative models, resulting in highly accurate and successful solutions. However, this race for quality comes at a tremendous computational cost, which incurs vast energy consumption and greenhouse gas emissions. At the heart of this problem are the measures that we use as a scientific community to evaluate our work. In this paper, we suggest relying on a multi-objective measure based on Pareto optimality, which takes into account both the quality of the model and its energy consumption. By applying our measure on the current state-of-the-art in generative audio models, we show that it can drastically change the significance of the results. We believe that this type of metric can be widely used by the community to evaluate their work, while putting computational cost -- and in fine energy consumption -- in the spotlight of deep learning research.},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Douwes, Constance and Esling, Philippe and Briot, Jean-Pierre},
	month = oct,
	year = {2021},
	note = {arXiv:2107.02621 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 5 pages, 2 figures, ICASSP 2022},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/Q78FAEKM/Douwes et al. - 2021 - Energy Consumption of Deep Generative Audio Models.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/F6TZNRBA/2107.html:text/html},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	doi = {10.48550/arXiv.1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/P6KZR7GL/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/VLWC73AW/1609.html:text/html},
}

@misc{engel_gansynth_2019,
	title = {{GANSynth}: {Adversarial} {Neural} {Audio} {Synthesis}},
	shorttitle = {{GANSynth}},
	url = {http://arxiv.org/abs/1902.08710},
	doi = {10.48550/arXiv.1902.08710},
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	month = apr,
	year = {2019},
	note = {arXiv:1902.08710 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Colab Notebook: http://goo.gl/magenta/gansynth-demo},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/R9AC2KN4/Engel et al. - 2019 - GANSynth Adversarial Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/TJKV9U8R/1902.html:text/html},
}

@article{ghahramani_probabilistic_2015,
	title = {Probabilistic machine learning and artificial intelligence},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14541},
	doi = {10.1038/nature14541},
	abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
	language = {en},
	number = {7553},
	urldate = {2022-11-08},
	journal = {Nature},
	author = {Ghahramani, Zoubin},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing, Neuroscience},
	pages = {452--459},
	file = {Full Text PDF:/Users/marcio/Zotero/storage/5A3M7EVZ/Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/YEHX2HD9/nature14541.html:text/html},
}

@misc{elsea_basics_1996,
	title = {Basics of {Digital} {Recording}},
	shorttitle = {Digital {Recording}},
	url = {http://artsites.ucsc.edu/EMS/music/tech_background/TE-16/teces_16.html},
	urldate = {2022-11-09},
	journal = {Univeristy of California, Santa Cruz},
	author = {Elsea, Peter},
	year = {1996},
	file = {Digital Recording:/Users/marcio/Zotero/storage/7ACP8LT9/teces_16.html:text/html;Elsea - 1996 - Basics of Digital Recording.pdf:/Users/marcio/Zotero/storage/AF8HAT9B/Elsea - 1996 - Basics of Digital Recording.pdf:application/pdf},
}

@misc{university_of_york_what_nodate,
	title = {What is {Computer} {Science}?},
	url = {https://www.cs.york.ac.uk/undergraduate/what-is-cs/},
	urldate = {2023-01-01},
	journal = {Computer Science - Computer Science, University of York},
	author = {{University of York}},
	file = {What is Computer Science? - Computer Science, University of York:/Users/marcio/Zotero/storage/S2CMJYK7/what-is-cs.html:text/html},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
	file = {Mitchell - 1997 - Machine Learning.pdf:/Users/marcio/Zotero/storage/HAPYJGNN/Mitchell - 1997 - Machine Learning.pdf:application/pdf},
}

@article{schulz_deep_2012,
	title = {Deep {Learning}},
	volume = {26},
	issn = {1610-1987},
	url = {https://doi.org/10.1007/s13218-012-0198-z},
	doi = {10.1007/s13218-012-0198-z},
	abstract = {Hierarchical neural networks for object recognition have a long history. In recent years, novel methods for incrementally learning a hierarchy of features from unlabeled inputs were proposed as good starting point for supervised training. These deep learning methods—together with the advances of parallel computers—made it possible to successfully attack problems that were not practical before, in terms of depth and input size. In this article, we introduce the reader to the basic concepts of deep learning, discuss selected methods in detail, and present application examples from computer vision and speech recognition.},
	language = {en},
	number = {4},
	urldate = {2023-01-01},
	journal = {KI - Künstliche Intelligenz},
	author = {Schulz, Hannes and Behnke, Sven},
	month = nov,
	year = {2012},
	keywords = {Hierarchical feature learning, Object categorization, Unsupervised learning},
	pages = {357--363},
	file = {Schulz and Behnke - 2012 - Deep Learning.pdf:/Users/marcio/Zotero/storage/TX442LC2/Schulz and Behnke - 2012 - Deep Learning.pdf:application/pdf},
}

@misc{bishop_mixture_1994,
	type = {Monograph},
	title = {Mixture density networks},
	url = {https://publications.aston.ac.uk/id/eprint/373/},
	abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the effectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.},
	language = {en-GB},
	urldate = {2023-02-02},
	author = {Bishop, Christopher M.},
	year = {1994},
	note = {Num Pages: 26
Place: Birmingham
Publisher: Aston University},
	file = {Full Text PDF:/Users/marcio/Zotero/storage/7WPXXDGE/Bishop - 1994 - Mixture density networks.pdf:application/pdf;Snapshot:/Users/marcio/Zotero/storage/QC94TP64/373.html:text/html},
}

@article{fradkov_early_2020,
	series = {21st {IFAC} {World} {Congress}},
	title = {Early {History} of {Machine} {Learning}},
	volume = {53},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896320325027},
	doi = {10.1016/j.ifacol.2020.12.1888},
	abstract = {Machine learning belongs to the crossroad of cybernetics (control science) and computer science. It is attracting recently an overwhelming interest, both of professionals and of the general public. In the talk a brief overview of the historical development of the machine learning field with a focus on the development of mathematical apparatus in its first decades is provided. A number of little-known facts published in hard to reach sources are presented.},
	language = {en},
	number = {2},
	urldate = {2023-02-02},
	journal = {IFAC-PapersOnLine},
	author = {Fradkov, Alexander L.},
	month = jan,
	year = {2020},
	keywords = {Convex Optimization, Machine Learning, Neural Networks, Separation Theorems},
	pages = {1385--1390},
	file = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/MWAGJ6WW/Fradkov - 2020 - Early History of Machine Learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/8QVUQ6EY/S2405896320325027.html:text/html},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
	doi = {10.1037/h0042519},
	language = {en},
	number = {6},
	urldate = {2023-02-02},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {386--408},
	file = {Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:/Users/marcio/Zotero/storage/L4GHFAJT/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf},
}

@book{marvin_minsky_perceptrons_1969,
	title = {Perceptrons: an introduction to computational geometry},
	isbn = {0 262 13043 2},
	shorttitle = {Perceptrons},
	language = {English},
	author = {{Marvin Minsky} and {Seymour Papert}},
	year = {1969},
}

@inproceedings{dean_mapreduce_2004,
	address = {San Francisco, CA},
	title = {{MapReduce}: {Simplified} {Data} {Processing} on {Large} {Clusters}},
	booktitle = {{OSDI}'04: {Sixth} {Symposium} on {Operating} {System} {Design} and {Implementation}},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	year = {2004},
	pages = {137--150},
}

@article{rumelhart_learning_1986,
	title = {Learning {Representations} by {Back}-propagating {Errors}},
	volume = {323},
	url = {http://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	keywords = {imported},
	pages = {533--536},
	file = {Rumelhart.backprop.nature.pdf:/Users/marcio/Zotero/storage/6SDQ5MJK/Rumelhart.backprop.nature.pdf:application/pdf},
}

@misc{ramesh_zero-shot_2021,
	title = {Zero-{Shot} {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2102.12092},
	doi = {10.48550/arXiv.2102.12092},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2102.12092 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/49IMMPP5/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/7AA54J68/2102.html:text/html},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/QPKNXP69/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/UE5DRLFE/2204.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/U6VI47C8/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/HNK4MK4Y/2005.html:text/html},
}

@incollection{rumelhart_learning_1987,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-0-262-29140-8},
	url = {https://ieeexplore.ieee.org/document/6302929},
	abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
	urldate = {2023-02-04},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and McClelland, James L.},
	year = {1987},
	note = {Conference Name: Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	pages = {318--362},
	file = {Chap8_PDP86.pdf:/Users/marcio/Zotero/storage/S83BNYE8/Chap8_PDP86.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/marcio/Zotero/storage/KP5LD8CN/6302929.html:text/html},
}

@book{rumelhart_parallel_1986,
	title = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}, {Vol}. 1: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, D. E. and McClelland, J. L.},
	year = {1986},
	keywords = {imported},
}

@inproceedings{hyotyniemi_turing_1996,
	title = {Turing machines are recurrent neural networks},
	language = {English},
	booktitle = {{STeP} '96 - {Genes}, {Nets} and {Symbols}; {Finnish} {Artificial} {Intelligence} {Conference}, {Vaasa}, {Finland}, 20-23 {August} 1996},
	publisher = {University of Vaasa, Finnish Artificial Intelligence Society (FAIS)},
	author = {Hyötyniemi, H.},
	editor = {Alander, J. and Honkela, T.},
	year = {1996},
	keywords = {recurrent networks, Turing machines},
	pages = {13--24},
}

@book{hebb_organization_1949,
	address = {New York},
	title = {The organization of behavior: {A} neuropsychological theory},
	isbn = {0-8058-4300-0},
	abstract = {Donald Hebb pioneered many current themes in behavioural neuroscience. He saw psychology as a biological science, but one in which the organization of behaviour must remain the central concern. Through penetrating theoretical concepts, including the "cell assembly," "phase sequence," and "Hebb synapse," he offered a way to bridge the gap between cells, circuits and behaviour. He saw the brain as a dynamically organized system of multiple distributed parts, with roots that extend into foundations of development and evolutionary heritage. He understood that behaviour, as brain, can be sliced at various levels and that one of our challenges is to bring these levels into both conceptual and empirical register. He could move between theory and fact with an ease that continues to inspire both students and professional investigators. Although facts continue to accumulate at an accelerating rate in both psychology and neuroscience, and although these facts continue to force revision in the details of Hebb's earlier contributions, his overall insistence that we look at behaviour and brain together â within a dynamic, relational and multilayered framework â remains. His work touches upon current studies of population coding, contextual factors in brain representations, synaptic plasticity, developmental construction of brain/behaviour relations, clinical syndromes, deterioration of performance with age and disease, and the formal construction of connectionist models. The collection of papers in this volume represent these and related themes that Hebb inspired. We also acknowledge our appreciation for Don Hebb as teacher, colleague and friend.},
	publisher = {Wiley},
	author = {Hebb, Donald O.},
	month = jun,
	year = {1949},
	note = {Published: Hardcover},
	keywords = {MSc checked network neural seminal},
}

@article{rosenblatt_perceptron_1958-1,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {0033-295X},
	url = {http://dx.doi.org/10.1037/h0042519},
	doi = {10.1037/h0042519},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {imported},
	pages = {386--408},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194},
	url = {http://dx.doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals, and Systems (MCSS)},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	note = {Publisher: Springer London},
	keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
	pages = {303--314},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
	number = {3},
	urldate = {2023-02-05},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pmid = {14403679},
	pmcid = {PMC1363130},
	pages = {574--591},
	file = {PubMed Central Full Text PDF:/Users/marcio/Zotero/storage/2CECLBPR/Hubel and Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@misc{gregor_deep_2014,
	title = {Deep {AutoRegressive} {Networks}},
	url = {http://arxiv.org/abs/1310.8499},
	doi = {10.48550/arXiv.1310.8499},
	abstract = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
	month = may,
	year = {2014},
	note = {arXiv:1310.8499 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Appears in Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China, 2014},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/5XTVJB7V/Gregor et al. - 2014 - Deep AutoRegressive Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/5YFXYBFR/1310.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/6S7FJVH6/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/7IZVF8EH/1312.html:text/html},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/LR79KKU4/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/AP83RL62/1406.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/JVYFISN4/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/MYU4AUYU/2006.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/NXL8WFAS/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/2DW6BUXF/1503.html:text/html},
}

@article{weng_what_2021,
	title = {What are diffusion models?},
	url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
	journal = {lilianweng.github.io},
	author = {Weng, Lilian},
	month = jul,
	year = {2021},
}

@article{abayomi-alli_data_2022,
	title = {Data {Augmentation} and {Deep} {Learning} {Methods} in {Sound} {Classification}: {A} {Systematic} {Review}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Data {Augmentation} and {Deep} {Learning} {Methods} in {Sound} {Classification}},
	url = {https://www.mdpi.com/2079-9292/11/22/3795},
	doi = {10.3390/electronics11223795},
	abstract = {The aim of this systematic literature review (SLR) is to identify and critically evaluate current research advancements with respect to small data and the use of data augmentation methods to increase the amount of data available for deep learning classifiers for sound (including voice, speech, and related audio signals) classification. Methodology: This SLR was carried out based on the standard SLR guidelines based on PRISMA, and three bibliographic databases were examined, namely, Web of Science, SCOPUS, and IEEE Xplore. Findings. The initial search findings using the variety of keyword combinations in the last five years (2017–2021) resulted in a total of 131 papers. To select relevant articles that are within the scope of this study, we adopted some screening exclusion criteria and snowballing (forward and backward snowballing) which resulted in 56 selected articles. Originality: Shortcomings of previous research studies include the lack of sufficient data, weakly labelled data, unbalanced datasets, noisy datasets, poor representations of sound features, and the lack of effective augmentation approach affecting the overall performance of classifiers, which we discuss in this article. Following the analysis of identified articles, we overview the sound datasets, feature extraction methods, data augmentation techniques, and its applications in different areas in the sound classification research problem. Finally, we conclude with the summary of SLR, answers to research questions, and recommendations for the sound classification task.},
	language = {en},
	number = {22},
	urldate = {2023-02-05},
	journal = {Electronics},
	author = {Abayomi-Alli, Olusola O. and Damaševičius, Robertas and Qazi, Atika and Adedoyin-Olowe, Mariam and Misra, Sanjay},
	month = jan,
	year = {2022},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {audio data, data augmentation, deep learning, feature extraction, sound data},
	pages = {3795},
	file = {Full Text PDF:/Users/marcio/Zotero/storage/VMPKB9MI/Abayomi-Alli et al. - 2022 - Data Augmentation and Deep Learning Methods in Sou.pdf:application/pdf},
}

@article{mushtaq_environmental_2020,
	title = {Environmental sound classification using a regularized deep convolutional neural network with data augmentation},
	volume = {167},
	issn = {0003-682X},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X2030493X},
	doi = {10.1016/j.apacoust.2020.107389},
	abstract = {The adoption of the environmental sound classification (ESC) tasks increases very rapidly over recent years due to its broad range of applications in our daily routine life. ESC is also known as Sound Event Recognition (SER) which involves the context of recognizing the audio stream, related to various environmental sounds. Some frequent and common aspects like non-uniform distance between acoustic source and microphone, the difference in the framework, presence of numerous sounds sources in audio recordings and overlapping various sound events make this ESC problem much complex and complicated. This study is to employ deep convolutional neural networks (DCNN) with regularization and data enhancement with basic audio features that have verified to be efficient on ESC tasks. In this study, the performance of DCNN with max-pooling (Model-1) and without max-pooling (Model-2) function are examined. Three audio attribute extraction techniques, Mel spectrogram (Mel), Mel Frequency Cepstral Coefficient (MFCC) and Log-Mel, are considered for the ESC-10, ESC-50, and Urban sound (US8K) datasets. Furthermore, to avoid the risk of overfitting due to limited numbers of data, this study also introduces offline data augmentation techniques to enhance the used datasets with a combination of L2 regularization. The performance evaluation illustrates that the best accuracy attained by the proposed DCNN without max-pooling function (Model-2) and using Log-Mel audio feature extraction on those augmented datasets. For ESC-10, ESC-50 and US8K, the highest achieved accuracies are 94.94\%, 89.28\%, and 95.37\% respectively. The experimental results show that the proposed approach can accomplish the best performance on environment sound classification problems.},
	language = {en},
	urldate = {2023-02-05},
	journal = {Applied Acoustics},
	author = {Mushtaq, Zohaib and Su, Shun-Feng},
	month = oct,
	year = {2020},
	keywords = {Data augmentation, Deep convolutional neural network, Environmental sound classification, ESC-10, ESC-50, Regularization, Urbansound8k},
	pages = {107389},
	file = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/C9LFLW25/Mushtaq and Su - 2020 - Environmental sound classification using a regular.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/NMKGVSJD/S0003682X2030493X.html:text/html},
}

@article{qian_data_2019,
	title = {Data augmentation using generative adversarial networks for robust speech recognition},
	volume = {114},
	issn = {0167-6393},
	url = {https://www.sciencedirect.com/science/article/pii/S0167639319300044},
	doi = {10.1016/j.specom.2019.08.006},
	abstract = {For noise robust speech recognition, data mismatch between training and testing is a significant challenge. Data augmentation is an effective way to enlarge the size and diversity of training data and solve this problem. Different from the traditional approaches by directly adding noise to the original waveform, in this work we utilize generative adversarial networks (GAN) for data generation to improve speech recognition under noise conditions. In this paper we investigate different configurations of GANs. Firstly the basic GAN is applied: the generated speech samples are based on spectrum feature level and produced frame by frame without dependence among them, and there is no true labels. Thus, an unsupervised learning framework is proposed to utilize these untranscribed data for acoustic modeling. Then, in order to better guide the data generation, condition information is introduced into GAN structures, and the conditional GAN is utilized: two different conditions are explored, including the acoustic state of each speech frame and the original paired clean speech of each speech frame. With the incorporation of specific condition information into data generation, these conditional GANs can provide true labels directly, which can be used for later acoustic modeling. During the acoustic model training, these true labels are combined with the soft labels which make the model better. The proposed GAN-based data augmentation approaches are evaluated on two different noisy tasks: Aurora4 (simulated data with additive noise and channel distortion) and the AMI meeting transcription task (real data with significant reverberation). The experiments show that the new data augmentation approaches can obtain the performance improvement under all noisy conditions, which including additive noise, channel distortion and reverberation. With these augmented data by basic GAN / conditional GAN, a relative 6\% to 14\% WER reduction can be obtained upon an advanced acoustic model.},
	language = {en},
	urldate = {2023-02-05},
	journal = {Speech Communication},
	author = {Qian, Yanmin and Hu, Hu and Tan, Tian},
	month = nov,
	year = {2019},
	keywords = {Conditional generative adversarial networks, Data augmentation, Generative adversarial networks, Robust speech recognition, Very deep convolutional neural network},
	pages = {1--9},
	file = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/E2K2VVTS/Qian et al. - 2019 - Data augmentation using generative adversarial net.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/FN2CIXSC/S0167639319300044.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/6LKCT296/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/RREQWYV4/1706.html:text/html},
}

@article{deng_deep_2014,
	title = {Deep {Learning}: {Methods} and {Applications}},
	volume = {7},
	issn = {1932-8346, 1932-8354},
	shorttitle = {Deep {Learning}},
	url = {https://nowpublishers.com/article/Details/SIG-039},
	doi = {10.1561/2000000039},
	abstract = {Deep Learning: Methods and Applications},
	language = {English},
	number = {3–4},
	urldate = {2023-02-05},
	journal = {Foundations and Trends® in Signal Processing},
	author = {Deng, Li and Yu, Dong},
	month = jun,
	year = {2014},
	note = {Publisher: Now Publishers, Inc.},
	pages = {197--387},
	file = {Full Text PDF:/Users/marcio/Zotero/storage/CWCL9L88/Deng and Yu - 2014 - Deep Learning Methods and Applications.pdf:application/pdf},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
	language = {en},
	number = {4},
	urldate = {2023-02-05},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
	file = {Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:/Users/marcio/Zotero/storage/CULEJW9C/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:application/pdf},
}

@misc{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	doi = {10.48550/arXiv.1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv:1505.05770 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/SY5GTYHH/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/LSCE4NUP/1505.html:text/html},
}

@article{weng_flow-based_2018,
	title = {Flow-based {Deep} {Generative} {Models}},
	url = {https://lilianweng.github.io/posts/2018-10-13-flow-models/},
	journal = {lilianweng.github.io},
	author = {Weng, Lilian},
	year = {2018},
}

@article{novotny_analysis_2019,
	title = {Analysis of {DNN} {Speech} {Signal} {Enhancement} for {Robust} {Speaker} {Recognition}},
	volume = {58},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230818303607},
	doi = {10.1016/j.csl.2019.06.004},
	abstract = {In this work, we present an analysis of a DNN-based autoencoder for speech enhancement, dereverberation and denoising. The target application is a robust speaker verification (SV) system. We start our approach by carefully designing a data augmentation process to cover a wide range of acoustic conditions and to obtain rich training data for various components of our SV system. We augment several well-known databases used in SV with artificially noised and reverberated data and we use them to train a denoising autoencoder (mapping noisy and reverberated speech to its clean version) as well as an x-vector extractor which is currently considered as state-of-the-art in SV. Later, we use the autoencoder as a preprocessing step for a text-independent SV system. We compare results achieved with autoencoder enhancement, multi-condition PLDA training and their simultaneous use. We present a detailed analysis with various conditions of NIST SRE 2010, 2016, PRISM and with re-transmitted data. We conclude that the proposed preprocessing can significantly improve both i-vector and x-vector baselines and that this technique can be used to build a robust SV system for various target domains.},
	language = {en},
	urldate = {2023-02-05},
	journal = {Computer Speech \& Language},
	author = {Novotný, Ondřej and Plchot, Oldřich and Glembek, Ondřej and Černocký, Jan “Honza” and Burget, Lukáš},
	month = nov,
	year = {2019},
	keywords = {Autoencoder, Embedding, Neural network, Robustness, Signal enhancement, Speaker verification},
	pages = {403--421},
	file = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/RDUX75EN/Novotný et al. - 2019 - Analysis of DNN Speech Signal Enhancement for Robu.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/ZK8GBXBV/S0885230818303607.html:text/html},
}

@misc{mehri_samplernn_2017,
	title = {{SampleRNN}: {An} {Unconditional} {End}-to-{End} {Neural} {Audio} {Generation} {Model}},
	shorttitle = {{SampleRNN}},
	url = {http://arxiv.org/abs/1612.07837},
	doi = {10.48550/arXiv.1612.07837},
	abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2017},
	note = {arXiv:1612.07837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/KJX35XI6/Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JJMDZLFE/1612.html:text/html},
}

@misc{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.08435},
	doi = {10.48550/arXiv.1802.08435},
	abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
	month = jun,
	year = {2018},
	note = {arXiv:1802.08435 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/B4UUA7E3/Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/K5FC44E8/1802.html:text/html},
}

@misc{noauthor_char2wav_nodate,
	title = {{Char2Wav}: {End}-to-{End} {Speech} {Synthesis} {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=B1VWyySKx},
	urldate = {2023-02-06},
}

@misc{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	doi = {10.48550/arXiv.1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = oct,
	year = {2014},
	note = {arXiv:1409.1259 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/NX9TIBY3/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/3PTEVRVA/1409.html:text/html},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2023-02-06},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	doi = {10.48550/arXiv.1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01279 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/X3IHGVR2/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/SRGM3W7Z/1704.html:text/html},
}

@misc{oord_conditional_2016,
	title = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
	url = {http://arxiv.org/abs/1606.05328},
	doi = {10.48550/arXiv.1606.05328},
	abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv:1606.05328 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/ZYP4KT6D/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/KVQHL65H/1606.html:text/html},
}

@misc{paine_fast_2016,
	title = {Fast {Wavenet} {Generation} {Algorithm}},
	url = {http://arxiv.org/abs/1611.09482},
	doi = {10.48550/arXiv.1611.09482},
	abstract = {This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2{\textasciicircum}L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A. and Huang, Thomas S.},
	month = nov,
	year = {2016},
	note = {arXiv:1611.09482 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Comment: Technical Report},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/RJHQ2HEX/Paine et al. - 2016 - Fast Wavenet Generation Algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/JPNGP2MX/1611.html:text/html},
}

@misc{donahue_adversarial_2019,
	title = {Adversarial {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	doi = {10.48550/arXiv.1802.04208},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2019},
	note = {arXiv:1802.04208 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {arXiv Fulltext PDF:/Users/marcio/Zotero/storage/JQY4JJP6/Donahue et al. - 2019 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/marcio/Zotero/storage/HZXQNN6T/1802.html:text/html},
}

@article{cao_deconvolutional_2020,
	title = {Deconvolutional neural network for image super-resolution},
	volume = {132},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020303403},
	doi = {10.1016/j.neunet.2020.09.017},
	abstract = {This study builds a fully deconvolutional neural network (FDNN) and addresses the problem of single image super-resolution (SISR) by using the FDNN. Although SISR using deep neural networks has been a major research focus, the problem of reconstructing a high resolution (HR) image with an FDNN has received little attention. A few recent approaches toward SISR are to embed deconvolution operations into multilayer feedforward neural networks. This paper constructs a deep FDNN for SISR that possesses two remarkable advantages compared to existing SISR approaches. The first improves the network performance without increasing the depth of the network or embedding complex structures. The second replaces all convolution operations with deconvolution operations to implement an effective reconstruction. That is, the proposed FDNN only contains deconvolution layers and learns an end-to-end mapping from low resolution (LR) to HR images. Furthermore, to avoid the oversmoothness of the mean squared error loss, the trained image is treated as a probability distribution, and the Kullback–Leibler divergence is introduced into the final loss function to achieve enhanced recovery. Although the proposed FDNN only has 10 layers, it is successfully evaluated through extensive experiments. Compared with other state-of-the-art methods and deep convolution neural networks with 20 or 30 layers, the proposed FDNN achieves better performance for SISR.},
	language = {en},
	urldate = {2023-02-06},
	journal = {Neural Networks},
	author = {Cao, Feilong and Yao, Kaixuan and Liang, Jiye},
	month = dec,
	year = {2020},
	keywords = {Convolutional neural networks (CNNs), Deconvolutional neural networks, Deep learning, Single image super-resolution (SISR)},
	pages = {394--404},
	file = {ScienceDirect Full Text PDF:/Users/marcio/Zotero/storage/A2M5I6YD/Cao et al. - 2020 - Deconvolutional neural network for image super-res.pdf:application/pdf;ScienceDirect Snapshot:/Users/marcio/Zotero/storage/88IEST29/S0893608020303403.html:text/html},
}

@inproceedings{rao_grapheme--phoneme_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Grapheme-to-phoneme conversion using {Long} {Short}-{Term} {Memory} recurrent neural networks},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178767/},
	doi = {10.1109/ICASSP.2015.7178767},
	abstract = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoderdecoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
	language = {en},
	urldate = {2023-02-06},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Rao, Kanishka and Peng, Fuchun and Sak, Hasim and Beaufays, Francoise},
	month = apr,
	year = {2015},
	pages = {4225--4229},
	file = {Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:/Users/marcio/Zotero/storage/9W6TPQHQ/Rao et al. - 2015 - Grapheme-to-phoneme conversion using Long Short-Te.pdf:application/pdf},
}

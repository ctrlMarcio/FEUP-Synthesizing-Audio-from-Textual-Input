\subsection{Unsupervised Sound Generation} \label{sec:unsupervised-generation}

This Section focuses on models that address unsupervised or self-supervised training through learning sound features and their distributions without relying on explicit labels or annotations. In unsupervised sound generation, models learn from unlabeled audio data to capture underlying patterns and structures. This enables the generation of novel sound samples and the representation of latent features. This approach is particularly valuable when labeled datasets are scarce or expensive to acquire. Next, this discussion covers notable models in this area, selected based on their suitability for generating audio.

\subsubsection{WaveGAN} \label{sec:wavegan}

\Acp{GAN} have a notable impact on generating coherent images at the local and global levels, as discussed in Section~\ref{sec:gan}. A model based on \acp{GAN} called WaveGAN \cite{donahue_adversarial_2019} was proposed in 2019, to synthesize waveforms in an unsupervised manner. The model modifies the transposed convolution operation, used in \acp{DCGAN} for image generation, to capture waveform structure at different timescales.

WaveGAN modifies the transposed convolution operation in \acp{DCGAN}, expanding conventional \acp{GAN} to encompass image generation tasks and precisely capture the structure of audio signals of varying timescales. This model uses lengthier, one-dimensional filters of 25 units in place of two-dimensional filters with dimensions of $5 \times 5$. The model also upsamples each layer by a factor of 4, as is done in traditional \acp{DCGAN}. Despite these modifications, WaveGAN has the same number of parameters, numerical operations, and output dimensionality as \acp{DCGAN} have.

The experiments conducted on WaveGAN show that it can synthesize one-second slices of audio waveforms with global coherence, which is suitable for sound effect generation. The model also learns to produce intelligible words when trained on a small-vocabulary speech dataset without labels.

The success of WaveGAN in generating coherent audio signals demonstrates that \acp{GAN} can generate high-quality sounds. This work opens up new possibilities for unsupervised synthesis of raw-waveform audio, such as music and speech. It also suggests that \acp{GAN} can learn to capture the structure of signals across various timescales, which is crucial for generating realistic audio.

\subsubsection{Generative Transformer for Audio Synthesis}

In this work, Verma and Chafe~\cite{verma_generative_2021}, proposed in 2022, explore an alternative architecture using transformer networks (see Section~\ref{sec:transformers}), which have shown great success in sequential modeling tasks such as language translation.

The authors develop a generative transformer model for raw audio waveforms. The model is trained to autoregressively predict the next audio sample by attending over previous context samples. Specifically, the input waveform is split into overlapping frames and embedded into a latent space. A series of multi-headed causal self-attention layers then learn to focus on relevant parts of the input context to predict the subsequent sample distribution.

To retain information about the relative sample positions, positional encodings are added. Training deeper models is facilitated by layer normalization and residual connections. In a neural network, residual connections (also known as skip connections) enable the direct flow of information from one layer to subsequent layers. The use of residual connections helps to mitigate the vanishing gradient problem and allows for more effective gradient propagation during training. The inclusion of these connections enables the model to learn new representations at each layer and retain useful features from previous layers, resulting in improved performance and faster convergence. Self-attention provides the model with flexibility and frees it from the fixed topology of convolutions in other models such as WaveNet (Section~\ref{sec:wavenet}).

During training, previous samples are fed as input to the model to predict the next sample, optimized with cross-entropy loss (see Section~\ref{sec:cross-entropy}). The authors quantitatively evaluate next-step sample accuracy and find that the transformer architecture can outperform WaveNet baselines substantially.

\subsubsection{wav2vec 2.0}

The wav2vec 2.0 model comprises three essential components: a convolutional feature encoder, a Transformer network, and a quantization module. This model was originally introduced in the 2020 paper by Baevski et al. \cite{baevski_wav2vec_2020} and designed for speech generation tasks. For further details on the convolutional layer and the Transformer network, refer to Sections~\ref{sec:conv-layer} and \ref{sec:transformers}.

Quantization is the process of discretizing continuous values into a finite set of discrete symbols or codes, particularly in the context of generative models. This method is comparable to the technique used in the \ac{VQ-VAE} (refer to Section~\ref{sec:vq-vae}) In this technique, the input data is mapped to a limited number of discrete codebook entries.

For convolution, the feature encoder takes the raw audio waveform as input and generates a sequence of speech representations underlying it. This consists of several convolutional blocks, with each block including 1D temporal convolution and layer normalization. Wide kernels (\textit{e.g.,} 10ms) are used in the convolutions and progressively reduce the resolution of the input to extract hierarchical features.

The output of the feature encoder is fed into a transformer network to build contextualized representations. For encoding positional information specific to speech generation tasks, we use a convolutional layer instead of absolute positional embeddings. The self-attention mechanism enables each time step to consider all other time steps, thus capturing long-range dependencies in the sequence. Several Transformer layers extract higher levels of contextual abstraction.

A quantization module is applied to the output of the feature encoder. It discretizes the continuous latent representations into a finite inventory of speech units. Multiple codebooks are maintained, and concatenating selections from each codebook construct discrete units.

After pre-training on unlabeled speech, the model is fine-tuned on transcribed speech for speech recognition by adding a randomly initialized output layer. Various augmentations are used during fine-tuning to improve robustness.

The key innovations are the joint training of discrete speech units and contextualized representations in a completely self-supervised fashion. Experiments demonstrate strong performance even with just minutes of labeled data, highlighting the benefits of pre-training on large unlabeled corpora.

\subsubsection{SoundStream} \label{sec:soundstream}

SoundStream is a neural audio codec proposed in 2021 \cite{zeghidour_soundstream_2021} that can efficiently compress speech, music, and general audio. A codec is software or hardware that compresses and decompresses audio signals. The model architecture consists of a fully convolutional encoder/decoder network and a residual vector quantizer.

The fully convolutional encoder receives a time-domain waveform as input. It produces a sequence of embeddings at a lower sampling rate, which is then quantized by the \ac{RVQ}. The fully convolutional decoder then receives the quantized embeddings and reconstructs an approximation of the original waveform. Both the encoder and decoder use only causal convolutions, so the overall architectural latency of the model is determined solely by the temporal resampling ratio between the original time-domain waveform and the embeddings.

While there are similarities between SoundStream and a standard \ac{AE} (see Section \ref{sec:autoencoders}) in terms of the encoder-decoder architecture, SoundStream includes additional components such as the \ac{RVQ} and the use of structured dropout for variable bitrate compression.

A \acf{RVQ} is a vector quantization method. It is a variant of the traditional vector quantization method present, for instance, in \acp{VQ-VAE} (see Section \ref{sec:vq-vae}). In an \ac{RVQ}, the input data is first transformed into a lower-dimensional space using a neural network encoder. The resulting embeddings are then quantized using a codebook of fixed-size vectors, where each input embedding is assigned to the nearest codebook vector. However, instead of encoding the input embedding directly as the index of the assigned codebook vector, an \ac{RVQ} computes the difference between the input embedding and the assigned codebook vector, known as the residual. The residual is then quantized using a second codebook, and the indices of both codebook vectors are transmitted as the compressed representation.

Using residual vectors in \acp{RVQ} allows for better compression performance than traditional vector quantization methods. It captures the fine details of the input data that may be lost during quantization. In SoundStream, the \ac{RVQ} is used to quantify the embeddings produced by the fully convolutional encoder, enabling efficient audio compression at low bitrates while maintaining high audio quality.
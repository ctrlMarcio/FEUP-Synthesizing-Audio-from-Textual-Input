\subsubsection{Deep Learning Architectures}

Generative \ac{DL} architectures establish blueprints for developing \ac{DL} networks that synthesize diverse and novel data samples according to a learned distribution. These architectures entail creating latent data constructs and learning to emulate the fundamental statistical patterns found in observed data.

Generative deep neural models have been applied to tasks comprising image synthesis, text generation, and audio synthesis. Their popularity has recently surged owing to their remarkable ability to generate high-quality data and effectively model complex distributions. In the following sections, we outline the most ordinarily used generative \ac{DL} architectures, presented chronologically, as summarized in Table~\ref{tab:archs}.

\begin{table*}[ht]
\centering
\caption{Comparison of Generative Deep Learning Architectures}
\label{tab:archs}
\begin{tabularx}{\textwidth}{|p{20mm}|c|p{25mm}|X|c|}
\hline
\textbf{Model} & \textbf{Year} & \textbf{Type} & \textbf{Key Characteristics} & \textbf{Inference} \\ \hline
DARN & 2013 & Autoregressive & Uses a single model to predict the probability distribution of each output token conditioned on the previous tokens & Sequential \\ \hline
VAE & 2013 & Variational Autoencoder & Learns a latent representation of the input data and generates new samples by sampling from the learned latent space & Parallel \\ \hline
GAN & 2014 & Generative Adversarial Network & Consists of a generator and a discriminator that compete in a two-player minimax game to generate realistic samples & Parallel \\ \hline
Normalizing Flows & 2015 & Flow-based models & Transforms a simple probability distribution into a complex one by applying a sequence of invertible transformations & Parallel \\ \hline
Diffusion & 2015 & Flow-based models & Uses a diffusion process to model the probability distribution of the data & Parallel \\ \hline
Transformers & 2017 & Attention-based models & Uses self-attention to capture global dependencies and generate sequences & Sequential \\ \hline

VQ-VAE &
    2018 &
    Variational Autoencoder &
    Discretizes the continuous latent space by mapping each latent vector to the closest codebook vector &
    Parallel \\ \hline

MS-VQ-VAE &
    2019 &
    Variational Autoencoder &
    Is an extension of the VQ-VAE that incorporates multiple discrete latent spaces of different scales, enabling hierarchical and diverse representations with improved abstraction levels and latent space expressiveness &
    Parallel \\ \hline

\end{tabularx}
\end{table*}

This section will discuss novel architectural designs for \ac{DL}. It is important to note that there is a discrepancy between the date of their conceptualization and their widespread adoption. This trend is common in machine learning because software theories have a faster rate of development compared to hardware theories.

This section does not describe all \ac{DL} architectures that had some relevance. It, however, describes the ones used either by the models developed or by systems studied for the state-of-the-art.

\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/feedforward}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/cnn}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/conv-layers}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/rnn}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/rnn-variants}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/ae}
\input{src/chapters/2-state-of-the-art/background/deep-learning-models/deep-learning/u-net}
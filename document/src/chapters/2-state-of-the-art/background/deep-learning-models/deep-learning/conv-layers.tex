\paragraph{Convolutional Operations} \label{sec:conv-layers}

This section will discuss three common layers in \acp{CNN}: convolutional layers, pooling layers, and transposed convolutions. It also introduces the concept of masked convolutions and their applications in specific tasks.

\label{sec:conv-layer}

A \textbf{convolutional layer} is a fundamental building block of \acp{CNN} (see Section \ref{sec:CNN}).

At a high level, a convolutional layer applies a set of learnable filters to the input data, extracting local features and patterns relevant to the task. The filters are typically small and slide over the input data, computing the dot product between the filter weights and the input values at each position. This operation is called convolution and can be seen in Figure \ref{fig:conv-layer}.

\begin{figure}[ht]
    \centering
    \ctikzfig{figures/2-sota/conv}
    \caption[Convolutional layer]{\textbf{Convolutional layer} --- The presented diagram depicts a 1-dimensional convolutional layer designed to process an input signal consisting of a single channel and a length of 7. The layer employs a single filter with a size of 3, resulting in an output signal of length 5 and a single channel. Each element of the output signal is produced by convolving the corresponding filter weights with a subset of the input signal. Specifically, the first output element is generated by convolving the filter with the first three elements of the input signal. The filter is then slid along the input signal with a stride of 1, such that each subsequent output element depends on a different subset of the input signal.}
    \label{fig:conv-layer}
\end{figure}

To be more precise, let us consider a 1D convolutional layer that takes an input tensor $X$ of size $C_{in} \times L_{in}$, where $C_{in}$ is the number of input channels and $L_{in}$ is the length of the input sequence. The layer also has $C_{out}$ filters, each of size $C_{in} \times k$, where $k$ is the size of the filter kernel. The output of the layer is a tensor $Y$ of size $C_{out} \times L_{out}$, where $L_{out}$ is the length of the output sequence.

The convolution operation can be expressed as follows:

\begin{equation}
	Y_{c,i} = \sum_{p=0}^{k-1}\sum_{k=0}^{C_{in}-1} X_{k,i+p} \cdot W_{c,k,p} + b_c
\end{equation}

where $c$ is the index of the output channel, $i$ is the spatial index of the output sequence, $p$ is the spatial index of the filter kernel, $k$ is the index of the input channel, $X_{k,i+p}$ is the input value at position $(k,i+p)$, $W_{c,k,p}$ is the weight of the filter at position $(c,k,p)$, and $b_c$ is the bias term for the $c$-th output channel.

The convolution operation is applied independently to each output channel and each spatial location of the output feature map, resulting in a set of feature maps that capture different aspects of the input data. The weights and biases of the filters are learned during training using backpropagation and gradient descent (see section \ref{sec:backpropagation}), optimizing a suitable loss function for the task at hand.

% alskjdaslkjdasld

\label{sec:pooling}

\textbf{Pooling} is a standard operation in \acp{CNN} that reduces the spatial dimensions of the input feature maps while retaining important information. Pooling is typically applied after convolutional layers to gradually reduce the feature maps' spatial dimensions and increase the network's receptive field. There are several types of pooling, including max pooling, average pooling, L2 pooling, and more. Of these, max pooling is one of the most commonly used.

\textbf{Max pooling} works by partitioning the input feature map into non-overlapping rectangular regions, called pooling kernels. For each pooling kernel, the maximum value is selected and used as the output value for that region. The size of the pooling kernel and the stride determine the amount of reduction in the spatial dimensions of the feature map. The stride is the number of pixels that the pooling kernel is shifted horizontally and vertically between each pooling operation. It can be seen in the Figure \ref{fig:pool}

\begin{figure}[ht]
    \centering
    \ctikzfig{figures/2-sota/pool}
    \caption[Pooling layer]{\textbf{Pooling layer} --- The Figure illustrates a $4 \times 4$ input tensor with a single channel. Each colored square in the tensor denotes a single element. The pooling layer partitions the input tensor into non-overlapping $2 \times 2$ regions and applies a pooling function to each region, resulting in a $2 \times 2$ output tensor with a single channel. The number of channels remains unchanged in this scenario, implying that the pooling function is applied separately to each channel of the input tensor, and the resulting output tensor retains the same number of channels as the input tensor. The primary objective of pooling layers is to decrease the spatial dimensions of the input tensor while maintaining the essential features. In this instance, the pooling layer decreases the spatial dimensions of the input tensor by half, resulting in a smaller output tensor. The output tensor is displayed on the right-hand side of the image, where each colored square represents a single element of the output tensor. The colors of the output tensor correspond to those of the input tensor regions from which they were derived.}
    \label{fig:pool}
\end{figure}

Max pooling has several benefits for \acp{CNN}~\cite{riesenhuber_hierarchical_1999}:
\begin{itemize}
    \item It helps to reduce the model's sensitivity to minor variations in the input. This is because the maximum value within each pooling kernel is selected, which is less sensitive to slight variations than taking the average or other statistics.
    \item It can prevent overfitting by reducing the number of parameters in the model. This is because the pooling operation reduces the spatial dimensions of the feature map, reducing the number of parameters in the subsequent layers of the network.
    \item It can increase the network's receptive field by combining the information from neighboring pixels.
\end{itemize}

There are some potential drawbacks to max pooling as well. One is that it can discard some information that may be important for the task. This is because only the maximum value within each pooling kernel is selected, and other information is discarded.

%laskdjl kajsdlkajs ldkja

\label{sec:transpoed-conv}

A \textbf{transposed convolution}, also known as a deconvolution, is a layer that performs the opposite operation of a convolutional layer. It takes an input tensor of size $C_{in} \times L_{in}$ and produces an output tensor of size $C_{out} \times L_{out}$, where $C_{in}$ and $C_{out}$ are the number of input and output channels, respectively, and $L_{in}$ and $L_{out}$ are the input and output lengths.

The transposed convolution applies a set of learnable filters to the input data. However, instead of sliding the filters over the input, it slides them over the output and computes the dot product between the filter weights and the output values at each position. This operation can be seen as an ``unfolding'' of the convolution operation. Hence the name ``transposed convolution''. An example can be seen in Figure \ref{fig:trans-conv}.

\begin{figure}[ht]
    \centering
    \ctikzfig{figures/2-sota/trans-conv}
    \caption[Transpoed convolution]{\textbf{Transposed convolution} --- This illustration shows how a transposed convolution operation works. The input array has three elements $(3, 4, 3)$ and is shown by the dark blue rectangles at the top. The filter array also has three elements $(5, 5, 2)$ and is shown by the green rectangles in the middle. The output array is obtained by sliding the filter over the input array and computing the element-wise products and sums. The stride parameter controls how much the filter moves, which is 1 in this example. The light blue rectangles at the bottom show the intermediate and final output arrays. The intermediate output arrays are $(15, 15, 6, 0, 0)$, $(0, 20, 20, 8, 0)$, and $(0, 0, 15, 15, 6)$. The final output array is the sum of the intermediate output arrays $(15, 35, 41, 23, 6)$. The gray-shaded regions indicate how the filter broadcasts each element in the input array to produce an intermediate output array.}
    \label{fig:trans-conv}
\end{figure}

The transposed convolution can be expressed as follows:

\begin{equation}
	X_{k,i} = \sum_{p=0}^{k-1}\sum_{c=0}^{C_{out}-1} Y_{c,i+p} \cdot W_{c,k,p} + b_k
\end{equation}

where $k$ is the index of the input channel, $i$ is the spatial index of the input sequence, $p$ is the spatial index of the filter kernel, $c$ is the index of the output channel, $Y_{c,i+p}$ is the output value at position $(c,i+p)$, $W_{c,k,p}$ is the weight of the filter at position $(c,k,p)$, and $b_k$ is the bias term for the $k$-th input channel.

Like convolutional layers, the weights and biases of the filters are learned during training using backpropagation and gradient descent.

\label{sec:masked-conv}

A \textbf{masked convolution} is a convolutional layer that selectively masks out specific input values based on their position in the input sequence. This masking is typically done by setting the weights of the filters to zero for certain positions in the kernel. For example, in an \ac{AR} language modeling task, where the goal is to predict the next word in a sentence given the previous words, a masked convolution can ensure that the model only has access to the previous, not the future words.

Another example where masked convolutions can be helpful is in audio generation tasks. In such tasks, the model takes as input a sequence of audio samples and generates a new sequence of samples that sound similar to the input. In some cases, generating audio that only depends on the previous time steps rather than the entire input sequence may be desirable. This can be achieved using a masked convolution that masks out the future time steps by setting the filter weights to zero for those positions in the kernel. By doing so, the model can only rely on the previous time steps to generate the following sample.

Masked convolutions can be implemented using standard convolutional layers with appropriate masking of the filter weights. Another approach is using a specialized masked convolutional layer that takes an additional mask tensor as input, indicating which input values should be masked.

One advantage of masked convolutions is that they can help prevent the model from overfitting to the training data by forcing it to rely on the input data available at each time step rather than the ground truth values that may not be available at test time. This can be particularly useful in tasks where the input data has a sequential or temporal structure, and the model needs to make predictions based on partial information.

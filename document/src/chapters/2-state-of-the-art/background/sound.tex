\subsection{Sound} \label{sec:sound}

A digital audio signal --- often called a waveform --- captures alterations in sound pressure overtime. This waveform represents how sound waves develop and spread throughout space. Distinct sounds that constitute our auditory experience can be perceived and interpreted through analyzing the changes in frequency and amplitude within the waveform. Waveforms encode the richness of sound, enabling us to capture and manipulate it for diverse purposes such as analysis, synthesis, and artistic expression.

Sound is a continuous phenomenon but can be discretized by taking samples at a specific time rate. The number of samples taken in one second is known as the "sampling rate." The standard value for this is 44,100 Hz. The sampling rate has a direct impact on the accuracy of the signal~\cite{elsea_basics_1996}.

With the discrete version of time, it becomes possible to represent sound through an array digitally. Knowledge of the array and the sampling rate enables a computer to reconstruct the sound. These arrays can become quite large. For instance, stereo sound with a sampling rate of 44,100 \ac{Hz} needs to accommodate 1,411,200 bits per second \cite{elsea_basics_1996}.

\subsubsection{Short-Time Fourier Transform} \label{sec:stft}

Even though digital sound media can be encoded as one-dimensional data~\cite{oord_wavenet_2016}, it can be converted. By using a \acf{DFT}, the array can be represented in the frequency domain~\cite{benois-pineau_deep_2021}. Since the contents of a sound sample typically vary over time, \acp{DFT} can be computed over successive time frames of the signal. This operation forms the basis of the \acf{STFT}. The equation for \ac{STFT} is as follows:

\begin{equation} \label{eq:stft}
    STFT\{x(n)\}(m, \omega) = \sum_{n=-\infty}^{\infty} x(n) w(n - mR) e^{-j\omega n}
\end{equation}

In this equation, $STFT\{x(n)\}(m, \omega)$ represents the time-frequency representation of the input signal $x(n)$ as a function of time index $m$ and frequency $\omega$. The function is a complex-valued function containing both magnitude and phase information of the signal's frequency components at different time intervals.

The summation symbol $\sum_{n=-\infty}^{\infty}$ denotes that the product of the signal, window function, and complex exponential is summed over all time indices $n$. The discrete-time input signal is represented by $x(n)$, sampled at integer time indices $n$.

The window function, represented as $w(n - mR)$, is utilized to isolate a specific time interval of the input signal. The window function is centered at the time index $mR$, where $R$ is the hop or step size between consecutive sample windows.

Finally, the complex exponential term $e^{-j\omega n}$ is utilized to analyze the signal's frequency content within the windowed interval. The variable $j$ is the imaginary unit, and $\omega$ represents the angular frequency.

In essence, the \ac{STFT} equation computes the Fourier Transform of the input signal $x(n)$ within a windowed time interval, providing a time-frequency representation of the signal. The window function isolates a specific time interval of the signal, and the complex exponential term analyzes its frequency content. This process is repeated for different time indices $m$, resulting in a time-frequency representation that allows the study of the signal's frequency components at various time intervals.

With the \ac{STFT}, one can generate spectrograms by plotting the time in the $x$ axis and the frequency in the $y$ axis. In short, a spectrogram is a graphical representation of the frequency content of a signal over time, typically displayed as a 2D image (see Figure \ref{fig:sound} for an example).

\begin{figure}[ht]
    \centering
    \ctikzfig{figures/2-sota/sound}
    \caption[Raw Wave vs. Spectrogram]{\textbf{Raw Wave vs. Spectrogram} --- A comparative analysis of a sound sample from the Audio MNIST dataset (showcased in section \ref{sec:dataset-amnist}), specifically entry number 9 of speaker Nicolas uttering the digit ``five''. The top plot illustrates the raw waveform with time (sample rate $\times$ time) on the X-axis and energy (amplitude) on the Y-axis, providing a temporal representation of the audio signal. The bottom plot presents a spectrogram generated using the \ac{STFT} method, displaying time on the X-axis and frequency on the Y-axis, offering a time-frequency representation that reveals the spectral content and evolution of the signal over time.
    }
    \label{fig:sound}
\end{figure}

\subsubsection{Meaning of Spectrograms for Machine Learning}

Representing sound as an image opens a multitude of opportunities. However, even though spectrograms can technically be processed using \acp{CNN} (see section \ref{sec:CNN}), there is a considerable difference between a spectrogram and a standard image. In a typical image, the axes represent the same concept, the spatial position. The elements of an actual image have the same meaning independent of where they are found. A sub-object of an image does not depend on the axes. At the same time, neighbor pixels are usually highly correlated.

On the other hand, the axes of the spectrograms have different meanings \cite{benois-pineau_deep_2021}. Moving a set of pixels horizontally and vertically means different things. Therefore, structures such as \ac{CNN} are not as helpful. One can still use them but should be careful about the shape of the filters and the axis along which the convolution is performed \cite{benois-pineau_deep_2021}.

\subsubsection{Soundscapes} \label{sec:soundscapes}

The digitalization of sound has allowed for multiple applications and use cases. Applications can generate speech, and movies and videos can embed audio, such as soundscapes. Soundscapes are the sonic environments or sound environments that surround environments. They are the complex and dynamic mix of sounds heard in everyday life, including sounds from nature, human-made, and cultural sounds~\cite{international_organization_for_standardization_iso_2014, schafer_tuning_1977}. In other words, a soundscape encompasses the auditory milieu characterized by a collection of naturally occurring and human-generated sounds as perceived, encountered, and comprehended within a contextual framework by individuals. It is paramount in audio content creation, augmenting the user experience across media applications by infusing emotional engagement, a greater sense of immersion, and attention~\cite{chandrasekera_virtual_2015}.

Nevertheless, for audio media generation with \ac{ML}, one usually finds models in the literature that solve music or speech generation, not soundscapes. This is no coincidence. These sounds are more straightforward and, thus, easier to generate. Speech, for instance, usually contains a single sound source (the speaker). Also, speech and music are highly structured over time and timbrically. This happens because speech is bound to grammar, and music is bound to an underlying structure. Both of them are timbrically bound to their authors. On the contrary, soundscapes have no specific structure. Hence the increased difficulty \cite{benois-pineau_deep_2021}.

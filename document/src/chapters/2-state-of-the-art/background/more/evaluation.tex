\subsubsection{Evaluation Metrics} \label{sec:evaluation}

The evaluation process can provide insights into the system's performance and reveal areas that need improvement. Moreover, a proper evaluation metric helps to compare the results of different models and choose the best one. The aim of this section is to provide a comprehensive overview of the available evaluation metrics for audio generation and to provide a foundation for the evaluation of the system developed in this thesis.

In this thesis, a comprehensive evaluation framework is developed that considers two different types of evaluations. The first type of evaluation focuses on metrics that can be used for training models offline, such as loss metrics, which play a crucial role in assessing the performance of a model. The second type of evaluation focuses on evaluating the broader impacts of a model, such as its environmental impact and inference time for a generation. These evaluations are essential in ensuring that the model not only performs well on its primary task but also has minimal negative impacts on other aspects of the system.

\paragraph{Loss Functions} \label{sec:loss-functions}

Evaluating generative audio systems is challenging due to the need for a standard set of metrics to capture the quality and diversity of the generated audio samples. Different studies often use different evaluation methodologies and metrics when reporting results, making a direct comparison to other systems intricate if not impossible~\cite{vinay_evaluating_2022}. Furthermore, the perceptual relevance and meaning of the reported metrics, in most cases unknown, prohibit any conclusive insights concerning practical usability and audio quality.

A review and comparison of the available evaluation metrics for audio generation is essential to provide a foundation for evaluating the system developed in this thesis. This section discusses some of the commonly used metrics for evaluating generative audio systems, such as \ac{MAE}, \ac{MSE}, \ac{KL} divergence, and \ac{ELBO}. It also discusses their advantages and limitations and how they can be applied to sound generation tasks.

\subparagraph{Mean Absolute Error} \label{sec:mae}

The \Acf{MAE} is a quantitative measure of the average magnitude of the errors between the predicted and actual values~\cite{willmott_advantages_2005}. It is computed as:

\begin{equation}
	\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i| 
\end{equation}

where $y_i$ denotes the true value, $\hat{y}_i$ denotes the predicted value, and $n$ denotes the number of samples. The \ac{MAE} is also called L1-norm loss or \acf{MAD}.  

The \Ac{MAE} is a relevant metric for evaluating generative models as it treats all errors equally and is arguably less sensitive to outliers compared to \ac{MSE} (see Section \ref{sec:mse}). Specifically, it indicates the average absolute difference between the predicted and actual values in the same unit as the output.

However, the \ac{MAE} has some notable limitations that should be considered. For instance, it does not directly capture the perceptual quality of the generated audio samples. The perceptual quality of audio can depend on factors such as timbre, pitch, or harmony, which are not explicitly determined by the \ac{MAE}. Therefore, to fully assess the quality of generative models, the \ac{MAE} should be complemented with other metrics and human evaluation. Additionally, the \ac{MAE} is a scale-dependent measure and cannot be used to compare predictions that use different scales.

To illustrate the difference between \ac{MAE} computed on raw audio versus spectrograms, consider the following example: For a 1D raw audio sample, the \ac{MAE} would measure the average absolute difference between the amplitude values of the audio signals. In contrast, if the audio data were represented as spectrograms, the \ac{MAE} would measure the average absolute difference between the magnitude values of the frequency bins. In this case, the spectrograms can be treated as images with a single channel, and the \ac{MAE} can be seen as a pixel-wise error metric.  

It is important to consider the limitations of the \ac{MAE} when evaluating generative models. For example, consider a scenario with two audio samples of a dog barking: one with a bark at one second and another with a bark at two seconds. Despite being similar sounds with different temporal positions, calculating their \ac{MAE} would lead to higher-than-expected error as it does not account for temporal alignment. Therefore, it is important to use other metrics and human evaluation methods along with \ac{MAE} to assess timing accuracy and perceptual quality of generated audio samples comprehensively.

\subparagraph{Mean Squared Error} \label{sec:mse}

\Acf{MSE} is a standard metric to evaluate the performance of a predictor or an estimator. It quantifies the average of the squared errors, the average squared difference between the estimated and actual values. \Ac{MSE} is always a non-negative value approaching zero as the error decreases. The smaller the \ac{MSE}, the better the predictor or estimator~\cite{hodson_mean_2021}.

\Ac{MSE} can be calculated as follows:

$$
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

Where $n$ is the number of data points, $y_i$ is the true value of the variable being predicted or estimated, and $\hat{y}_i$ is the predicted or estimated value.

\Ac{MSE} incorporates both the variance and the bias of the predictor or estimator. The variance measures how widely spread the estimates are from one data sample to another. The bias measures the distance of the average estimated value from the true value. For an unbiased estimator, the \ac{MSE} equals the variance.

\Ac{MSE} can compare different predictors or estimators and select the one that minimizes the \ac{MSE}. For instance, in linear regression, \ac{MSE} can be used to find the best-fitting line that minimizes the sum of squared errors. \Ac{MSE} can also evaluate the quality of a generative model that produces audio samples from textual input. In this case, \ac{MSE} can measure how similar the generated audio samples are to the target audio samples regarding their amplitude values.

Unlike \ac{MAE} (see Section \ref{sec:mae}), which assigns equal weight to all errors, \ac{MSE} penalizes larger errors more than smaller ones. This means that \ac{MSE} is more sensitive to outliers and may not reflect the overall discrepancy between the generated and target audio samples well. Moreover, \ac{MSE} does not account for perceptual aspects of audio quality, such as timbre, pitch, or loudness. Therefore, \ac{MSE} should be used with other metrics and evaluation methods, such as \ac{KL} divergence (see Section \ref{sec:kld}), subjective listening tests, or qualitative analysis.

\Ac{MSE} can be applied to sound generation tasks in different ways, depending on the representation of the audio data. Similar to \ac{MAE}, it can be applied to 1D raw audio and spectrograms.

It should be noted that \ac{MSE} is subject to the same temporal issue as \ac{MAE}. \Ac{MSE} may not be effective in identifying differences in timing accuracy. Therefore, it is crucial to employ other metrics and evaluation methods that specifically focus on timing aspects and perceptual quality when assessing audio produced by text-trained models.

\subparagraph{Cross-Entropy} \label{sec:cross-entropy}

Cross Entropy Loss is commonly used to evaluate \ac{DL} models, especially in classification or sequence generation tasks. It measures the dissimilarity between predicted and target distributions by calculating the average negative log-likelihood of predicting each class or element correctly.

Cross Entropy Loss can be applied in audio generation tasks to produce discrete elements, such as musical notes or phonemes. For instance, if one aims to create music based on textual input with particular note sequences, Cross Entropy Loss can assess the prediction accuracy of each note at every time step.

Mathematically, given data samples $x_i$ and their respective true labels $y_i$, where $i$ ranges from 1 to $n$, Cross Entropy Loss can be calculated in the following way:

\begin{equation}
    CE = -\frac{1}{n}\sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij}\log(p_{ij})
\end{equation}

This is where:
The symbol $C$ represents the number of classes or elements.
The notation $y_{ij}$ indicates whether sample $x_i$ belongs to class $j$ (or has element $j$).
Additionally, $p_{ij}$ represents the predicted probability that sample $x_i$ belongs to class $j$ (or has element $j$).

The objective is to minimize the Cross Entropy Loss during training, so that the generative model can learn to predict precise and coherent distributions over classes or elements.

Nonetheless, it is important to consider some limitations when using Cross Entropy Loss to evaluate audio generation systems. First, the model assumes independence between individual predictions within one sample. However, this assumption may not hold for sequential audio data where the context and dependencies between elements are critical. Second, the Cross Entropy Loss does not directly capture perceptual aspects of audio quality, such as timbre or tonality. Therefore, it is advisable to combine Cross Entropy Loss with other evaluation metrics, such as \ac{MAE}, \ac{MSE}, or subjective listening tests to achieve a comprehensive understanding of the generative model's performance.

To summarize, the Cross Entropy Loss is commonly employed as a loss function for evaluating generative models that involve discrete element generation. Although it is applicable for audio generation tasks with categorical outputs such as music note prediction, it should be combined with other evaluation methods to gain a more comprehensive assessment of accuracy and perceptual quality.

\subparagraph{KL Divergence} \label{sec:kld}

\acf{KL} divergence, also known as relative entropy, is a non-symmetric measure of the difference between two probability distributions. It is a mathematical quantity that quantifies the distance between two probability distributions.

In simple terms, \ac{KL} divergence measures the difference between the probability distribution predicted by a model and the true underlying distribution of the data. \Ac{KL} divergence is commonly used to evaluate generative models.

\ac{KL} divergence is calculated as the expectation of the logarithmic difference between the predicted probability distribution and the actual distribution. It is a scalar value, and the smaller the \ac{KL} divergence, the closer the predicted distribution is to the real distribution.

The \ac{DL} system can be trained to produce sounds near the target sounds in terms of their probability distribution by using \ac{KL} divergence as a loss function. The concept is that the sound does not have to be alike the input, but only its distribution. Maximizing the log-likelihood between the generated output and the given input can be seen as minimizing the \ac{KL} divergence \cite{huzaifah_deep_2021}.

\subparagraph{Evidence Lower Bound (ELBO)} \label{sec:elbo}

\Acf{ELBO} is a lower bound on the log-likelihood of some observed data commonly used in variational Bayesian methods \cite{blei_variational_2017}.

The \ac{ELBO} is defined as follows:

\begin{equation}
ELBO = E_{Z \sim q}\left[\log \frac{p(X,Z; \theta)}{q(Z)} \right]
\end{equation}

where $X$ and $Z$ are random variables with joint distribution $p(X,Z; \theta)$, $\theta$ are the parameters of the model, and $q(Z)$ is an approximate posterior distribution for the latent variable $Z$. The \ac{ELBO} can be seen as the difference between two terms: the expected log joint probability of the data and the latent variables under the model and the entropy of the approximate posterior distribution.

The \ac{ELBO} has several desirable properties. First, it is a lower bound on the log-likelihood of the data, $\log p(X; \theta)$, also known as the evidence. Meaning that the \ac{ELBO} is a quantity that is always less than or equal to the log-likelihood of the data, which is the logarithm of the probability of the data given the model parameters. The log-likelihood of the data is also called the evidence because it indicates how well the model fits the data. The higher the log-likelihood, the more evidence we have that the model is suitable for the data. However, computing the log-likelihood of the data is often intractable. Therefore, the model can be optimized more easily using \ac{ELBO}.

Second, it is a tractable objective function that can be optimized concerning $\theta$ and $q(Z)$. This allows us to perform variational inference, approximating the posterior distribution $p(Z|X; \theta)$ by finding the $q(Z)$ that maximizes the \ac{ELBO}. This can be done using gradient-based methods, thus being used in machine learning systems.

Third, it can be decomposed into two significant components: the reconstruction term and the regularization term. The reconstruction term is the expected log-likelihood of the data given the latent variables under the model, $E_{Z \sim q}\left[\log p(X|Z; \theta)\right]$. It measures how well the model fits the data. The regularization term is the negative \ac{KL} divergence (see section \ref{sec:kld}) between the approximate posterior and the prior distributions, $-D_{KL}(q(Z)||p(Z))$. It measures how close the approximate posterior is to the prior. The \ac{KL} divergence is always non-negative, and it is zero if and only if $q(Z) = p(Z)$. Therefore, maximizing the \ac{ELBO} encourages data fidelity and posterior regularization.

The \ac{ELBO} can be applied to sound generation tasks using a deep generative model such as a \ac{VAE} (see section \ref{sec:vae}). This model can be trained by maximizing the \ac{ELBO} concerning its parameters and latent variables. The \ac{ELBO} can then be used to evaluate the quality and diversity of the generated sounds by comparing them to the target sounds. For instance, the \ac{ELBO} for a \ac{VAE} can be written as:

\begin{equation}
ELBO = E_{Z \sim q_\phi(Z|X)}\left[\log p_\theta(X|Z)\right] - D_{KL}(q_\phi(Z|X)||p(Z))
\end{equation}

The first term is the reconstruction term, which measures how well the decoder network reconstructs the input sound $X$ from the latent variable $Z$. The second term is the regularization term, which measures the proximity of the approximate posterior distribution to a prior distribution $p(Z)$.

By maximizing the \ac{ELBO}, a model learns to generate realistic sounds similar to the input sounds regarding their conditional distribution while ensuring that the latent variables have a smooth and regular structure that facilitates interpolation and manipulation.

\paragraph{Model Evaluation Functions}

The second type of evaluation in this thesis involves assessing the wider impacts and implications of the developed audio generation model. Although metrics used for training models offline provide insights into performance, it is equally important to evaluate how a model affects aspects beyond its primary task. This includes considerations such as the environmental impact and inference time during generation, among others. Understanding these broader impacts ensures that the model not only performs well in its intended purpose but also operates with minimal negative consequences or trade-offs in other areas of the system. Conducting evaluations encompassing these factors will provide a more comprehensive understanding of how well-rounded and sustainable our audio generation system is.

\subparagraph{Evaluating Energy Expended}

Evaluating the amount of energy expended by a deep learning model is crucial in developing and deploying these systems. With the increasing demand for machine learning applications and the complexity of deep learning models, energy efficiency has become a critical factor in designing and deploying deep learning systems.

With the growing concern for environmental sustainability, the energy footprint of deep learning models has become an essential topic in the field. Most of the recent advances produced by deep learning approaches rely on significant increases in size and complexity \cite{douwes_energy_2021}. Such improvements are backed by an increase in power consumption and carbon emissions. The high energy consumption of deep learning models during both the training and inference phases significantly impacts the environment, and it is imperative to address this issue.

Therefore, evaluating the amount of energy a deep learning model expends is essential in ensuring its practicality and scalability. This is a crucial step in ensuring that the deep learning models developed today are not only accurate, but also energy-efficient and sustainable for future deployment.

This evaluation can be done in two ways: physically measuring the energy expended by the machines on both learning and inference time or by approximating given average numbers per neuron, for instance.

A good model is a compromise between accuracy and complexity. If the model trains significantly longer to train or infer and does not provide way better results, in the context of this research, the model is not much better than a simpler counterpart.
\paragraph{Acoustic Data Augmentation}

According to Abayomi-Alli et al. \cite{abayomi-alli_data_2022}, the most commonly used data augmentation tools for audio \ac{ML} tasks are the addition of noise, time shifting, pitch shifting, \ac{GAN}-based methods (see Section \ref{sec:gan}), time stretching, and concatenation. Other techniques, such as overlapping, are also helpful. All these techniques play a critical role in increasing the size of the training dataset and providing the model with a diverse range of input data. This Section thoroughly explores these techniques and discusses their impact on the performance of sound-based \ac{ML} models.

Table \ref{tab:acoustic-data-augmentation} provides a concise summary of the advantages and limitations of each technique.

\begin{table}[hp]
\caption{Summary of Acoustic Data Augmentation Methods}
\label{tab:acoustic-data-augmentation}
\begin{tabularx}{\textwidth}{|p{2cm}|X|X|X|}
\hline
\textbf{Method}     & \textbf{Description}                                                                             & \textbf{Advantages}                                                                                  & \textbf{Limitations}                                                                                             \\ \hline
Addition of Noise   & Adding random noise to the original audio signals. & Increases dataset diversity, helps model learn to handle noisy environments.                         & Noise must be chosen to align with the problem at hand.                            \\ \hline
Time Shifting       & Altering the temporal structure of an audio signal by shifting it. & Helps model learn sound patterns invariant to temporal changes, improves generalization. & Trimming and shifting sections may introduce artifacts.                                    \\ \hline
Pitch Shifting      & Altering an audio signal's frequency. & Trains models to recognize sound patterns free from pitch variations.                                & May introduce artifacts or distortions.                                       \\ \hline
\ac{GAN} Based Methods   & Utilizing \ac{GAN} networks to generate synthetic audio signals.            & Generates high-quality data.                      & Requires significant computational resources.                     \\ \hline
Time Stretching     & Changing the time duration while preserving their spectral content.             & Generates samples with different time durations.                     & Simple methods may introduce artifacts; sophisticated methods are computationally intensive.     \\ \hline
Sound Concatenation & Joining snippets of multiple audio signals to form a new one.               & Trains models that recognize specific sound sequences.                         & Labels associated with the original audio signals may need to be modified to represent the new signal. \\ \hline
Sound Overlapping   & Merging two or more audio signals to form a new combined audio signal.                           & Helps models identify sound patterns amidst overlapping sounds.              & Amplitude adjustment and normalization are crucial to prevent overpowering or clipping in the composite signal.  \\ \hline
\end{tabularx}
\end{table}

\subparagraph{Addition of Noise}
Data augmentation for audio with the addition of noise is a common technique that involves adding random noise to the original audio signals to produce new, diverse audio samples.

The process of adding noise to audio signals involves several steps:

\begin{enumerate}
    \item Select a noise source: This can be any type of noise, generated or real \cite{novotny_analysis_2019}, such as white noise, babble noise, static noise, factory noise, jet cockpit, shouting, background noise, and others, \cite{abayomi-alli_data_2022}. The noises should be chosen according to the problem at hand.
    \item Specify the noise level: For instance, the augmented sound might be $y' = y + 0.05 \times Wn$ where $y$ is the initial sound, $y'$ the augmented sound, and $Wn$ some white noise \cite{mushtaq_environmental_2020}.
    \item Add the noise: The selected noise source is then added to the audio signal by adding the noise and audio signals element-wise.
    \item Normalize the output: Finally, the resulting audio signal with added noise is normalized to prevent clipping or overloading.
\end{enumerate}

Repeating these steps with different noise sources and noise levels makes it possible to generate multiple, diverse audio samples that can be used for data augmentation purposes.

\subparagraph{Time Shifting} \label{sec:time-shifting}
Time shifting, also known as time warping, is a data augmentation technique that involves altering the temporal structure of an audio signal.

Time shifting involves shifting the entire audio signal by a certain amount of time, either forwards or backward. This can be achieved by adding or removing samples from the audio signal or changing the existing samples' position within the signal.

One approach to implementing time shifting involves trimming the length of the audio signal and using the trimmed sections to create new, diverse audio samples. For example, consider an audio signal of size 150. If this signal is trimmed to a length of 125, up to 25 new audio samples can be generated by shifting the trimmed sections. These new samples can be labeled the same as the original audio signal.

Time shifting by trimming and shifting sections of the audio signal can significantly impact the performance of sound-based \ac{ML} models. By providing the model with diverse, time-shifted versions of the audio signal, this technique can help the model learn to identify sound patterns invariant to temporal changes, such as the presence of a particular sound event or the spoken words in an audio recording. This can lead to better generalization performance on new, unseen data and improved overall model performance.

\subparagraph{Pitch Shifting}
Pitch shifting is a technique used in audio data augmentation that involves altering an audio signal's fundamental frequency.

Pitch shifting is achieved by adjusting the pitch of an audio signal positively or negatively. For example, a plus or minus two shift can be implemented \cite{mushtaq_environmental_2020}. This process results in audio signals that have a different pitch. This can be helpful in training models to recognize sound patterns that are free from pitch variations.

\subparagraph{GAN Based Methods}
Utilizing \acp{GAN} (see Section~\ref{sec:gan}) in data augmentation for audio signals can be a powerful and effective method, albeit slower than other techniques. In this approach, a \ac{GAN} network is trained on the available audio data to learn the underlying patterns and distributions present in the data. The network then generates new, synthetic audio signals similar to the input data \cite{qian_data_2019}.

The success of the \ac{GAN}-based data augmentation method depends heavily on the quality of the \ac{GAN} training, as well as the diversity of the input data. If the \ac{GAN} is trained well and the input data is diverse, the generated data will be of high quality.

It is important to note that \acp{GAN} require considerable computational resources and training time compared to other data augmentation techniques. However, the results obtained from \acp{GAN} can be highly effective and accurate, making this approach a valuable addition to the data augmentation toolkit for audio machine learning tasks.

\subparagraph{Time Stretching}
Time stretching as a data augmentation technique for audio signals involves changing the time duration of the audio signals, typically by increasing or decreasing the time axis of the audio signals. The purpose of time stretching is to generate new audio samples from the original audio signals with different time durations.

A straightforward way of implementing time stretching is to use a stretching factor. For example, if the stretching factor is $1.2$, then the time axis of the audio signal is increased by 20\%. To achieve this, one approach is to use a naive algorithm that duplicates some of the samples in the audio signal according to the stretching factor. However, this simple method can result in undesirable artifacts, such as pitch changes, if the stretching factor is not an integer.

More sophisticated methods, such as phase vocoder-based time stretching, can produce high-quality time stretching with minimal artifacts. These methods use time and frequency domain processing techniques to stretch the audio signal while preserving its spectral content and temporal structure. The resulting audio signal has a different time duration while preserving the original pitch~\cite{akaishi_improving_2023}.

\subparagraph{Sound Concatenation}
Mixing up sounds, or sound concatenation, is a method for audio signals where multiple audio signals are joined to form a new and diverse audio signal. This technique can be achieved by taking snippets of multiple audio signals and concatenating them randomly or using cross-fade techniques to ensure a seamless transition between the different audio snippets.

This technique would be advantageous in a sound generation setting where one wants to make the network learn a prompt such as ``dog barking and then car honking''. When applying sound concatenation, one must consider that the label will also change.

\subparagraph{Sound Overlapping}

Sound overlapping, also referred to as sound mixing or audio blending, is a technical process that involves merging two or more audio signals to form a new combined audio signal. This process is currently utilized in popular data augmentation platforms~\cite{maguolo_audiogmenter_2022} to aid the model in identifying sound patterns amidst overlapping sounds, which is a frequent occurrence in real-world applications.

There are several steps involved in the process of sound overlapping. The first step involves selecting multiple audio signals, which can originate from the same or different sources depending on the desired outcome and problem at hand. The next step is to adjust the amplitude of each audio signal to achieve a balanced combination and prevent one signal from overpowering others. This step is crucial. This can be achieved by either normalizing the amplitude of each signal or scaling them based on a predetermined factor. After appropriately adjusting the amplitudes, the selected audio signals are combined by adding them element-wise. A new composite audio signal is generated using this process, which includes overlapping sounds from the original signals. It is essential to normalize the output to prevent clipping or overloading, ensuring a well-balanced and usable composite sound waveform.

Repeating these steps with different combinations of audio signals and amplitude adjustments can generate diverse composite audio samples for data augmentation purposes.

It is important to note that when using sound overlapping as a data augmentation technique, the labels associated with the original audio signals must also be considered. Sometimes, the labels may need to be combined or modified to accurately represent the new composite audio signal.
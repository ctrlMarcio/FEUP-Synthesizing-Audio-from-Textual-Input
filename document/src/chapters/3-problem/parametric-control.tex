\section{Parametric Control} \label{sec:parametric-control}

If one creates a generator model that generates sounds without any input, one would only generate random sounds without any meaning behind them. Without conditioning, WaveNet generates ``babble'' \cite{huzaifah_deep_2021}.

An end-to-end model must be able to take textual input and generate a sound from it. For this, the textual input must be somehow passed to the model during training and inference.

This is called parametric control, sound modeling, or model conditioning. ``Sound modelling is [...] developing algorithms that generate sound under parametric control'' \cite{huzaifah_deep_2021}. This is the ability to manipulate and control the characteristics of the generated audio by adjusting the model parameters. For example, a generative model for musical audio synthesis may allow the user to control the pitch, timbre, or duration of the generated audio by adjusting the specific parameters of the model. The goal of parametric control in generative audio models is to allow the user to fine-tune the characteristics of the generated audio and achieve the desired results. Parametric control is essential in this work because the output must be tailored specifically for the user's input prompt.

This is a crucial job for these types of models and also one of the toughest obstacles in creating digital audio \cite{huzaifah_deep_2021}.

Luckily, this is a solved problem in the data generation realm. Given the quick advances in generative deep learning technologies, every generator production model relies on model conditioning. For instance, transformers receive an input text vector natively. Also, \acp{GAN} can be conditioned on a specific range of inputs. These examples illustrate that it is more than possible to incorporate this kind of technology in an end-to-end model for sound generation.

Modern models for multimodal learning, which involve processing information from different media types such as images, sound, and text, have been based on a variation of \acp{VAE} as seen in sections \ref{sec:data-generators} and \ref{sec:related-work}. One approach to conditioning \acp{VAE} on text involves training different media types, such as images and text, to share a common latent space. This is achieved by optimizing a joint objective function that balances the reconstruction loss of each modality with the alignment of the respective latent spaces.

The text input is first encoded into a latent representation using an encoder network during inference. The image decoder network then decodes this latent representation to generate the output image that corresponds to the given text input.

This is lightly debated in Section~\ref{sec:model}, and more deeply argued in Section~\ref{sec:sol-models}.
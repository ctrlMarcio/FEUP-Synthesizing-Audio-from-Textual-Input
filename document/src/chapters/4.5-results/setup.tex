\section{Experimental Setup} \label{sec:res-setup}

This section outlines the experimental setup utilized to assess the audio generation capabilities of the GANmix model.

The GANmix model was trained using the \ac{BCE} loss, which was implemented through pytorch's \texttt{BCEWithLogitsLoss} module. Although \ac{BCE} is commonly employed for binary classification, it can also effectively train the generator in \acp{GAN}. In the context of GANmix, the generator's goal is to create authentic embeddings that can fool the discriminator into classifying them as authentic. To achieve this, the generator's loss is calculated using the discriminator's output for the fake embeddings generated by the generator and the true label --- either real or fake --- for a real embedding.

By using \ac{BCE}, the generator calculates its loss by measuring the difference between the discriminator's classification of genuine and false embeddings. This loss directs the generator to generate more realistic embeddings over time by minimizing this difference. Therefore, despite being a binary classification loss function, \ac{BCE} is a suitable approach for training the generator in \acp{GAN}.

Two datasets, the Audio MNIST dataset~\cite{becker_interpreting_2018} and the Clotho dataset~\cite{drossos_clotho_2019}, were utilized in these experiments. These datasets were previously described in detail in Section~\ref{sec:sol-datasets}.

The Audio MNIST dataset contains short audio clips where each clip represents a spoken digit. This dataset sets a standard for evaluating audio classification tasks.

In contrast, the Clotho dataset is more extensive and diverse. It offers a variety of audio samples including environmental sounds and speech from various sources. This dataset offers a diverse and comprehensive range of audio data, allowing the GANmix model to learn and generate audio samples that capture the complexity and diversity present in real-world audio recordings.

For a thorough understanding of the datasets, including their characteristics, preprocessing steps, and data augmentation techniques, please refer to Section~\ref{sec:sol-datasets}.

The experiments were conducted on three distinct hardware configurations, referred to as \textit{Kaggle}, \textit{\ac{LIACC} 1}, and \textit{\ac{LIACC} 2}, each selected based on the available resources during the development process.

Kaggle, the initial configuration implemented for the GANmix model development phase, utilized a Tesla V100 \ac{GPU} equipped with 12 \ac{GB} of memory, 73.1 \ac{GB} of disk space, and 13 \ac{GB} of \ac{RAM}.

As development progressed, the \ac{LIACC} 1 became available, which offered around the same computational power, \ac{LIACC} 1 used a GeForce GTX 1080 \ac{GPU} with 8 \ac{GB} of memory, 50 \ac{GB} of disk space, and 32 \ac{GB} of \ac{RAM}.

In the final stages of the project, the third configuration, \ac{LIACC} 2, was made available. \ac{LIACC} 2 employed a \ac{GPU} Quadro RTX 8000 with 48 \ac{GB} of memory, 50 \ac{GB} of disk space, and 128 \ac{GB} of \ac{RAM}, allowing for extensive training and evaluation of the GANmix model.

These hardware configurations were chosen to facilitate GANmix model training and evaluation throughout the developmental stages.

The GANmix model was implemented using the Python programming language and the PyTorch deep learning framework. There is a further discussion about this framework in Section~\ref{sec:dl-frameworks}.

Before training, a preprocessing step was applied to the audio data by randomly Random cropping was done with a set duration of 5 seconds. Random cropping is discussed further in Section~\ref{sec:findings}. This approach enabled a more diverse dataset since the majority of its samples had longer durations. Random cropping aided in capturing diverse segments of the audio and improved the model's ability to generate realistic audio samples.

The GANmix model underwent training using a set of specific hyperparameters. The batch size ranged between 1 and 32, providing varying trade-offs between computational efficiency and model convergence. The training epochs varied from a few tens to a few hundreds, depending on the dataset and model complexity. 

The learning rates utilized to train the GANmix model ranged from $1 \times 10^{-5}$ to $1 \times 10^{-2}$, chosen based on empirical observations and prior research.

For some models, the training process was halted before reaching the maximum number of epochs due to evidence of convergence. This choice was made to optimize computational resources while obtaining satisfactory outcomes.
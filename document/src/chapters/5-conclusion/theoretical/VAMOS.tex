\subsubsection{VAMOS - Variable Audio Model for Sound Synthesis}

In the field of audio and sound processing, VAMOS (Variable Audio Model for Sound Synthesis) is a novel implementation inspired by the well-known DALL-E 2 framework (see Section~\ref{sec:dall-e-2}). While DALL-E 2's lies in its ability to generate images from textual descriptions, VAMOS extends this concept to the auditory domain, generating audio outputs based on corresponding textual inputs. This section describes the architecture, components, and underlying mechanisms that make up the VAMOS model.

Indeed, the VAMOS model goes beyond mere conceptualization, as its development was diligently initiated from the ground up. The foundation of this innovative model has been established through craftsmanship, with each component designed. The VAMOS architecture is available in open-source. Appendix~\ref{ann:vamos} shows and explains the significant code portions.

\paragraph{Model Composition}

VAMOS is comprised of four distinct yet interconnected models, each contributing to a comprehensive audio generation process. These models — CLAP, Text Encoder, Audio Encoder (ResNet), and VamGen — work together to synthesize audio content that aligns with given textual cues. It is crucial to note that these models were thoughtfully constructed with functions designed for training and inference, without requiring the time-intensive process of fine-tuning.

\subparagraph{CLAP}

The foundation of VAMOS's cross-modal capability is CLAP. Similar to OpenAI's CLIP~\cite{radford_learning_2021}, but specifically designed for audio, CLAP brings text and audio together by projecting them onto a common dimensional space. In practice, CLAP uses independent feature extraction techniques for audio and text inputs, enabling flexibility for differing sources and types of information. These extraction methods, whether state-of-the-art or custom-built, produce latent feature vectors that allow for comparability between textual and auditory data.

Linear layers connecting audio and text features to a predefined embedding space are central to the operation of CLAP. This embedding space is achieved through distinct linear transformations for audio and text inputs. The fundamental principle of CLAP is to align the output of these linear layers for corresponding text and audio inputs. This alignment is accomplished by pairwise similarity calculations, which serve as the foundation for CLAP's loss function. The loss function guides the convergence of both audio and text features.

\subparagraph{Text Encoder: Unveiling Semantic Context}

The Text Encoder plays a crucial role in translating textual inputs into semantically-rich representations in VAMOS. Complementary to CLAP, it harnesses BERT's transformative capabilities~\cite{devlin_bert_2018}, a language model renowned for its contextual understanding.

To accomplish this, the Text Encoder follows a multi-step process guided by the principles of BERT. It begins by breaking down the input text, dividing the words and phrases into tokens that are then mapped onto a sequence of input IDs. Through the encoder transformer, the tokenized sequence is converted into an encoding that captures the deep semantic context embedded in the text.

During the development of VAMOS's Text Encoder, a deliberate decision was made to adopt a pre-existing model rather than create a new one tailored for this specific project. This decision was influenced by the specialized nature of the Text Encoder, which exclusively concentrates on processing textual input. Since the central focus of this thesis focuses on audio, adopting a verified text encoding model facilitated a more efficient development process, allocating resources to the innovative challenges presented by audio synthesis.

It is important to mention that the decision to utilize BERT is motivated by its dual quality of being a cutting-edge language processing model as well as an open-source tool. The HuggingFace library allows for BERT's capabilities to be integrated into VAMOS's architecture.

\subparagraph{Audio Encoder: ResNet-based Auditory Embedding}

To address the auditory aspect of VAMOS, the Audio Encoder utilizes the ResNet architecture~\cite{he_deep_2015} to generate informative embeddings from audio inputs. Unlike the Text Encoder, which depends on pre-existing implementations, the Audio Encoder is a customized solution designed specifically for audio-related tasks. A defining feature of the ResNet design is the integration of residual connections, otherwise known as ResBlocks, which enhances the network's capability to process complex audio features. These ResBlocks neatly divide the layers, permitting specific computations and bolstering the network's flexibility.

The fundamental principle of the Audio Encoder rests on leveraging residual connections to optimize audio feature extraction. By selectively including or excluding layers, the network achieves a flexible architecture. This allows it to capture both complex and simple audio features. Although the ResNet architecture is complex, the implementation presented in this thesis is customizable, giving users the ability to create ResBlocks specific to their use cases.

\paragraph{VamGen: Auditory Diffusion}

At the top of the VAMOS architecture is VamGen, a model rooted in DALL-E 2 but adapted for audio synthesis. VamGen exploits the potential of text-audio alignment to generate auditory output from textual prompts.

A key tenet of VamGen is a diffusion decoding process (see Section~\ref{sec:diffusion}). The textual input is encoded into latent features, which undergo a diffusion process to transform them into audio latent features. These audio latent features serve as the basis for the subsequent generation of spectrograms, simpler representations of audio suitable for further processing.

\subparagraph{Generative Stack Components}

VamGen's generative stack comprises two key components: the prior and the decoder. The prior generates image embeddings based on captions, thereby aligning textual cues with image-like representations. The decoder utilizes these image embeddings and captions to produce spectrogram outputs. The decoder, modeled as a diffusion process, provides a dynamic framework in which audio embeddings transform into coherent spectrograms gradually.

The diffusion-based approach to generating audio embeddings emphasizes the stochastic nature of the process, allowing for creative freedom within a structured framework.

\subparagraph{Implementation}

Though time constraints limited the realization of the full VamGen model, significant progress was made in its development. The foundation of VamGen lies in the U-net architecture, custom-built to align with the diffusion process.

In essence, the U-net architecture, celebrated for its skill in image segmentation tasks (see Section~\ref{sec:u-net}), inherently facilitates the diffusion process - a valuable feature that aligns effortlessly with VamGen's aim to synthesize audio from text input.

It consists of two interrelated parts: an encoder and a decoder. The encoder component skillfully converts raw audio inputs into intermediate representations. Simultaneously, the decoder resamples these intermediate representations into a format that resembles the original one.

The U-net integrates seamlessly into the diffusion process thanks to its innate ability to maintain the input's original dimensions. This attribute is critical in preserving the fidelity of audio representations during the diffusion process. As a result, maintaining these dimensions ensures a faithful reconstruction of audio content, preserving the essence of the original input. This preservation and subsequent fusion with the new dimensions introduced by VamGen's diffusion process come together to shape an audio synthesis narrative that resembles segmentation - a clear distinction between noise and authentic input.

The U-net architecture has been fully developed and tested. In addition to the public repository, its code is available in the annex~\ref{ann:vamos}.

\paragraph{Concluding Remarks}

It is important to acknowledge that the VAMOS model presented here is a prototype, designed to serve as a foundation for future advancements. Although these models have not undergone training and some elements remain incomplete, the investigation of cross-modal alignment and audio synthesis highlights the possibility of merging textual and auditory domains.

In summary, VAMOS represents a significant advancement in harnessing the creative synergy between text and audio, fostering a rich auditory experience through the intersection of innovative models and cross-modal alignment.
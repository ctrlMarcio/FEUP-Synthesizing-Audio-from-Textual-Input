\chapter*{Resumo}
%\addcontentsline{toc}{chapter}{Resumo}

Hoje em dia, o áudio é um elemento essencial na maioria do conteúdo produzido online. Normalmente, é trabalho manual que está por detrás dos terabytes de áudio publicados diariamente. Se algum produtor desejar um som específico, tem de o pesquisar em bases de dados online, sintetizá-lo ou até gravá-lo. Essa quantidade de trabalho é uma restrição à criação de conteúdo, principalmente se esses sons forem elaborados ou muito específicos.

Os ficheiros de áudio apresentam apenas uma dimensão, \textit{i.e.}, a amplitude da sua onda sonora recolhida num determinado intervalo de tempo. Em comparação, os ficheiros de imagem apresentam três dimensões de dados. Independentemente, as dependências temporais constituem um desafio no som, uma vez que são mais complexas e intrincadas do que as das imagens. Por exemplo, existe a expetativa de que o timbre de um determinado instrumento ou o ruído de fundo de um determinado ambiente sonoro se mantenha ao longo do tempo.

Esta dissertação apresenta uma investigação exaustiva de modelos avançados de IA generativa. Sugere alguns modelos e executa outros. Redirecciona a sua atenção para um modelo baseado em GANs que opera no espaço latente. Ao contrário de sistemas anteriores que limitam a síntese de áudio a domínios específicos, como a música ou a fala, o sistema proposto ultrapassa estas restrições, gerando áudio a partir de qualquer pedido textual. Outro foco desta dissertação é o desenvolvimento de diversas estruturas de modelos de deep learning, todas baseadas em abordagens modernas, bem como os desafios que surgem quando se trabalha com hardware limitado.

Dado que já existe trabalho significativo com modelos que criam imagens ou realizam a conversão de texto em fala, a expetativa é que um modelo generativo de áudio de ponta produza resultados satisfatórios. Os resultados deste trabalho terão impacto na velocidade do processo de produção para criadores de conteúdo, engenheiros de som e todos os interessados na criação de produtos sonoros.

\chapter*{Abstract}
%\addcontentsline{toc}{chapter}{Abstract}

Nowadays, audio is a fundamental aspect of most online content. Generating terabytes of audio content daily relies heavily on manual labor. Content creators often find themselves tackling arduous tasks such as researching sounds from online databases, synthesizing complex audio, or even resorting to self-recording. This substantial workload poses a significant hurdle to content creation, particularly when dealing with intricate or highly specific sounds.

Unlike image files, which present three-dimensional data dimensions and are utilized by mainstream generative models, audio files are restricted to a single dimension, representing the amplitude of sound waves at specific time intervals. However, the challenge of long-term dependencies in audio is more complex than in images. Instrument timbre and persistent background noise are factors that further complicate the audio synthesis process.

This dissertation presents an extensive investigation of advanced generative AI models. It suggests certain models and carries out others. It redirects its attention to a model based on GANs that operates within the latent space. Unlike previous systems that limit audio synthesis to specific domains such as music or speech, the proposed system overcomes these constraints by generating audio from any textual prompt. Another focus of this dissertation is the development of varied deep learning model structures, all based on cutting-edge methods, as well as the challenges that arise when working with limited hardware.

The main contribution of this work focuses on investigating cutting-edge generative AI models, as well as developing and enhancing generative AI frameworks customized for audio synthesis. This involves thoroughly exploring model components, training strategies, and performance benchmarks. This research bridges a critical gap in the current AI landscape by allowing the creation of diverse and contextually rich audio content from textual cues. It not only demonstrates the capabilities of AI-powered audio synthesis, but also offers a useful resource for researchers and professionals exploring this growing field.


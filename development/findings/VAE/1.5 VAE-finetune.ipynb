{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.436591Z","iopub.status.busy":"2023-04-14T12:40:37.436049Z","iopub.status.idle":"2023-04-14T12:40:37.500009Z","shell.execute_reply":"2023-04-14T12:40:37.498865Z","shell.execute_reply.started":"2023-04-14T12:40:37.436561Z"},"trusted":true},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.502628Z","iopub.status.busy":"2023-04-14T12:40:37.502218Z","iopub.status.idle":"2023-04-14T12:40:37.510911Z","shell.execute_reply":"2023-04-14T12:40:37.509873Z","shell.execute_reply.started":"2023-04-14T12:40:37.502587Z"},"trusted":true},"outputs":[],"source":["##### file system\n","INPUT_DIR = \"../data/input/audio-mnist/data\"\n","WORKING_DIR = \"../data/working\"\n","\n","##### dataset\n","SAMPLE_RATE = 48_000\n","MAX_INPUT_SIZE = 65_536\n","\n","##### training\n","TRAIN_PERCENTAGE = 0.8\n","BATCH_SIZE = 32\n","DATA_PERCENTAGE = 0.5\n","\n","##### notebook\n","EXPLORING = False"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.513095Z","iopub.status.busy":"2023-04-14T12:40:37.512634Z","iopub.status.idle":"2023-04-14T12:40:37.521600Z","shell.execute_reply":"2023-04-14T12:40:37.520565Z","shell.execute_reply.started":"2023-04-14T12:40:37.513059Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(WORKING_DIR):\n","    # create all dirs required\n","    os.makedirs(WORKING_DIR)"]},{"cell_type":"markdown","metadata":{},"source":["# Explore Data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.525882Z","iopub.status.busy":"2023-04-14T12:40:37.524904Z","iopub.status.idle":"2023-04-14T12:40:37.745752Z","shell.execute_reply":"2023-04-14T12:40:37.744742Z","shell.execute_reply.started":"2023-04-14T12:40:37.525841Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","from scipy.io import wavfile\n","import shutil\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data for PyTorch"]},{"cell_type":"markdown","metadata":{},"source":["I will be using PyTorch, to load a custom dataset it needs some fixing.\n","\n","- I will create a csv file with information regarding all files"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.748549Z","iopub.status.busy":"2023-04-14T12:40:37.747812Z","iopub.status.idle":"2023-04-14T12:40:37.756372Z","shell.execute_reply":"2023-04-14T12:40:37.755400Z","shell.execute_reply.started":"2023-04-14T12:40:37.748509Z"},"trusted":true},"outputs":[],"source":["def create_csv_info():\n","    # initialize the dataset\n","    dataset = pd.DataFrame(columns=[\"file\", \"label\", \"speaker\"])\n","\n","    for speaker in tqdm(os.listdir(INPUT_DIR)):\n","        # check if \"speaker\" is a folder\n","        # \"speaker\" should be a folder with all the recordings from the given\n","        #   speaker, the name of the speaker is the name of the folder\n","        if not os.path.isdir(os.path.join(INPUT_DIR, speaker)):\n","            continue\n","\n","        for file in os.listdir(os.path.join(INPUT_DIR, speaker)):\n","            label = file.split(\"_\")[0]\n","            file_path = os.path.join(speaker, file)\n","\n","            dataset = pd.concat([dataset, pd.DataFrame([[file_path, label, speaker]], columns=[\"file\", \"label\", \"speaker\"])])\n","\n","    # write dataset as csv\n","    dataset.to_csv(os.path.join(WORKING_DIR, \"dataset.csv\"), index=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:40:37.758542Z","iopub.status.busy":"2023-04-14T12:40:37.758027Z","iopub.status.idle":"2023-04-14T12:41:50.529382Z","shell.execute_reply":"2023-04-14T12:41:50.528148Z","shell.execute_reply.started":"2023-04-14T12:40:37.758495Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 61/61 [00:21<00:00,  2.84it/s]\n"]}],"source":["if not os.path.exists('/kaggle/working/dataset.csv'):\n","    create_csv_info()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:50.531574Z","iopub.status.busy":"2023-04-14T12:41:50.531157Z","iopub.status.idle":"2023-04-14T12:41:51.603528Z","shell.execute_reply":"2023-04-14T12:41:51.602418Z","shell.execute_reply.started":"2023-04-14T12:41:50.531535Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","import numpy as np\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:51.605671Z","iopub.status.busy":"2023-04-14T12:41:51.605024Z","iopub.status.idle":"2023-04-14T12:41:51.616983Z","shell.execute_reply":"2023-04-14T12:41:51.615713Z","shell.execute_reply.started":"2023-04-14T12:41:51.605627Z"},"trusted":true},"outputs":[],"source":["class AudioSample():\n","    # this defines one audio sample\n","    def __init__(self, sample_rate, samples):\n","        self.sample_rate = sample_rate\n","        self.samples = samples\n","\n","    def display(self):\n","        plt.plot(self.samples)\n","        plt.show()\n","\n","\n","class AudioDescription():\n","    # apart from the audio also has the label and the speaker\n","    def __init__(self, audio_sample, label, speaker):\n","        self.audio_sample = audio_sample\n","        self.label = label\n","        self.speaker = speaker\n","\n","    def display(self):\n","        self.audio_sample.display()\n","        print(\"Label: \", self.label)\n","        print(\"Speaker: \", self.speaker)\n","\n","\n","class AudioMNIST(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        csv_file_full_path = os.path.join(csv_file)\n","\n","        self.dataset = pd.read_csv(csv_file_full_path)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        file_path = os.path.join(self.root_dir, self.dataset.iloc[idx, 0])\n","\n","        sample_rate, samples = wavfile.read(file_path)\n","\n","        audio_description = AudioDescription(\n","            AudioSample(sample_rate, samples),\n","            self.dataset.iloc[idx, 1],\n","            self.dataset.iloc[idx, 2]\n","        )\n","\n","        if self.transform:\n","            audio_description = self.transform(audio_description)\n","\n","        return audio_description\n"]},{"cell_type":"markdown","metadata":{},"source":["## Transforms"]},{"cell_type":"markdown","metadata":{},"source":["The data in dataset does not have all the same size. So transformations are\n","required. Two transformations are possible. Padding and Cropping."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:51.622547Z","iopub.status.busy":"2023-04-14T12:41:51.621839Z","iopub.status.idle":"2023-04-14T12:41:51.634317Z","shell.execute_reply":"2023-04-14T12:41:51.633252Z","shell.execute_reply.started":"2023-04-14T12:41:51.622499Z"},"trusted":true},"outputs":[],"source":["class Padding():\n","    def __init__(self, output_size=MAX_INPUT_SIZE, mode=\"edge\"):\n","        # output size is an integer\n","        # for mode check np.pad https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n","        self.output_size = output_size\n","        self.mode = mode\n","\n","    def __call__(self, audio_description):\n","        # sample is an AudioDescription\n","        if len(audio_description.audio_sample.samples) > self.output_size:\n","            # throw error; cannot pad sample to a smaller size\n","            raise ValueError(\"Cannot pad sample to a smaller size\")\n","\n","        new_samples = np.pad(audio_description.audio_sample.samples, ((self.output_size - len(audio_description.audio_sample.samples) + 1) //\n","                                                                      2, (self.output_size - len(audio_description.audio_sample.samples)) // 2), mode=self.mode)\n","\n","        return AudioDescription(\n","            AudioSample(audio_description.audio_sample.sample_rate, new_samples),\n","            audio_description.label,\n","            audio_description.speaker\n","        )\n","\n","\n","class RandomCrop():\n","    def __init__(self, output_size=int(MAX_INPUT_SIZE/2)):\n","        # output size is an integer\n","        self.output_size = output_size\n","\n","    def __call__(self, audio_description):\n","        # audio_sample is an AudioDescription\n","        if len(audio_description.audio_sample.samples) <= self.output_size:\n","            # pad the sample\n","            audio_description = Padding(self.output_size)(audio_description)\n","        else:\n","            # get random start index\n","            start_index = np.random.randint(\n","                0, len(audio_description.audio_sample.samples) - self.output_size)\n","\n","            audio_description = AudioDescription(AudioSample(\n","                audio_description.audio_sample.sample_rate, audio_description.audio_sample.samples[start_index:start_index + self.output_size]),\n","                audio_description.label,\n","                audio_description.speaker\n","            )\n","\n","        return audio_description\n"]},{"cell_type":"markdown","metadata":{},"source":["## Putting it all together"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:51.636223Z","iopub.status.busy":"2023-04-14T12:41:51.635825Z","iopub.status.idle":"2023-04-14T12:41:51.720069Z","shell.execute_reply":"2023-04-14T12:41:51.718897Z","shell.execute_reply.started":"2023-04-14T12:41:51.636185Z"},"trusted":true},"outputs":[],"source":["dataset = AudioMNIST(WORKING_DIR + \"/dataset.csv\",\n","                     INPUT_DIR, transform=RandomCrop())\n","\n","# split dataset into train and test\n","train_size = int(TRAIN_PERCENTAGE * len(dataset))\n","test_size = len(dataset) - train_size\n","\n","train_dataset, test_dataset = torch.utils.data.random_split(\n","    dataset, [train_size, test_size])\n","\n","\n","def my_collate(batch):\n","    # batch is a list of AudioDescription\n","    samples = []\n","    labels = []\n","    speakers = []\n","\n","    for audio_description in batch:\n","        samples.append(audio_description.audio_sample.samples)\n","        labels.append(audio_description.label)\n","        speakers.append(audio_description.speaker)\n","\n","    # convert the list of audio samples to a tensor with shape [N, W]\n","    X = torch.tensor(samples)\n","\n","    # add a new dimension to the tensor to get the desired shape [N, 1, W]\n","    X = X.unsqueeze(1)\n","\n","    # convert the lists of labels and speakers to tensors\n","    y = torch.tensor(labels)\n","    z = torch.tensor(speakers)\n","\n","    # return the modified tensors\n","    return X, y, z\n","\n","\n","dataloader_train = DataLoader(\n","    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=my_collate)\n","dataloader_test = DataLoader(\n","    test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=my_collate)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:51.724001Z","iopub.status.busy":"2023-04-14T12:41:51.721494Z","iopub.status.idle":"2023-04-14T12:41:52.696553Z","shell.execute_reply":"2023-04-14T12:41:52.695388Z","shell.execute_reply.started":"2023-04-14T12:41:51.723930Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n","/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n"]},{"name":"stdout","output_type":"stream","text":["Shape of X [N, C, W]: torch.Size([32, 1, 32768])\n","Shape of y: torch.Size([32]) torch.int64\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n","/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n"]}],"source":["# show one sample with the loader\n","for sample, label, speaker in dataloader_train:\n","    print(f\"Shape of X [N, C, W]: {sample.shape}\")\n","    print(f\"Shape of y: {label.shape} {label.dtype}\")\n","    break\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.698568Z","iopub.status.busy":"2023-04-14T12:41:52.698226Z","iopub.status.idle":"2023-04-14T12:41:52.704665Z","shell.execute_reply":"2023-04-14T12:41:52.703623Z","shell.execute_reply.started":"2023-04-14T12:41:52.698534Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import random\n","import time\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.707479Z","iopub.status.busy":"2023-04-14T12:41:52.706735Z","iopub.status.idle":"2023-04-14T12:41:52.806023Z","shell.execute_reply":"2023-04-14T12:41:52.804581Z","shell.execute_reply.started":"2023-04-14T12:41:52.707442Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["# get CPU or GPU device for training\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.809438Z","iopub.status.busy":"2023-04-14T12:41:52.809090Z","iopub.status.idle":"2023-04-14T12:41:52.833396Z","shell.execute_reply":"2023-04-14T12:41:52.832357Z","shell.execute_reply.started":"2023-04-14T12:41:52.809409Z"},"trusted":true},"outputs":[],"source":["class VAE(nn.Module):\n","    def __init__(self, convolutional_layers=2, input_size=int(MAX_INPUT_SIZE/2), latent_dim=10):\n","        super().__init__()\n","\n","        if convolutional_layers < 1:\n","            raise ValueError(\"Convolutional layers must be at least 1\")\n","\n","        self.convolutional_layers = convolutional_layers\n","        self.latent_dim = latent_dim\n","        self.input_size = input_size\n","\n","        self.encoder, self.meaner, self.varer = self._build_encoder()\n","        self.decoder = self._build_decoder()\n","\n","    def forward(self, x):\n","        # float the input\n","        x = x.float()\n","\n","        # pass it through the common part of the encoder\n","        y = self.encoder(x)\n","\n","        # in a VAE, after the encoder, it splits into two branches\n","        # one with latent_dims of means, and other of vars\n","        mean = self.meaner(y)\n","        logvar = self.varer(y)\n","\n","        # get a list from the mean and vars\n","        z = self._sample(mean, logvar)\n","\n","        # take that list of latent values and decode it\n","        decoded = self.decoder(z)\n","        return decoded, mean, logvar\n","\n","    def _build_encoder(self):\n","        layers = []\n","\n","        layers.append(nn.Conv1d(1, 32, kernel_size=9, stride=1, padding=4))\n","        layers.append(nn.BatchNorm1d(32))\n","        layers.append(nn.Tanh())\n","        layers.append(nn.MaxPool1d(kernel_size=2, stride=2))\n","\n","        for i in range(self.convolutional_layers - 1):\n","            # add a convolutional layer with 2^i filters, kernel size 9, stride 1, padding 'same'\n","            layer = nn.Conv1d(2**i*32, 2**(i+1)*32,\n","                              kernel_size=9, stride=1, padding=4)\n","            layers.append(layer)\n","            layers.append(nn.BatchNorm1d(2**(i+1)*32))\n","            # add a tanh activation function\n","            layers.append(nn.Tanh())\n","\n","            # add a max pooling layer with kernel size 2 and stride 2\n","            layers.append(nn.MaxPool1d(kernel_size=2, stride=2))\n","\n","        # output both the mean and log variance of the latent variables\n","        mean_layer = nn.Sequential(\n","            # global average pooling\n","            nn.AdaptiveAvgPool1d(1),\n","            # flatten the output\n","            nn.Flatten(),\n","            # add a linear layer with latent_dims outputs\n","            nn.Linear(2**(self.convolutional_layers-1)*32, self.latent_dim)\n","        )\n","\n","        var_layer = nn.Sequential(\n","            # global average pooling\n","            nn.AdaptiveAvgPool1d(1),\n","            # flatten the output\n","            nn.Flatten(),\n","            # add a linear layer with latent_dims outputs\n","            nn.Linear(2**(self.convolutional_layers-1)*32, self.latent_dim)\n","        )\n","\n","        return nn.Sequential(*layers), mean_layer, var_layer\n","\n","    def _build_decoder(self):\n","        layers = []\n","\n","        # the input is the latent vector\n","        # one should transform it to a tensor with shape\n","        # [N, 2^convolutional_layers*32, W/2^convolutional_layers]\n","        # where N is the batch size, W is the width of the original input\n","        layers.append(\n","            nn.Linear(self.latent_dim, 2**(self.convolutional_layers-1)\n","                      * 32 * (self.input_size // 2**self.convolutional_layers))\n","        )\n","\n","        # reshape the tensor to [N, 2^convolutional_layers*32, W/2^convolutional_layers]\n","        layers.append(\n","            nn.Unflatten(1, (2**(self.convolutional_layers-1)*32,\n","                         self.input_size // 2**self.convolutional_layers))\n","        )\n","\n","        # i decreasing from convolutional_layers to 0\n","        for i in range(self.convolutional_layers - 2, -1, -1):\n","            layers.append(nn.Upsample(scale_factor=2))\n","            layers.append(nn.ConvTranspose1d(2**(i+1)*32, 2**i *\n","                                             32, kernel_size=9, stride=1, padding=4))\n","            layers.append(nn.BatchNorm1d(2**i*32))\n","            layers.append(nn.Tanh())\n","\n","        layers.append(nn.Upsample(scale_factor=2))\n","        layers.append(nn.ConvTranspose1d(\n","            32, 1, kernel_size=9, stride=1, padding=4))\n","        layers.append(nn.BatchNorm1d(1))\n","        layers.append(nn.Tanh())\n","\n","        return nn.Sequential(*layers)\n","\n","    def _sample(self, mean, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return eps * std + mean\n","\n","    def loss_function(self, recon_x, x, mean, logvar):\n","        # VAE loss function\n","        # get max and min values\n","        recon_x = torch.clamp(recon_x, 0, 1) # normalize output\n","        x = torch.clamp(x, 0, 1) # normalize input\n","        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","        KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n","\n","        return BCE + KLD\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.837710Z","iopub.status.busy":"2023-04-14T12:41:52.837194Z","iopub.status.idle":"2023-04-14T12:41:52.851674Z","shell.execute_reply":"2023-04-14T12:41:52.851010Z","shell.execute_reply.started":"2023-04-14T12:41:52.837681Z"},"trusted":true},"outputs":[],"source":["def train_step(data_loader, model, optimizer):\n","    model.train()\n","\n","    train_loss = 0\n","\n","    for batch, (sample, label, _) in enumerate(data_loader):\n","        # normalize the audio wav sample\n","        sample = sample / 32768\n","        sample = sample.float()\n","\n","        # send variables to GPU (if available)\n","        sample, label = sample.to(device), label.to(device)\n","\n","        # compute prediction error\n","        recon_batch, mean, logvar = model(sample)\n","        loss = model.loss_function(recon_batch, sample, mean, logvar)\n","\n","        # backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(sample)\n","            print(\n","                f\"loss: {loss:>7f}  [{current:>5d}/{len(data_loader.dataset):>5d}]\")\n","\n","        del sample, label\n","\n","    train_loss /= len(data_loader.dataset)\n","    return train_loss\n","\n","\n","def test(data_loader, model, name):\n","    model.eval()\n","\n","    test_loss = 0\n","    showed = False\n","\n","    with torch.no_grad():\n","        for sample, label, _ in data_loader:\n","            # normalize the audio wav sample\n","            # passing to the realm of values between -1 and 1\n","            # or wav format 32-bit floating-point\n","            # check https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html\n","            sample = sample / 32768\n","\n","            sample, label = sample.to(device), label.to(device)\n","\n","            sample.float()\n","\n","            pred, mean, logvar = model(sample)\n","            loss = model.loss_function(pred, sample, mean, logvar)\n","            \n","            test_loss += loss\n","\n","            # show a random wave\n","            if not showed:\n","                showed = True\n","                sample = sample.cpu().detach().numpy()\n","                pred = pred.cpu().detach().numpy()\n","\n","                # Create the first plot\n","                plt.figure()\n","                plt.plot(sample[0][0])\n","                plt.title('Original Sample')\n","                plt.legend(['Original'])\n","                plt.show()\n","\n","                # Create the second plot\n","                plt.figure()\n","                plt.plot(pred[0][0])\n","                plt.title('Reconstructed Sample')\n","                plt.legend(['Reconstructed'])\n","                plt.show()\n","\n","                # save the sound\n","                wavfile.write(\"test_sample_\" + str(name) +\n","                              \".wav\", SAMPLE_RATE, sample[0][0])\n","                wavfile.write(\"test_pred_\" + str(name) +\n","                              \".wav\", SAMPLE_RATE, pred[0][0])\n","\n","    test_loss /= len(data_loader)\n","\n","    return test_loss\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.855109Z","iopub.status.busy":"2023-04-14T12:41:52.854760Z","iopub.status.idle":"2023-04-14T12:41:52.867002Z","shell.execute_reply":"2023-04-14T12:41:52.865630Z","shell.execute_reply.started":"2023-04-14T12:41:52.855081Z"},"trusted":true},"outputs":[],"source":["# things function deletes variables only if they exist, ignoring the error\n","# it's useful when one wants to delete multiple variables without knowing if they exist or not\n","# this notebook does that to save GPU variables\n","def del_if_found(variable):\n","    try:\n","        del variable\n","    except:\n","        pass\n","\n","if 'model' in globals():\n","    del model"]},{"cell_type":"markdown","metadata":{},"source":["## Model settings"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.869249Z","iopub.status.busy":"2023-04-14T12:41:52.868679Z","iopub.status.idle":"2023-04-14T12:41:52.876050Z","shell.execute_reply":"2023-04-14T12:41:52.874847Z","shell.execute_reply.started":"2023-04-14T12:41:52.869210Z"},"trusted":true},"outputs":[],"source":["# maximum number of iterations without improvement\n","MAX_ITERATIONS_WITHOUT_IMPROVEMENT = 5\n","# number of convolutional layers of the encoder\n","CONV_LAYERS = 8\n","# possible learning rate ranges\n","LR_RANGE = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n","# number of latent dimensions (average + stds)\n","LATENT_DIM = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Find the optimal learning rate with the learning rate test"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:52.878582Z","iopub.status.busy":"2023-04-14T12:41:52.878320Z","iopub.status.idle":"2023-04-14T12:41:59.886629Z","shell.execute_reply":"2023-04-14T12:41:59.885400Z","shell.execute_reply.started":"2023-04-14T12:41:52.878557Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with 8 convolutional layers\n","Number of parameters: 255124043\n"]}],"source":["# creates the model\n","model = VAE(convolutional_layers=CONV_LAYERS, latent_dim=LATENT_DIM).to(device)\n","\n","# display information regarding the mode\n","print(f\"Training with {CONV_LAYERS} convolutional layers\")\n","num_params = sum(p.numel() for p in model.parameters())\n","print(f\"Number of parameters: {num_params}\")\n","\n","if EXPLORING:\n","    # sets the adam optimizer with the first learning rate in the range\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LR_RANGE[0])\n","\n","    # initialize empty list to store losses for each learning rate\n","    losses = []\n","\n","    # loop through each learning rate in the range\n","    for lr in LR_RANGE:\n","        # set the learning rate for the optimizer\n","        optimizer.param_groups[0]['lr'] = lr\n","\n","        # train the model for 3 epochs using the current learning rate\n","        for i in range(3):\n","            # call the train function and pass in the train dataloader, model, optimizer, and number of epochs to train for\n","            train_step(dataloader_train, model, optimizer)\n","\n","            # print a message indicating that the epoch has completed\n","            print(f\"Epoch {i+1} completed\")\n","\n","        # test the model on the test dataset and store the loss in the losses list\n","        loss = test(dataloader_test, model, f\"{CONV_LAYERS}_{lr}\")\n","        losses.append(loss)\n","\n","        # print a message indicating the learning rate and the corresponding loss\n","        print(f\"====> Learning rate: {lr} Loss: {loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training actually"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:59.888800Z","iopub.status.busy":"2023-04-14T12:41:59.888199Z","iopub.status.idle":"2023-04-14T12:41:59.896053Z","shell.execute_reply":"2023-04-14T12:41:59.894793Z","shell.execute_reply.started":"2023-04-14T12:41:59.888747Z"},"trusted":true},"outputs":[],"source":["if EXPLORING:\n","    # plot the learning rate vs loss curve\n","    plt.plot(torch.tensor(LR_RANGE), torch.tensor(losses), label='Loss')\n","    plt.xscale('log')\n","    plt.xlabel('Learning rate')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    # get optimal learning rate\n","    opt_lr = lr_range[losses.index(min(losses))]\n","else:\n","    opt_lr = 0.01\n","    \n","# reset optimizer with optimal learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=opt_lr)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-14T12:41:59.898411Z","iopub.status.busy":"2023-04-14T12:41:59.897460Z","iopub.status.idle":"2023-04-14T13:34:45.608048Z","shell.execute_reply":"2023-04-14T13:34:45.606250Z","shell.execute_reply.started":"2023-04-14T12:41:59.898375Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0:\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n","/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n","/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n","/tmp/ipykernel_10000/1663252155.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  X = torch.tensor(samples)\n"]},{"name":"stdout","output_type":"stream","text":["loss: 700338.062500  [    0/24000]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 73.19 MiB free; 7.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m iterations_without_improvement \u001b[39m<\u001b[39m MAX_ITERATIONS_WITHOUT_IMPROVEMENT:\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(dataloader_train, model, optimizer)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m====> Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m Training loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m test(dataloader_test, model, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCONV_LAYERS\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(data_loader, model, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m sample, label \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mto(device), label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[39m# compute prediction error\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m recon_batch, mean, logvar \u001b[39m=\u001b[39m model(sample)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_function(recon_batch, sample, mean, logvar)\n\u001b[1;32m     18\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[23], line 20\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     19\u001b[0m \u001b[39m# pass it through the common part of the encoder\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     22\u001b[0m \u001b[39m# in a VAE, after the encoder, it splits into two branches\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# one with latent_dims of means, and other of vars\u001b[39;00m\n\u001b[1;32m     24\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeaner(y)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 7.93 GiB total capacity; 6.58 GiB already allocated; 73.19 MiB free; 7.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# start training\n","start_time = time.time()\n","epoch = 0\n","iterations_without_improvement = 0\n","best_loss = float('inf')\n","\n","while iterations_without_improvement < MAX_ITERATIONS_WITHOUT_IMPROVEMENT:\n","    print(f\"Epoch {epoch}:\")\n","    train_loss = train_step(dataloader_train, model, optimizer)\n","    print(f\"====> Epoch: {epoch} Training loss: {train_loss:.4f}\")\n","    loss = test(dataloader_test, model, f\"{CONV_LAYERS}_{epoch}\")\n","    print(f\"====> Test set loss: {loss:.4f}\")\n","\n","    if loss < best_loss:\n","        best_loss = loss\n","        iterations_without_improvement = 0\n","        # save the model if it has improved\n","        torch.save(model.state_dict(), f\"model_{CONV_LAYERS}.pth\")\n","    else:\n","        iterations_without_improvement += 1\n","\n","    epoch += 1\n","\n","time_took = time.time() - start_time\n","print(f\"Training time: {time_took} seconds\")\n","print(f\"Epochs: {epoch}\")\n","\n","print(\"Done!\")\n","\n","# del variables on gpu and skip if one of them is not found\n","del_if_found(model)\n","del_if_found(sample)\n","del_if_found(label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
